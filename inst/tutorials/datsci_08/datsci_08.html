<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Rafael Irizarry &amp; Fatih Uenal" />


<meta name="progressive" content="true" />
<meta name="allow-skip" content="false" />

<title>datsci_08: Machine Learning</title>


<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>

<!-- taken from https://github.com/rstudio/rmarkdown/blob/67b7f5fc779e4cfdfd0f021d3d7745b6b6e17149/inst/rmd/h/default.html#L296-L362 -->
<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("section-TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>
<!-- end tabsets -->


<link rel="stylesheet" href="css/datsci_style.css" type="text/css" />

</head>

<body>



<div class="pageContent band">
<div class="bandContent page">

<div class="topics">

<html lang="en">
<div id="section-datsci_08-machine-learning" class="section level1">
<h1>datsci_08: Machine Learning</h1>
<div id="section-welcome-to-data-science-machine-learning" class="section level2">
<h2>Welcome to Data Science: Machine Learning!</h2>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:100px;height:100px;" class = "img_left"></p>
<p><strong>Usage:</strong> This tutorial accompanies the textbook <a href="https://rafalab.github.io/dsbook/">Introduction to Data Science</a> by <a href="http://rafalab.github.io/pages/about.html">Prof Rafael Irizarry</a>. It contains material from the textbook which is offered under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a>.</p>
</div>
<div id="section-data-science-machine-learning" class="section level3">
<h3>Data Science: Machine Learning</h3>
<p>We’re excited to have you join us in this course, which is designed to teach you about linear regression, one of the most common statistical modeling approaches used in data science.</p>
<p>This is the <strong>eighth</strong> in a series of courses in the Introduction to Data Science program, a series of courses that prepare you to do data analysis in <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>, from simple computations to machine learning. We assume that you have taken the preceding seven courses in the series or are already familiar with the content covered in them.</p>
<p>Perhaps the most popular data science methodologies come from machine learning. What distinguishes machine learning from other computer guided decision processes is that it builds prediction algorithms using data. Some of the most popular products that use machine learning include the handwriting readers implemented by the postal service, speech recognition, movie recommendation systems, and spam detectors.</p>
<p>In this course, you will learn popular machine learning algorithms, principal component analysis, and regularization by building a movie recommendation system. You will learn about training data, a set of data used to discover potentially predictive relationships, and how the data can come in the form of the outcome we want to predict and features that we will use to predict this outcome. As you build the movie recommendation system, you will learn how to train algorithms using training data so you can predict the outcome for future datasets. You will also learn about overtraining and techniques to avoid it such as cross-validation. All of these skills are fundamental to machine learning.</p>
<p>The class notes for this course series can be found in Professor Irizarry’s freely available <a href="https://rafalab.github.io/dsbook/">Introduction to Data Science book</a>.</p>
</div>
<div id="section-in-this-course-you-will-learn" class="section level3">
<h3>In this course, you will learn:</h3>
<ul>
<li><p>The basics of machine learning</p></li>
<li><p>How to perform cross-validation to avoid overtraining</p></li>
<li><p>Several popular machine learning algorithms</p></li>
<li><p>How to build a recommendation system</p></li>
<li><p>What regularization is and why it is useful</p></li>
</ul>
</div>
<div id="section-course-overview" class="section level3">
<h3>Course overview</h3>
<p>There are six major sections in this course: introduction to machine learning; machine learning basics; linear regression for prediction, smoothing, and working with matrices; distance, knn, cross validation, and generative models; classification with more than two classes and the caret package; and model fitting and recommendation systems.</p>
<p><strong>Section 1: Introduction to Machine Learning</strong></p>
<p>In this section, you’ll be introduced to some of the terminology and concepts you’ll need going forward.</p>
<p><strong>Section 2: Machine Learning Basics</strong></p>
<p>In this section, you’ll learn how to start building a machine learning algorithm using training and test data sets and the importance of conditional probabilities for machine learning.</p>
<p><strong>Section 3: Linear Regression for Prediction, Smoothing, and Working with Matrices</strong></p>
<p>In this section, you’ll learn why linear regression is a useful baseline approach but is often insufficiently flexible for more complex analyses, how to smooth noisy data, and how to use matrices for machine learning.</p>
<p><strong>Section 4: Distance, Knn, Cross Validation, and Generative Models</strong></p>
<p>In this section, you’ll learn different types of discriminative and generative approaches for machine learning algorithms.</p>
<p><strong>Section 5: Classification with More than Two Classes and the Caret Package</strong></p>
<p>In this section, you’ll learn how to overcome the curse of dimensionality using methods that adapt to higher dimensions and how to use the <strong>caret</strong> package to implement many different machine learning algorithms.</p>
<p><strong>Section 6: Model Fitting and Recommendation Systems</strong></p>
<p>In this section, you’ll learn how to apply the machine learning algorithms you have learned.</p>
</div>
<div id="section-meet-the-course-instructor" class="section level3">
<h3>Meet the Course Instructor</h3>
<div class="infobox">
<p><img src="images/photo2014.jpg" alt="Dr Fatih Uenal." style="width:150px;height:200px;" class = "img_left" ></p>
<p><strong>Fatih Uenal</strong> is currenlty a Visitng Postdoctoral Researcher at the University of Cambridge, Department of Psychology, where he conducts research on the psychology of anthropocentrism and social and ecological dominance. Prior to his current position, he has worked as a postdoc at <a href="https://scholar.harvard.edu/fatih-uenal/home">Harvard University</a>. Together with <a href="http://rafalab.github.io/pages/about.html">Prof Rafael Irizarry</a> he programmed this interactive tutorial based on the the textbook <a href="https://rafalab.github.io/dsbook/"><em>Introduction to Data Science</em></a>. This interactive tutorial is developed using the <code>learnr</code> package. It has a general social scientists audience in mind and is suited for undergrad and graduate levels of study.</p>
<p>Webpage: <a href="https://scholar.harvard.edu/fatih-uenal/home" class="uri">https://scholar.harvard.edu/fatih-uenal/home</a></p>
</div>
<hr />
</div>
<div id="section-essential-course-information" class="section level3">
<h3>Essential Course Information</h3>
<div id="section-course-objectives" class="section level4">
<h4><strong>Course Objectives</strong></h4>
<p>“Data science” is a catch-all term used to describe the practice of working with and analyzing messy data sources to draw meaningful conclusions using techniques developed by computer scientists and computational statisticians. The purpose of this course is to give students who are training as quantitative social scientists a broad introduction to this skillset via the statistical programming language, <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>. You will learn how to conduct many statistical analyses such as univariate statistics (e.g., ANOVA, correlation, regression) in <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> that you may have already done in SPSS, Excel, or another such program. Additionally, we will build on this foundation to explore new skillsets uncommon in the social sciences, such as natural language processing, automated data curation, and machine learning.</p>
<p>At the end of this course you will be able to:</p>
<ul>
<li><p>To answer research questions in Social Sciences (e.g., Psychology) with data</p></li>
<li><p>Understand the basics of research designs in Social Sciences, and how they relate to data-analysis strategies</p></li>
<li><p>Develop an intuitive, practical, and conceptual understanding of strategies for asking and answering questions with data</p></li>
<li><p>To use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>, a free and open-source statistics software program.</p></li>
<li><p>Develop a basic understanding of frequently used Data Science Techniques.</p></li>
<li><p>Practice your newly acquired skills with interesting, interactive, and fun projects.</p></li>
</ul>
<p><strong>NOTE</strong>: The schedule and procedures described in this syllabus are subject to change depending on specific needs and requirements. You will always be notified of changes on the homepage (see “last update”).</p>
</div>
<div id="section-course-structure" class="section level4">
<h4><strong>Course Structure</strong></h4>
<p>This is the first module in a series of a 8 week-intensive course. and I suggest that you devote approx 10 hours a week to learning <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>, or if you are teaching graduate students, I’d recommend adopting the schedule below, which is designed for an intense but doable semester-long course, one module per week. It is intended to take the average graduate student roughly 10 hours per week to complete all required tasks.However, some number of students will find programming to be more challenging and may take up to 15 hours per week. Some will breeze through the material in 5.</p>
</div>
<div id="section-grading" class="section level4">
<h4><strong>Grading</strong></h4>
<p>Each Monday, lessons will be assigned from datacamp.com. Some of these lessons will be complete DataCamp courses, and others will be specific modules of courses. This will all be managed by assigning content to your (free) DataCamp account. The amount of content assigned will vary between one and two courses of content. DataCamp considers a course to be roughly 4 hours of lessons, which includes practice time. Realistically, the time you need will depend upon how intuitive you find <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to be. For students already familiar with other programming languages and those with previous <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> experience, “8 hours” of courses is realistically closer to 2 hours; for complete novices that also find the material difficult, 8 hours is a realistic estimate. It is strongly recommended that you stretch out DataCamp lessons across the assignment period, for example, allocating 1 hour each day. You will gain the most by treating this as a foreign language immersion course by using R every day, including for your own research. Remember that you can always go to the <strong>Slack Group</strong> for help.</p>
</div>
<div id="section-passing-rate" class="section level4">
<h4><strong>Passing Rate</strong></h4>
<p>The passing rate is 70%.</p>
</div>
</div>
<div id="section-pre-course-survey" class="section level3">
<h3>Pre-Course Survey</h3>
<p>Insert Survey Link here</p>
<p><em>If you cannot see the survey above, click this link to access it in a new window.</em></p>
</div>
</div>
<div id="section-section-1-introduction-to-machine-learning" class="section level2">
<h2>Section 1: Introduction to Machine Learning</h2>
<p>In the <strong>Introduction to Machine Learning</strong> section, you will be introduced to machine learning.</p>
<p>After completing this section, you will be able to:</p>
<ul>
<li><p>Explain the difference between the <strong>outcome</strong> and the <strong>features</strong>.</p></li>
<li><p>Explain when to use <strong>classification</strong> and when to use <strong>prediction</strong>.</p></li>
<li><p>Explain the importance of <strong>prevalence</strong>.</p></li>
<li><p>Explain the difference between <strong>sensitivity</strong> and <strong>specificity</strong>.</p></li>
</ul>
<p>This section has one part: <strong>introduction to machine learning</strong>. There are comprehension checks at the end.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your own learning.</p>
<div id="section-introduction-and-motivation" class="section level3">
<h3>Introduction and Motivation</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html">textbook section - 27 Introduction to machine learning</a>.</p>
</div>
<p>Perhaps the most popular data science methodologies come from the field of <em>machine learning</em>. Machine learning success stories include the handwritten zip code readers implemented by the postal service, speech recognition technology such as Apple’s Siri, movie recommendation systems, spam and malware detectors, housing price predictors, and driverless cars. Although today Artificial Intelligence and machine learning are often used interchangeably, we make the following distinction: while the first artificial intelligence algorithms, such as those used by chess playing machines, implemented decision making based on programmable rules derived from theory or first principles, in machine learning decisions are based on algorithms <strong>built with data</strong>.</p>
</div>
<div id="section-notation" class="section level3">
<h3>Notation</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#notation-1">textbook section - 27.1 Notation</a>.</p>
</div>
<p>In machine learning, data comes in the form of:</p>
<p>1. the <em>outcome</em> we want to predict and</p>
<p>2. the <em>features</em> that we will use to predict the outcome</p>
<p>We want to build an algorithm that takes feature values as input and returns a prediction for the outcome when we don’t know the outcome. The machine learning approach is to <em>train</em> an algorithm using a dataset for which we do know the outcome, and then apply this algorithm in the future to make a prediction when we don’t know the outcome.</p>
<p>Here we will use <span class="math inline">\(Y\)</span> to denote the outcome and <span class="math inline">\(X_1, \dots, X_p\)</span> to denote features. Note that features are sometimes referred to as predictors or covariates. We consider all these to be synonyms.</p>
<p>Prediction problems can be divided into categorical and continuous outcomes. For categorical outcomes, <span class="math inline">\(Y\)</span> can be any one of <span class="math inline">\(K\)</span> classes. The number of classes can vary greatly across applications. For example, in the digit reader data, <span class="math inline">\(K=10\)</span> with the classes being the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. In speech recognition, the outcomes are all possible words or phrases we are trying to detect. Spam detection has two outcomes: spam or not spam. In this course, we denote the <span class="math inline">\(K\)</span> categories with indexes <span class="math inline">\(k=1,\dots,K\)</span>. However, for binary data we will use <span class="math inline">\(k=0,1\)</span> for mathematical conveniences that we demonstrate later.</p>
<p>The general setup is as follows. We have a series of features and an unknown outcome we want to predict:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
outcome
</th>
<th style="text-align:center;">
feature 1
</th>
<th style="text-align:center;">
feature 2
</th>
<th style="text-align:center;">
feature 3
</th>
<th style="text-align:center;">
feature 4
</th>
<th style="text-align:center;">
feature 5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
?
</td>
<td style="text-align:center;">
<span class="math inline">\(X_1\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_2\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_3\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_4\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_5\)</span>
</td>
</tr>
</tbody>
</table>
<p>To <em>build a model</em> that provides a prediction for any set of observed values <span class="math inline">\(X_1=x_1, X_2=x_2, \dots X_5=x_5\)</span>, we collect data for which we know the outcome:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
outcome
</th>
<th style="text-align:left;">
feature 1
</th>
<th style="text-align:left;">
feature 2
</th>
<th style="text-align:left;">
feature 3
</th>
<th style="text-align:left;">
feature 4
</th>
<th style="text-align:left;">
feature 5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_{1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,3}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,5}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_{2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,3}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,5}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_n\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,3}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,5}\)</span>
</td>
</tr>
</tbody>
</table>
<p>When the output is continuous we refer to the machine learning task as <em>prediction</em>, and the main output of the model is a function <span class="math inline">\(f\)</span> that automatically produces a prediction, denoted with <span class="math inline">\(\hat{y}\)</span>, for any set of predictors: <span class="math inline">\(\hat{y} = f(x_1, x_2, \dots, x_p)\)</span>. We use the term <em>actual outcome</em> to denote what we ended up observing. So we want the prediction <span class="math inline">\(\hat{y}\)</span> to match the actual outcome <span class="math inline">\(y\)</span> as well as possible. Because our outcome is continuous, our predictions <span class="math inline">\(\hat{y}\)</span> will not be either exactly right or wrong, but instead we will determine an <em>error</em> defined as the difference between the prediction and the actual outcome <span class="math inline">\(y - \hat{y}\)</span>.</p>
<p>When the outcome is categorical, we refer to the machine learning task as <em>classification</em>, and the main output of the model will be a <em>decision rule</em> which prescribes which of the <span class="math inline">\(K\)</span> classes we should predict. In this scenario, most models provide functions of the predictors for each class <span class="math inline">\(k\)</span>, <span class="math inline">\(f_k(x_1, x_2, \dots, x_p)\)</span>, that are used to make this decision. When the data is binary a typical decision rules looks like this: if <span class="math inline">\(f_1(x_1, x_2, \dots, x_p) &gt; C\)</span>, predict category 1, if not the other category, with <span class="math inline">\(C\)</span> a predetermined cutoff. Because the outcomes are categorical, our predictions will be either right or wrong.</p>
<p>Notice that these terms vary among courses, text books, and other publications. Often <em>prediction</em> is used for both categorical and continuous outcomes, and the term <em>regression</em> can be used for the continuous case. Here we avoid using <em>regression</em> to avoid confusion with our previous use of the term <em>linear regression</em>. In most cases it will be clear if our outcomes are categorical or continuous, so we will avoid using these terms when possible.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p><span class="math inline">\(x_1,\dots, x_p\)</span> denote the features, <span class="math inline">\(Y\)</span> denotes the outcomes, and <span class="math inline">\(\hat{Y}\)</span> denotes the predictions.</p></li>
<li><p>Machine learning prediction tasks can be divided into <strong>categorical</strong> and <strong>continuous</strong> outcomes. We refer to these as <strong>classification</strong> and <strong>prediction</strong>, respectively.</p></li>
</ul>
</div>
</div>
<div id="section-an-example" class="section level3">
<h3>An Example</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#an-example">textbook section - 27.2 An Example</a>.</p>
</div>
<p>Let’s consider the zip code reader example. The first step in handling mail received in the post office is sorting letters by zip code:</p>
<p><img src="images/how-to-write-a-address-on-an-envelope-how-to-write-the-address-on-an-envelope-write-address-on-envelope-india-finishedenvelope-x69070.png" width="40%" /></p>
<p>Originally, humans had to sort these by hand. To do this, they had to read the zip codes on each letter. Today, thanks to machine learning algorithms, a computer can read zip codes and then a robot sorts the letters. In this part of the course, we will learn how to build algorithms that can read a digit.</p>
<p>The first step in building an algorithm is to understand what are the outcomes and features. Below are three images of written digits. These have already been read by a human and assigned an outcome <span class="math inline">\(Y\)</span>. These are considered known and serve as the training set.</p>
<p><img src="datsci_08_files/figure-html/digit-images-example-1.png" width="624" /></p>
<p>The images are converted into <span class="math inline">\(28 \times 28 = 784\)</span> pixels and, for each pixel, we obtain a grey scale intensity between 0 (white) and 255 (black), which we consider continuous for now. The following plot shows the individual features for each image:</p>
<p><img src="datsci_08_files/figure-html/example-images-1.png" width="624" /></p>
<p>For each digitized image <span class="math inline">\(i\)</span>, we have a categorical outcome <span class="math inline">\(Y_i\)</span> which can be one of 10 values (<span class="math inline">\(0,1,2,3,4,5,6,7,8,9\)</span>), and features <span class="math inline">\(X_{i,1}, \dots, X_{i,784}\)</span>. We use bold face <span class="math inline">\(\mathbf{X}_i = (X_{i,1}, \dots, X_{i,784})\)</span> to distinguish the vector of predictors from the individual predictors. When referring to an arbitrary set of features rather than a specific image in our dataset, we drop the index <span class="math inline">\(i\)</span> and use <span class="math inline">\(Y\)</span> and <span class="math inline">\(\mathbf{X} = (X_{1}, \dots, X_{784})\)</span>. We use upper case variables because, in general, we think of the predictors as random variables. We use lower case, for example <span class="math inline">\(\mathbf{X} = \mathbf{x}\)</span>, to denote observed values. When we code we stick to lower case.</p>
<p>The machine learning task is to build an algorithm that returns a prediction for any of the possible values of the features. Here, we will learn several approaches to building these algorithms. Although at this point it might seem impossible to achieve this, we will start with simple examples and build up our knowledge until we can attack more complex ones. In fact, we start with an artificially simple example with just one predictor and then move on to a slightly more realistic example with two predictors. Once we understand these, we will attack real-world machine learning challenges involving many predictors.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p><span class="math inline">\(Y_i\)</span> = an outcome for observation or index i.</p></li>
<li><p>We use boldface for <span class="math inline">\(X_i\)</span> to distinguish the vector of predictors from the individual predictors <span class="math inline">\(X_{i,1}, \dots, X_{i,784}\)</span>.</p></li>
<li><p>When referring to an arbitrary set of features and outcomes, we drop the index i and use <span class="math inline">\(Y\)</span> and bold <span class="math inline">\(X\)</span>.</p></li>
<li><p>Uppercase is used to refer to variables because we think of predictors as random variables.</p></li>
<li><p>Lowercase is used to denote observed values. For example, <span class="math inline">\(X = x\)</span>.</p></li>
</ul>
</div>
</div>
<div id="section-comprehension-check-introduction-to-machine-learning" class="section level3">
<h3>1.1 Comprehension Check: Introduction to Machine Learning</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-2-machine-learning-basics" class="section level2">
<h2>Section 2: Machine Learning Basics</h2>
<p>In the <strong>Machine Learning Basics</strong> section, you will learn the basics of machine learning.</p>
<p>After completing this section, you will be able to:</p>
<ul>
<li><p>Start to use the <strong>caret</strong> package.</p></li>
<li><p>Construct and interpret a <strong>confusion matrix</strong>.</p></li>
<li><p>Use <strong>conditional probabilities</strong> in the context of machine learning.</p></li>
</ul>
<p>This section has two parts: <strong>basics of evaluating machine learning algorithms</strong> and <strong>conditional probabilities</strong>. There are comprehension checks at the end of each part.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your own learning.</p>
<div id="section-caret-package-training-and-test-sets-and-overall-accuracy" class="section level3">
<h3>Caret package, training and test sets, and overall accuracy</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#evaluation-metrics">textbook section - 27.4 Evaluation metrics</a>.</p>
</div>
<p>Before we start describing approaches to optimize the way we build algorithms, we first need to define what we mean when we say one approach is better than another. In this section, we focus on describing ways in which machine learning algorithms are evaluated. Specifically, we need to quantify what we mean by “better”.</p>
<p>For our first introduction to machine learning concepts, we will start with a boring and simple example: how to predict sex using height. As we explain machine learning step by step, this example will let us set down the first building block. Soon enough, we will be attacking more interesting challenges. We use the <strong>caret</strong> package, which has several useful functions for building and assessing machine learning methods and we introduce in more detail in the textbook <a href="https://rafalab.github.io/dsbook/caret.html">(Section - 30 The caret package)</a>.</p>
<pre class="r"><code>library(tidyverse)
library(caret)</code></pre>
<p>For a first example, we use the height data in dslabs:</p>
<pre class="r"><code>library(dslabs)
data(heights)</code></pre>
<p>We start by defining the outcome and predictors.</p>
<pre class="r"><code>y &lt;- heights$sex
x &lt;- heights$height</code></pre>
<p>In this case, we have only one predictor, height, and <code>y</code> is clearly a categorical outcome since observed values are either <code>Male</code> or <code>Female</code>. We know that we will not be able to predict <span class="math inline">\(Y\)</span> very accurately based on <span class="math inline">\(X\)</span> because male and female average heights are not that different relative to within group variability. But can we do better than guessing? To answer this question, we need a quantitative definition of better.</p>
</div>
<div id="section-training-and-test-sets" class="section level3">
<h3>Training and test sets</h3>
<p>Ultimately, a machine learning algorithm is evaluated on how it performs in the real world with completely new datasets. However, when developing an algorithm, we usually have a dataset for which we know the outcomes, as we do with the heights: we know the sex of every student in our dataset. Therefore, to mimic the ultimate evaluation process, we typically split the data into two parts and act as if we don’t know the outcome for one of these. We stop pretending we don’t know the outcome to evaluate the algorithm, but only <em>after</em> we are done constructing it. We refer to the group for which we know the outcome, and use to develop the algorithm, as the <em>training</em> set. We refer to the group for which we pretend we don’t know the outcome as the <em>test</em> set.</p>
<p>A standard way of generating the training and test sets is by randomly splitting the data. The <strong>caret</strong> package includes the function <code>createDataPartition</code> that helps us generates indexes for randomly splitting the data into training and test sets:</p>
<pre class="r"><code>set.seed(2007)
test_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)</code></pre>
<p>The argument <code>times</code> is used to define how many random samples of indexes to return, the argument <code>p</code> is used to define what proportion of the data is represented by the index, and the argument <code>list</code> is used to decide if we want the indexes returned as a list or not. We can use the result of the <code>createDataPartition</code> function call to define the training and test sets like this:</p>
<pre class="r"><code>test_set &lt;- heights[test_index, ]
train_set &lt;- heights[-test_index, ]</code></pre>
<p>We will now develop an algorithm using <strong>only</strong> the training set. Once we are done developing the algorithm, we will <em>freeze</em> it and evaluate it using the test set. The simplest way to evaluate the algorithm when the outcomes are categorical is by simply reporting the proportion of cases that were correctly predicted <strong>in the test set</strong>. This metric is usually referred to as <em>overall accuracy</em>.</p>
</div>
<div id="section-overall-accuracy" class="section level3">
<h3>Overall accuracy</h3>
<p>To demonstrate the use of overall accuracy, we will build two competing algorithms and compare them.</p>
<p>Let’s start by developing the simplest possible machine algorithm: guessing the outcome.</p>
<pre class="r"><code>y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), length(test_index), replace = TRUE)</code></pre>
<p>Note that we are completely ignoring the predictor and simply guessing the sex.</p>
<p>In machine learning applications, it is useful to use factors to represent the categorical outcomes because R functions developed for machine learning, such as those in the <strong>caret</strong> package, require or recommend that categorical outcomes be coded as factors. So convert <code>y_hat</code> to factors using the <code>factor</code> function:</p>
<pre class="r"><code>y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), length(test_index), replace = TRUE) %&gt;%
  factor(levels = levels(test_set$sex))</code></pre>
<p>The <em>overall accuracy</em> is simply defined as the overall proportion that is predicted correctly:</p>
<pre class="r"><code>mean(y_hat == test_set$sex)</code></pre>
<pre><code>## [1] 0.5104762</code></pre>
<p>Not surprisingly, our accuracy is about 50%. We are guessing!</p>
<p>Can we do better? Exploratory data analysis suggests we can because, on average, males are slightly taller than females:</p>
<pre class="r"><code>heights %&gt;% group_by(sex) %&gt;% summarize(mean(height), sd(height))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sex"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["mean(height)"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["sd(height)"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"Female","2":"64.93942","3":"3.760656"},{"1":"Male","2":"69.31475","3":"3.611024"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>But how do we make use of this insight? Let’s try another simple approach: predict <code>Male</code> if height is within two standard deviations from the average male:</p>
<pre class="r"><code>y_hat &lt;- ifelse(x &gt; 62, &quot;Male&quot;, &quot;Female&quot;) %&gt;% 
  factor(levels = levels(test_set$sex))</code></pre>
<p>The accuracy goes up from 0.50 to about 0.80:</p>
<pre class="r"><code>mean(y == y_hat)</code></pre>
<pre><code>## [1] 0.7933333</code></pre>
<p>But can we do even better? In the example above, we used a cutoff of 62, but we can examine the accuracy obtained for other cutoffs and then pick the value that provides the best results. But remember, <strong>it is important that we optimize the cutoff using only the training set</strong>: the test set is only for evaluation. Although for this simplistic example it is not much of a problem, later we will learn that evaluating an algorithm on the training set can lead to <em>overfitting</em>, which often results in dangerously over-optimistic assessments.</p>
<p>Here we examine the accuracy of 10 different cutoffs and pick the one yielding the best result:</p>
<pre class="r"><code>cutoff &lt;- seq(61, 70)
accuracy &lt;- map_dbl(cutoff, function(x){
  y_hat &lt;- ifelse(train_set$height &gt; x, &quot;Male&quot;, &quot;Female&quot;) %&gt;% 
    factor(levels = levels(test_set$sex))
  mean(y_hat == train_set$sex)
})</code></pre>
<p>We can make a plot showing the accuracy obtained on the training set for males and females:</p>
<p><img src="datsci_08_files/figure-html/accuracy-vs-cutoff-1.png" width="624" /></p>
<p>We see that the maximum value is:</p>
<pre class="r"><code>max(accuracy)</code></pre>
<pre><code>## [1] 0.8495238</code></pre>
<p>which is much higher than 0.5. The cutoff resulting in this accuracy is:</p>
<pre class="r"><code>best_cutoff &lt;- cutoff[which.max(accuracy)]
best_cutoff</code></pre>
<pre><code>## [1] 64</code></pre>
<p>We can now test this cutoff on our test set to make sure our accuracy is not overly optimistic:</p>
<pre class="r"><code>y_hat &lt;- ifelse(test_set$height &gt; best_cutoff, &quot;Male&quot;, &quot;Female&quot;) %&gt;% 
  factor(levels = levels(test_set$sex))
y_hat &lt;- factor(y_hat)
mean(y_hat == test_set$sex)</code></pre>
<pre><code>## [1] 0.8038095</code></pre>
<p>We see that it is a bit lower than the accuracy observed for the training set, but it is still better than guessing. And by testing on a dataset that we did not train on, we know our result is not due to cherry-picking a good result.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Note: the <code>set.seed()</code> function is used to obtain reproducible results. If you have R 3.6 or later, please use the <code>sample.kind = "Rounding"</code> argument whenever you set the seed for this course.</p></li>
<li><p>To mimic the ultimate evaluation process, we randomly split our data into two — a training set and a test set — and act as if we don’t know the outcome of the test set. We develop algorithms using only the training set; the test set is used only for evaluation.</p></li>
<li><p>The <code>createDataPartition()</code> function from the <strong>caret</strong> package can be used to generate indexes for randomly splitting data.</p></li>
<li><p>Note: contrary to what the documentation says, this course will use the argument p as the percentage of data that goes to testing. The indexes made from <code>createDataPartition()</code> should be used to create the test set. Indexes should be created on the outcome and not a predictor.</p></li>
<li><p>The simplest evaluation metric for categorical outcomes is overall accuracy: the proportion of cases that were correctly predicted in the test set.</p></li>
</ul>
</div>
</div>
<div id="section-comprehension-check-basics-of-evaluating-machine-learning-algorithms" class="section level3">
<h3>2.1 Comprehension Check: Basics of Evaluating Machine Learning Algorithms</h3>
<p>Insert assessment here</p>
</div>
<div id="section-confusion-matrix" class="section level3">
<h3>Confusion matrix</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#the-confusion-matrix">textbook section - 27.4.3 The confusion matrix</a>.</p>
</div>
<p>The prediction rule we developed in the previous section predicts <code>Male</code> if the student is taller than 64 inches. Given that the average female is about 64 inches, this prediction rule seems wrong. What happened? If a student is the height of the average female, shouldn’t we predict <code>Female</code>?</p>
<p>Generally speaking, overall accuracy can be a deceptive measure. To see this, we will start by constructing what is referred to as the <em>confusion matrix</em>, which basically tabulates each combination of prediction and actual value. We can do this in R using the function <code>table</code>:</p>
<pre class="r"><code>table(predicted = y_hat, actual = test_set$sex)</code></pre>
<pre><code>##          actual
## predicted Female Male
##    Female     48   32
##    Male       71  374</code></pre>
<p>If we study this table closely, it reveals a problem. If we compute the accuracy separately for each sex, we get:</p>
<pre class="r"><code>test_set %&gt;% 
  mutate(y_hat = y_hat) %&gt;%
  group_by(sex) %&gt;% 
  summarize(accuracy = mean(y_hat == sex))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sex"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["accuracy"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"Female","2":"0.4033613"},{"1":"Male","2":"0.9211823"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>There is an imbalance in the accuracy for males and females: too many females are predicted to be male. We are calling almost half of the females male! How can our overall accuracy be so high then? This is because the <em>prevalence</em> of males in this dataset is high. These heights were collected from three data sciences courses, two of which had more males enrolled:</p>
<pre class="r"><code>prev &lt;- mean(y == &quot;Male&quot;)
prev</code></pre>
<pre><code>## [1] 0.7733333</code></pre>
<p>So when computing overall accuracy, the high percentage of mistakes made for females is outweighed by the gains in correct calls for men. <strong>This can actually be a big problem in machine learning.</strong> If your training data is biased in some way, you are likely to develop algorithms that are biased as well. The fact that we used a test set does not matter because it is also derived from the original biased dataset. This is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm.</p>
<p>There are several metrics that we can use to evaluate an algorithm in a way that prevalence does not cloud our assessment, and these can all be derived from the confusion matrix. A general improvement to using overall accuracy is to study <em>sensitivity</em> and <em>specificity</em> separately.</p>
</div>
<div id="section-sensitivity-and-specificity" class="section level3">
<h3>Sensitivity and specificity</h3>
<p>To define sensitivity and specificity, we need a binary outcome. When the outcomes are categorical, we can define these terms for a specific category. In the digits example, we can ask for the specificity in the case of correctly predicting 2 as opposed to some other digit. Once we specify a category of interest, then we can talk about positive outcomes, <span class="math inline">\(Y=1\)</span>, and negative outcomes, <span class="math inline">\(Y=0\)</span>.</p>
<p>In general, <em>sensitivity</em> is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive: <span class="math inline">\(\hat{Y}=1\)</span> when <span class="math inline">\(Y=1\)</span>. Because an algorithm that calls everything positive (<span class="math inline">\(\hat{Y}=1\)</span> no matter what) has perfect sensitivity, this metric on its own is not enough to judge an algorithm. For this reason, we also examine <em>specificity</em>, which is generally defined as the ability of an algorithm to not predict a positive <span class="math inline">\(\hat{Y}=0\)</span> when the actual outcome is not a positive <span class="math inline">\(Y=0\)</span>. We can summarize in the following way:</p>
<ul>
<li>High sensitivity: <span class="math inline">\(Y=1 \implies \hat{Y}=1\)</span></li>
<li>High specificity: <span class="math inline">\(Y=0 \implies \hat{Y} = 0\)</span></li>
</ul>
<p>Although the above is often considered the definition of specificity, another way to think of specificity is by the proportion of positive calls that are actually positive:</p>
<ul>
<li>High specificity: <span class="math inline">\(\hat{Y}=1 \implies Y=1\)</span>.</li>
</ul>
<p>To provide precise definitions, we name the four entries of the confusion matrix:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Actually Positive
</th>
<th style="text-align:left;">
Actually Negative
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Predicted positive
</td>
<td style="text-align:left;">
True positives (TP)
</td>
<td style="text-align:left;">
False positives (FP)
</td>
</tr>
<tr>
<td style="text-align:left;">
Predicted negative
</td>
<td style="text-align:left;">
False negatives (FN)
</td>
<td style="text-align:left;">
True negatives (TN)
</td>
</tr>
</tbody>
</table>
<p>Sensitivity is typically quantified by <span class="math inline">\(TP/(TP+FN)\)</span>, the proportion of actual positives (the first column = <span class="math inline">\(TP+FN\)</span>) that are called positives (<span class="math inline">\(TP\)</span>). This quantity is referred to as the <em>true positive rate</em> (TPR) or <em>recall</em>.</p>
<p>Specificity is defined as <span class="math inline">\(TN/(TN+FP)\)</span> or the proportion of negatives (the second column = <span class="math inline">\(FP+TN\)</span>) that are called negatives (<span class="math inline">\(TN\)</span>). This quantity is also called the true negative rate (TNR). There is another way of quantifying specificity which is <span class="math inline">\(TP/(TP+FP)\)</span> or the proportion of outcomes called positives (the first row or <span class="math inline">\(TP+FP\)</span>) that are actually positives (<span class="math inline">\(TP\)</span>). This quantity is referred to as <em>positive predictive value (PPV)</em> and also as <em>precision</em>. Note that, unlike TPR and TNR, precision depends on prevalence since higher prevalence implies you can get higher precision even when guessing.</p>
<p>The multiple names can be confusing, so we include a table to help us remember the terms. The table includes a column that shows the definition if we think of the proportions as probabilities.</p>
<table>
<colgroup>
<col width="18%" />
<col width="10%" />
<col width="20%" />
<col width="16%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th>Measure of</th>
<th>Name 1</th>
<th>Name 2</th>
<th>Definition</th>
<th>Probability representation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sensitivity</td>
<td>TPR</td>
<td>Recall</td>
<td><span class="math inline">\(\frac{\mbox{TP}}{\mbox{TP} + \mbox{FN}}\)</span></td>
<td><span class="math inline">\(\mbox{Pr}(\hat{Y}=1 \mid Y=1)\)</span></td>
</tr>
<tr class="even">
<td>specificity</td>
<td>TNR</td>
<td>1-FPR</td>
<td><span class="math inline">\(\frac{\mbox{TN}}{\mbox{TN}+\mbox{FP}}\)</span></td>
<td><span class="math inline">\(\mbox{Pr}(\hat{Y}=0 \mid Y=0)\)</span></td>
</tr>
<tr class="odd">
<td>specificity</td>
<td>PPV</td>
<td>Precision</td>
<td><span class="math inline">\(\frac{\mbox{TP}}{\mbox{TP}+\mbox{FP}}\)</span></td>
<td><span class="math inline">\(\mbox{Pr}(Y=1 \mid \hat{Y}=1)\)</span></td>
</tr>
</tbody>
</table>
<p>Here TPR is True Positive Rate, FPR is False Positive Rate, and PPV is Positive Predictive Value. The <strong>caret</strong> function <code>confusionMatrix</code> computes all these metrics for us once we define what category “positive” is. The function expects factors as input, and the first level is considered the positive outcome or <span class="math inline">\(Y=1\)</span>. In our example, <code>Female</code> is the first level because it comes before <code>Male</code> alphabetically. If you type this into R you will see several metrics including accuracy, sensitivity, specificity, and PPV.</p>
<pre class="r"><code>cm &lt;- confusionMatrix(data = y_hat, reference = test_set$sex)</code></pre>
<p>You can acceess these directly, for example, like this:</p>
<pre class="r"><code>cm$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>##  Accuracy 
## 0.8038095</code></pre>
<pre class="r"><code>cm$byClass[c(&quot;Sensitivity&quot;,&quot;Specificity&quot;, &quot;Prevalence&quot;)]</code></pre>
<pre><code>## Sensitivity Specificity  Prevalence 
##   0.4033613   0.9211823   0.2266667</code></pre>
<p>We can see that the high overall accuracy is possible despite relatively low sensitivity. As we hinted at above, the reason this happens is because of the low prevalence (0.23): the proportion of females is low. Because prevalence is low, failing to predict actual females as females (low sensitivity) does not lower the accuracy as much as failing to predict actual males as males (low specificity). This is an example of why it is important to examine sensitivity and specificity and not just accuracy. Before applying this algorithm to general datasets, we need to ask ourselves if prevalence will be the same.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Overall accuracy can sometimes be a deceptive measure because of unbalanced classes.</p></li>
<li><p>A general improvement to using overall accuracy is to study sensitivity and specificity separately. <strong>Sensitivity</strong>, also known as the true positive rate or recall, is the proportion of actual positive outcomes correctly identified as such. <strong>Specificity</strong>, also known as the true negative rate, is the proportion of actual negative outcomes that are correctly identified as such.</p></li>
<li><p>A confusion matrix tabulates each combination of prediction and actual value. You can create a confusion matrix in R using the <code>table()</code> function or the <code>confusionMatrix()</code> function from the <strong>caret</strong> package.</p></li>
</ul>
</div>
</div>
<div id="section-balanced-accuracy-and-f_1-score" class="section level3">
<h3>Balanced accuracy and <span class="math inline">\(F_1\)</span> score</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#balanced-accuracy-and-f_1-score">textbook section - 27.4.5 Balanced accuracy and <span class="math inline">\(F_1\)</span> score</a>.</p>
</div>
<p>Although we usually recommend studying both specificity and sensitivity, very often it is useful to have a one-number summary, for example for optimization purposes. One metric that is preferred over overall accuracy is the average of specificity and sensitivity, referred to as <em>balanced accuracy</em>. Because specificity and sensitivity are rates, it is more appropriate to compute the <em>harmonic</em> average. In fact, the <em><span class="math inline">\(F_1\)</span>-score</em>, a widely used one-number summary, is the harmonic average of precision and recall:</p>
<p><span class="math display">\[
\frac{1}{\frac{1}{2}\left(\frac{1}{\mbox{recall}} + 
    \frac{1}{\mbox{precision}}\right) }
\]</span></p>
<p>Because it is easier to write, you often see this harmonic average rewritten as:</p>
<p><span class="math display">\[
2 \times \frac{\mbox{precision} \cdot \mbox{recall}}
{\mbox{precision} + \mbox{recall}}
\]</span></p>
<p>when defining <span class="math inline">\(F_1\)</span>.</p>
<p>Remember that, depending on the context, some types of errors are more costly than others. For example, in the case of plane safety, it is much more important to maximize sensitivity over specificity: failing to predict a plane will malfunction before it crashes is a much more costly error than grounding a plane when, in fact, the plane is in perfect condition. In a capital murder criminal case, the opposite is true since a false positive can lead to executing an innocent person. The <span class="math inline">\(F_1\)</span>-score can be adapted to weigh specificity and sensitivity differently. To do this, we define <span class="math inline">\(\beta\)</span> to represent how much more important sensitivity is compared to specificity and consider a weighted harmonic average:</p>
<p><span class="math display">\[
\frac{1}{\frac{\beta^2}{1+\beta^2}\frac{1}{\mbox{recall}} + 
    \frac{1}{1+\beta^2}\frac{1}{\mbox{precision}} }
\]</span></p>
<p>The <code>F_meas</code> function in the <strong>caret</strong> package computes this summary with <code>beta</code> defaulting to 1.</p>
<p>Let’s rebuild our prediction algorithm, but this time maximizing the F-score instead of overall accuracy:</p>
<pre class="r"><code>cutoff &lt;- seq(61, 70)
F_1 &lt;- map_dbl(cutoff, function(x){
  y_hat &lt;- ifelse(train_set$height &gt; x, &quot;Male&quot;, &quot;Female&quot;) %&gt;% 
    factor(levels = levels(test_set$sex))
  F_meas(data = y_hat, reference = factor(train_set$sex))
})</code></pre>
<p>As before, we can plot these <span class="math inline">\(F_1\)</span> measures versus the cutoffs:</p>
<p><img src="datsci_08_files/figure-html/f_1-vs-cutoff-1.png" width="624" /></p>
<p>We see that it is maximized at <span class="math inline">\(F_1\)</span> value of:</p>
<pre class="r"><code>max(F_1)</code></pre>
<pre><code>## [1] 0.6470588</code></pre>
<p>This maximum is achieved when we use the following cutoff:</p>
<pre class="r"><code>best_cutoff &lt;- cutoff[which.max(F_1)]
best_cutoff</code></pre>
<pre><code>## [1] 66</code></pre>
<p>A cutoff of 65 makes more sense than 64. Furthermore, it balances the specificity and sensitivity of our confusion matrix:</p>
<pre class="r"><code>y_hat &lt;- ifelse(test_set$height &gt; best_cutoff, &quot;Male&quot;, &quot;Female&quot;) %&gt;% 
  factor(levels = levels(test_set$sex))
sensitivity(data = y_hat, reference = test_set$sex)</code></pre>
<pre><code>## [1] 0.6302521</code></pre>
<pre class="r"><code>specificity(data = y_hat, reference = test_set$sex)</code></pre>
<pre><code>## [1] 0.8325123</code></pre>
<p>We now see that we do much better than guessing, that both sensitivity and specificity are relatively high, and that we have built our first machine learning algorithm. It takes height as a predictor and predicts female if you are 65 inches or shorter.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>For optimization purposes, sometimes it is more useful to have a one number summary than studying both specificity and sensitivity. One preferred metric is <strong>balanced accuracy</strong>. Because specificity and sensitivity are rates, it is more appropriate to compute the <em>harmonic average</em>. In fact, the <strong>F1-score</strong>, a widely used one-number summary, is the harmonic average of precision and recall.</p></li>
<li><p>Depending on the context, some type of errors are more costly than others. The <strong>F1-score</strong> can be adapted to weigh specificity and sensitivity differently.</p></li>
<li><p>You can compute the <strong>F1-score</strong> using the <code>F_meas()</code> function in the <strong>caret</strong> package.</p></li>
</ul>
</div>
</div>
<div id="section-prevalence-matters-in-practice" class="section level3">
<h3>Prevalence matters in practice</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#prevalence-matters-in-practice">textbook section - 27.4.6 Prevalence matters in practice</a>.</p>
</div>
<p>A machine learning algorithm with very high sensitivity and specificity may not be useful in practice when prevalence is close to either 0 or 1. To see this, consider the case of a doctor that specializes in a rare disease and is interested in developing an algorithm for predicting who has the disease. The doctor shares data with you and you then develop an algorithm with very high sensitivity. You explain that this means that if a patient has the disease, the algorithm is very likely to predict correctly. You also tell the doctor that you are also concerned because, based on the dataset you analyzed, 1/2 the patients have the disease: <span class="math inline">\(\mbox{Pr}(\hat{Y}=1)\)</span>. The doctor is neither concerned nor impressed and explains that what is important is the precision of the test: <span class="math inline">\(\mbox{Pr}(Y=1 | \hat{Y}=1)\)</span>. Using Bayes theorem, we can connect the two measures:</p>
<p><span class="math display">\[ \mbox{Pr}(Y = 1\mid \hat{Y}=1) = \mbox{Pr}(\hat{Y}=1 \mid Y=1) \frac{\mbox{Pr}(Y=1)}{\mbox{Pr}(\hat{Y}=1)}\]</span></p>
<p>The doctor knows that the prevalence of the disease is 5 in 1,000, which implies that <span class="math inline">\(\mbox{Pr}(Y=1) \, / \,\mbox{Pr}(\hat{Y}=1) = 1/100\)</span> and therefore the precision of your algorithm is less than 0.01. The doctor does not have much use for your algorithm.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>A machine learning algorithm with very high sensitivity and specificity may not be useful in practice when prevalence is close to either 0 or 1. For example, if you develop an algorithm for disease diagnosis with very high sensitivity, but the prevalence of the disease is pretty low, then the precision of your algorithm is probably very low based on Bayes’ theorem.</li>
</ul>
</div>
</div>
<div id="section-roc-and-precision-recall-curves" class="section level3">
<h3>ROC and precision-recall curves</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#roc-and-precision-recall-curves">textbook section - 27.4.7 ROC and precision-recall curves</a>.</p>
</div>
<p>When comparing the two methods (guessing versus using a height cutoff), we looked at accuracy and <span class="math inline">\(F_1\)</span>. The second method clearly outperformed the first. However, while we considered several cutoffs for the second method, for the first we only considered one approach: guessing with equal probability. Note that guessing <code>Male</code> with higher probability would give us higher accuracy due to the bias in the sample:</p>
<pre class="r"><code>p &lt;- 0.9
n &lt;- length(test_index)
y_hat &lt;- sample(c(&quot;Male&quot;, &quot;Female&quot;), n, replace = TRUE, prob=c(p, 1-p)) %&gt;% 
  factor(levels = levels(test_set$sex))
mean(y_hat == test_set$sex)</code></pre>
<pre><code>## [1] 0.7390476</code></pre>
<p>But, as described above, this would come at the cost of lower sensitivity. The curves we describe in this section will help us see this.</p>
<p>Remember that for each of these parameters, we can get a different sensitivity and specificity. For this reason, a very common approach to evaluating methods is to compare them graphically by plotting both.</p>
<p>A widely used plot that does this is the <em>receiver operating characteristic</em> (ROC) curve. If you are wondering where this name comes from, you can consult the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC Wikipedia page</a>.</p>
<p>The ROC curve plots sensitivity (TPR) versus 1 - specificity or the false positive rate (FPR). Here we compute the TPR and FPR needed for different probabilities of guessing male:</p>
<pre class="r"><code>probs &lt;- seq(0, 1, length.out = 10)
guessing &lt;- map_df(probs, function(p){
  y_hat &lt;- 
    sample(c(&quot;Male&quot;, &quot;Female&quot;), n, replace = TRUE, prob=c(p, 1-p)) %&gt;% 
    factor(levels = c(&quot;Female&quot;, &quot;Male&quot;))
  list(method = &quot;Guessing&quot;,
       FPR = 1 - specificity(y_hat, test_set$sex),
       TPR = sensitivity(y_hat, test_set$sex))
})</code></pre>
<p>We can use similar code to compute these values for our our second approach. By plotting both curves together, we are able to compare sensitivity for different values of specificity:</p>
<!--We can construct an ROC curve for the height-based approach:-->
<!--
<img src="datsci_08_files/figure-html/roc-2-1.png" width="624" />
-->
<p><img src="datsci_08_files/figure-html/roc-3-1.png" width="576" /></p>
<p>We can see that we obtain higher sensitivity with this approach for all values of specificity, which implies it is in fact a better method. Note that ROC curves for guessing always fall on the identiy line. Also note that when making ROC curves, it is often nice to add the cutoff associated with each point.</p>
<p>The packages <strong>pROC</strong> and <strong>plotROC</strong> are useful for generating these plots.</p>
<p>ROC curves have one weakness and it is that neither of the measures plotted depends on prevalence. In cases in which prevalence matters, we may instead make a precision-recall plot. The idea is similar, but we instead plot precision against recall:</p>
<p><img src="datsci_08_files/figure-html/precision-recall-1-1.png" width="624" /></p>
<p>From this plot we immediately see that the precision of guessing is not high. This is because the prevalence is low. We also see that if we change positives to mean Male instead of Female, the ROC curve remains the same, but the precision recall plot changes.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>A very common approach to evaluating accuracy and F1-score is to compare them graphically by plotting both. A widely used plot that does this is the <strong>receiver operating characteristic (ROC) curve</strong>. The ROC curve plots sensitivity (TPR) versus 1 - specificity or the false positive rate (FPR).</p></li>
<li><p>However, ROC curves have one weakness and it is that neither of the measures plotted depend on prevalence. In cases in which prevalence matters, we may instead make a <strong>precision-recall plot</strong>, which has a similar idea with ROC curve.</p></li>
</ul>
</div>
</div>
<div id="section-comprehension-check-practice-with-machine-learning-part-1" class="section level3">
<h3>2.1 Comprehension Check: Practice with Machine Learning, Part 1</h3>
<p>Insert assessment here</p>
</div>
<div id="section-comprehension-check-practice-with-machine-learning-part-2" class="section level3">
<h3>2.1 Comprehension Check: Practice with Machine Learning, Part 2</h3>
<p>Insert assessment here</p>
</div>
<div id="section-conditional-probabilities-and-expectations" class="section level3">
<h3>Conditional probabilities and expectations</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#conditional-probabilities-and-expectations">textbook section - 27.6 Conditional probabilities and expectations</a>.</p>
</div>
<p>In machine learning applications, we rarely can predict outcomes perfectly. For example, spam detectors often miss emails that are clearly spam, Siri often misunderstands the words we are saying, and your bank at times thinks your card was stolen when it was not. The most common reason for not being able to build perfect algorithms is that it is impossible. To see this, note that most datasets will include groups of observations with the same exact observed values for all predictors, but with different outcomes. Because our prediction rules are functions, equal inputs (the predictors) implies equal outputs (the predictions). Therefore, for a challenge in which the same predictors are associated with different outcomes across different individual observations, it is impossible to predict correctly for all these cases. We saw a simple example of this in the previous section: for any given height <span class="math inline">\(x\)</span>, you will have both males and females that are <span class="math inline">\(x\)</span> inches tall.</p>
<p>However, none of this means that we can’t build useful algorithms that are much better than guessing, and in some cases better than expert opinions. To achieve this in an optimal way, we make use of probabilistic representations of the problem based on the ideas presented in the textbook <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#conditional-expectations">(Section - 27.6.2 Conditional expectations)</a>. Observations with the same observed values for the predictors may not all be the same, but we can assume that they all have the same probability of this class or that class. We will write this idea out mathematically for the case of categorical data.</p>
</div>
<div id="section-conditional-probabilities" class="section level3">
<h3>Conditional probabilities</h3>
<p>We use the notation <span class="math inline">\((X_1 = x_1,\dots,X_p=x_p)\)</span> to represent the fact that we have observed values <span class="math inline">\(x_1,\dots,x_p\)</span> for covariates <span class="math inline">\(X_1, \dots, X_p\)</span>. This does not imply that the outcome <span class="math inline">\(Y\)</span> will take a specific value. Instead, it implies a specific probability. In particular, we denote the <em>conditional probabilities</em> for each class <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
\mbox{Pr}(Y=k \mid X_1 = x_1,\dots,X_p=x_p), \, \mbox{for}\,k=1,\dots,K
\]</span></p>
<p>To avoid writing out all the predictors, we will use the bold letters like this: <span class="math inline">\(\mathbf{X} \equiv (X_1,\dots,X_p)\)</span> and <span class="math inline">\(\mathbf{x} \equiv (x_1,\dots,x_p)\)</span>. We will also use the following notation for the conditional probability of being class <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
p_k(\mathbf{x}) = \mbox{Pr}(Y=k \mid \mathbf{X}=\mathbf{x}), \, \mbox{for}\, k=1,\dots,K
\]</span></p>
<p>Note: We will be using the <span class="math inline">\(p(x)\)</span> notation to represent conditional probabilities as functions of the predictors. Do not confuse it with the <span class="math inline">\(p\)</span> that represents the number of predictors.</p>
<p>These probabilities guide the construction of an algorithm that makes the best prediction: for any given <span class="math inline">\(\mathbf{x}\)</span>, we will predict the class <span class="math inline">\(k\)</span> with the largest probability among <span class="math inline">\(p_1(x), p_2(x), \dots p_K(x)\)</span>. In mathematical notation, we write it like this: <span class="math inline">\(\hat{Y} = \max_k p_k(\mathbf{x})\)</span>.</p>
<p>In machine learning, we refer to this as <em>Bayes’ Rule</em>. But keep in mind that this is a theoretical rule since in practice we don’t know <span class="math inline">\(p_k(\mathbf{x}), k=1,\dots,K\)</span>. In fact, estimating these conditional probabilities can be thought of as the main challenge of machine learning. The better our probability estimates <span class="math inline">\(\hat{p}_k(\mathbf{x})\)</span>, the better our predictor:</p>
<p><span class="math display">\[\hat{Y} = \max_k \hat{p}_k(\mathbf{x})\]</span></p>
<p>So what we will predict depends on two things: 1) how close are the <span class="math inline">\(\max_k p_k(\mathbf{x})\)</span> to 1 or 0 (perfect certainty) and 2) how close our estimates <span class="math inline">\(\hat{p}_k(\mathbf{x})\)</span> are to <span class="math inline">\(p_k(\mathbf{x})\)</span>. We can’t do anything about the first restriction as it is determined by the nature of the problem, so our energy goes into finding ways to best estimate conditional probabilities. The first restriction does imply that we have limits as to how well even the best possible algorithm can perform. You should get used to the idea that while in some challenges we will be able to achieve almost perfect accuracy, with digit readers for example, in others our success is restricted by the randomness of the process, with movie recommendations for example.</p>
<p>Before we continue, it is important to remember that defining our prediction by maximizing the probability is not always optimal in practice and depends on the context. As discussed above, sensitivity and specificity may differ in importance. But even in these cases, having a good estimate of the <span class="math inline">\(p_k(x), k=1,\dots,K\)</span> will suffice for us to build optimal prediction models, since we can control the balance between specificity and sensitivity however we wish. For instance, we can simply change the cutoffs used to predict one outcome or the other. In the plane example, we may ground the plane anytime the probability of malfunction is higher than 1 in a million as opposed to the default 1/2 used when error types are equally undesired.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>Conditional probabilities for each class:</li>
</ul>
<p><span class="math display">\[
p_k(\mathbf{x}) = \mbox{Pr}(Y=k \mid \mathbf{X}=\mathbf{x}), \, \mbox{for}\, k=1,\dots,K
\]</span></p>
<ul>
<li>In machine learning, this is referred to as Bayes’ Rule. This is a theoretical rule because in practice we don’t know 𝑝(𝑥). Having a good estimate of the 𝑝(𝑥) will suffice for us to build optimal prediction models, since we can control the balance between specificity and sensitivity however we wish. In fact, estimating these conditional probabilities can be thought of as the main challenge of machine learning.</li>
</ul>
</div>
</div>
<div id="section-conditional-expectations" class="section level3">
<h3>Conditional expectations</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#conditional-expectations">textbook section - 27.6.2 Conditional expectations</a>.</p>
</div>
<p>For binary data, you can think of the probability <span class="math inline">\(\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x})\)</span> as the proportion of 1s in the stratum of the population for which <span class="math inline">\(\mathbf{X}=\mathbf{x}\)</span>. Many of the algorithms we will learn can be applied to both categorical and continuous data due to the connection between <em>conditional probabilities</em> and <em>conditional expectations</em>.</p>
<p>Because the expectation is the average of values <span class="math inline">\(y_1,\dots,y_n\)</span> in the population, in the case in which the <span class="math inline">\(y\)</span>s are 0 or 1, the expectation is equivalent to the probability of randomly picking a one since the average is simply the proportion of ones:</p>
<p><span class="math display">\[
\mbox{E}(Y \mid \mathbf{X}=\mathbf{x})=\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x}).
\]</span></p>
<p>As a result, we often only use the expectation to denote both the conditional probability and conditional expectation.</p>
<p>Just like with categorical outcomes, in most applications the same observed predictors do not guarantee the same continuous outcomes. Instead, we assume that the outcome follows the same conditional distribution. We will now explain why we use the conditional expectation to define our predictors.</p>
</div>
<div id="section-the-loss-function" class="section level3">
<h3>The loss function</h3>
<p>Up to now we have described evaluation metrics that apply exclusively to categorical data. Specifically, for binary outcomes, we have described how sensitivity, specificity, accuracy, and <span class="math inline">\(F_1\)</span> can be used as quantification. However, these metrics are not useful for continuous outcomes. In this section, we describe how the general approach to defining “best” in machine learning is to define a <em>loss function</em>, which can be applied to both categorical and continuous data.</p>
<p>The most commonly used loss function is the squared loss function. If <span class="math inline">\(\hat{y}\)</span> is our predictor and <span class="math inline">\(y\)</span> is the observed outcome, the squared loss function is simply:</p>
<p><span class="math display">\[
(\hat{y} - y)^2
\]</span></p>
<p>Because we often have a test set with many observations, say <span class="math inline">\(N\)</span>, we use the mean squared error (MSE):</p>
<p><span class="math display">\[
\mbox{MSE} = \frac{1}{N} \mbox{RSS} = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
\]</span></p>
<p>In practice, we often report the root mean squared error (RMSE), which is <span class="math inline">\(\sqrt{\mbox{MSE}}\)</span>, because it is in the same units as the outcomes. But doing the math is often easier with the MSE and it is therefore more commonly used in textbooks, since these usually describe theoretical properties of algorithms.</p>
<p>If the outcomes are binary, both RMSE and MSE are equivalent to one minus accuracy, since <span class="math inline">\((\hat{y} - y)^2\)</span> is 0 if the prediction was correct and 1 otherwise. In general, our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible.</p>
<p>Because our data is usually a random sample, we can think of the MSE as a random variable and the observed MSE can be thought of as an estimate of the expected MSE, which in mathematical notation we write like this:</p>
<p><span class="math display">\[
\mbox{E}\left\{ \frac{1}{N}\sum_{i=1}^N (\hat{Y}_i - Y_i)^2 \right\}
\]</span></p>
<p>This is a theoretical concept because in practice we only have one dataset to work with. But in theory, we think of having a very large number of random samples (call it <span class="math inline">\(B\)</span>), apply our algorithm to each, obtain an MSE for each random sample, and think of the expected MSE as:</p>
<p><span class="math display">\[
\frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b - y_i^b\right)^2 
\]</span></p>
<p>with <span class="math inline">\(y_{i}^b\)</span> denoting the <span class="math inline">\(i\)</span>th observation in the <span class="math inline">\(b\)</span>th random sample and <span class="math inline">\(\hat{y}_i^b\)</span> the resulting prediction obtained from applying the exact same algorithm to the <span class="math inline">\(b\)</span>th random sample. Again, in practice we only observe one random sample, so the expected MSE is only theoretical. However, in the textbook <a href="https://rafalab.github.io/dsbook/cross-validation.html">(Section - 29 Cross validation)</a> we describe an approach to estimating the MSE that tries to mimic this theoretical quantity.</p>
<p>Note that there are loss functions other than the squared loss. For example, the <em>Mean Absolute Error</em> uses absolute values, <span class="math inline">\(|\hat{Y}_i - Y_i|\)</span> instead of squaring the errors <span class="math inline">\((\hat{Y}_i - Y_i)^2\)</span>. However, in this course we focus on minimizing square loss since it is the most widely used.</p>
</div>
<div id="section-conditional-expectation-minimizes-squared-loss-function" class="section level3">
<h3>Conditional expectation minimizes squared loss function</h3>
<p>Why do we care about the conditional expectation in machine learning? This is because the expected value has an attractive mathematical property: it minimizes the MSE. Specifically, of all possible predictions <span class="math inline">\(\hat{Y}\)</span>,</p>
<p><span class="math display">\[
\hat{Y} = \mbox{E}(Y \mid \mathbf{X}=\mathbf{x}) \, \mbox{ minimizes } \, \mbox{E}\{ (\hat{Y} - Y)^2  \mid  \mathbf{X}=\mathbf{x} \}
\]</span></p>
<p>Due to this property, a succinct description of the main task of machine learning is that we use data to estimate:</p>
<p><span class="math display">\[
f(\mathbf{x}) \equiv \mbox{E}( Y  \mid  \mathbf{X}=\mathbf{x} )
\]</span></p>
<p>for any set of features <span class="math inline">\(\mathbf{x} = (x_1, \dots, x_p)\)</span>. Of course this is easier said than done, since this function can take any shape and <span class="math inline">\(p\)</span> can be very large. Consider a case in which we only have one predictor <span class="math inline">\(x\)</span>. The expectation <span class="math inline">\(\mbox{E}\{ Y \mid X=x \}\)</span> can be any function of <span class="math inline">\(x\)</span>: a line, a parabola, a sine wave, a step function, anything. It gets even more complicated when we consider instances with large <span class="math inline">\(p\)</span>, in which case <span class="math inline">\(f(\mathbf{x})\)</span> is a function of a multidimensional vector <span class="math inline">\(\mathbf{x}\)</span>. For example, in our digit reader example <span class="math inline">\(p = 784\)</span>! <strong>The main way in which competing machine learning algorithms differ is in their approach to estimating this expectation.</strong></p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>Due to the connection between <strong>conditional probabilities</strong> and <strong>conditional expectations</strong>:</li>
</ul>
<p><span class="math display">\[
p_k(\mathbf{x}) = \mbox{Pr}(Y=k \mid \mathbf{X}=\mathbf{x}), \, \mbox{for}\, k=1,\dots,K
\]</span></p>
<p>we often only use the expectation to denote both the conditional probability and conditional expectation.</p>
<ul>
<li>For continuous outcomes, we define a loss function to evaluate the model. The most commonly used one is <strong>MSE (Mean Squared Error)</strong>. The reason why we care about the conditional expectation in machine learning is that the expected value minimizes the MSE:</li>
</ul>
<p><span class="math display">\[
\hat{Y} = \mbox{E}(Y \mid \mathbf{X}=\mathbf{x}) \, \mbox{ minimizes } \, \mbox{E}\{ (\hat{Y} - Y)^2  \mid  \mathbf{X}=\mathbf{x} \}
\]</span></p>
<p>Due to this property, a succinct description of the main task of machine learning is that we use data to estimate for any set of features. <strong>The main way in which competing machine learning algorithms differ is in their approach to estimating this expectation</strong>.</p>
</div>
</div>
<div id="section-comprehension-check-conditional-probabilities-part-1" class="section level3">
<h3>2.2 Comprehension Check: Conditional Probabilities Part 1</h3>
<p>Insert assessment here</p>
</div>
<div id="section-comprehension-check-conditional-probabilities-part-2" class="section level3">
<h3>2.2 Comprehension Check: Conditional Probabilities Part 2</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-3-linear-regression-for-prediction-smoothing-and-working-with-matrices" class="section level2">
<h2>Section 3: Linear Regression for Prediction, Smoothing, and Working with Matrices</h2>
<p>In the <strong>Linear Regression for Prediction, Smoothing, and Working with Matrices Overview</strong> section, you will learn why linear regression is a useful baseline approach but is often insufficiently flexible for more complex analyses, how to smooth noisy data, and how to use matrices for machine learning.</p>
<p>After completing this section, you will be able to:</p>
<ul>
<li><p>Use <strong>linear regression for prediction</strong> as a baseline approach.</p></li>
<li><p>Use <strong>logistic regression</strong> for categorical data.</p></li>
<li><p>Detect trends in noisy data using <strong>smoothing</strong> (also known as <strong>curve fitting</strong> or <strong>low pass filtering</strong>).</p></li>
<li><p>Convert predictors to <strong>matrices</strong> and outcomes to <strong>vectors</strong> when all predictors are numeric (or can be converted to numerics in a meaningful way).</p></li>
<li><p>Perform basic <strong>matrix algebra</strong> calculations.</p></li>
</ul>
<p>This section has three parts: <strong>linear regression for prediction</strong>, <strong>smoothing</strong>, and <strong>working with matrices</strong>. There are comprehension checks periodically.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your own learning.</p>
<div id="section-linear-regression-for-prediction" class="section level3">
<h3>Linear Regression for Prediction</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#linear-regression">textbook section - 31.1 Linear regression</a>.</p>
</div>
<p>Linear regression can be considered a machine learning algorithm. In the textbook <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#two-or-seven">(Section - 27.8 Case study: is it a 2 or a 7?)</a> we demonstrated how linear regression can be too rigid to be useful. This is generally true, but for some challenges it works rather well. It also serves as a baseline approach: if you can’t beat it with a more complex approach, you probably want to stick to linear regression. To quickly make the connection between regression and machine learning, we will reformulate Galton’s study with heights, a continuous outcome.</p>
<pre class="r"><code>library(HistData)

set.seed(1983)
galton_heights &lt;- GaltonFamilies %&gt;%
  filter(gender == &quot;male&quot;) %&gt;%
  group_by(family) %&gt;%
  sample_n(1) %&gt;%
  ungroup() %&gt;%
  select(father, childHeight) %&gt;%
  rename(son = childHeight)</code></pre>
<p>Suppose you are tasked with building a machine learning algorithm that predicts the son’s height <span class="math inline">\(Y\)</span> using the father’s height <span class="math inline">\(X\)</span>. Let’s generate testing and training sets:</p>
<pre class="r"><code>y &lt;- galton_heights$son
test_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)

train_set &lt;- galton_heights %&gt;% slice(-test_index)
test_set &lt;- galton_heights %&gt;% slice(test_index)</code></pre>
<p>In this case, if we were just ignoring the father’s height and guessing the son’s height, we would guess the average height of sons.</p>
<pre class="r"><code>m &lt;- mean(train_set$son)
m</code></pre>
<pre><code>## [1] 69.18182</code></pre>
<p>Our squared loss is:</p>
<pre class="r"><code>mean((m - test_set$son)^2)</code></pre>
<pre><code>## [1] 7.651849</code></pre>
<p>Can we do better? In the regression section, we learned that if the pair <span class="math inline">\((X,Y)\)</span> follow a bivariate normal distribution, the conditional expectation (what we want to estimate) is equivalent to the regression line:</p>
<p><span class="math display">\[
f(x) = \mbox{E}( Y  \mid  X= x ) = \beta_0 + \beta_1 x
\]</span></p>
<p>In the textbook <a href="https://rafalab.github.io/dsbook/linear-models.html#lse">(Section - 18.3 Least squares estimates)</a> we introduced least squares as a method for estimating the slope <span class="math inline">\(\beta_0\)</span> and intercept <span class="math inline">\(\beta_1\)</span>:</p>
<pre class="r"><code>fit &lt;- lm(son ~ father, data = train_set)
fit$coef</code></pre>
<pre><code>## (Intercept)      father 
##  35.9756122   0.4816698</code></pre>
<p>This gives us an estimate of the conditional expectation:</p>
<p><span class="math display">\[ \hat{f}(x) = 52 + 0.25 x \]</span></p>
<p>We can see that this does indeed provide an improvement over our guessing approach.</p>
<pre class="r"><code>y_hat &lt;- fit$coef[1] + fit$coef[2]*test_set$father
mean((y_hat - test_set$son)^2)</code></pre>
<pre><code>## [1] 6.470245</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>Linear regression can be considered a machine learning algorithm. Although it can be too rigid to be useful, it works rather well for some challenges. It also serves as a baseline approach: if you can’t beat it with a more complex approach, you probably want to stick to linear regression.</li>
</ul>
</div>
</div>
<div id="section-predict-function" class="section level3">
<h3>Predict Function</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#the-predict-function">textbook section - 31.1.1 The predict function</a>.</p>
</div>
<p>The <code>predict</code> function is very useful for machine learning applications. This function takes a fitted object from functions such as <code>lm</code> or <code>glm</code> (we learn about <code>glm</code> soon) and a data frame with the new predictors for which to predict. So in our current example, we would use <code>predict</code> like this:</p>
<pre class="r"><code>y_hat &lt;- predict(fit, test_set)</code></pre>
<p>Using <code>predict</code>, we can get the same results as we did previously:</p>
<pre class="r"><code>y_hat &lt;- predict(fit, test_set)
mean((y_hat - test_set$son)^2)</code></pre>
<pre><code>## [1] 6.470245</code></pre>
<p><code>predict</code> does not always return objects of the same types; it depends on what type of object is sent to it. To learn about the specifics, you need to look at the help file specific for the type of fit object that is being used. The <code>predict</code> is actually a special type of function in R (called a <em>generic function</em>) that calls other functions depending on what kind of object it receives. So if <code>predict</code> receives an object coming out of the <code>lm</code> function, it will call <code>predict.lm</code>. If it receives an object coming out of <code>glm</code>, it calls <code>predict.glm</code>. These two functions are similar but different. You can learn more about the differences by reading the help files:</p>
<pre class="r"><code>?predict.lm
?predict.glm</code></pre>
<p>There are many other versions of <code>predict</code> and many machine learning algorithms have a <code>predict</code> function.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>The <code>predict()</code> function takes a fitted object from functions such as <code>lm()</code> or <code>glm()</code> and a data frame with the new predictors for which to predict. We can use predict like this:</li>
</ul>
<pre class="r"><code>y_hat &lt;- predict(fit, test_set)</code></pre>
<ul>
<li><code>predict()</code> is a generic function in R that calls other functions depending on what kind of object it receives. To learn about the specifics, you can read the help files using code like this:</li>
</ul>
<pre class="r"><code>?predict.lm    # or ?predict.glm</code></pre>
</div>
</div>
<div id="section-comprehension-check-linear-regression" class="section level3">
<h3>3.1 Comprehension Check: Linear Regression</h3>
<p>Insert assessment here</p>
</div>
<div id="section-regression-for-a-categorical-outcome" class="section level3">
<h3>Regression for a Categorical Outcome</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#logistic-regression">textbook section - 31.3 Logistic regression</a>.</p>
</div>
<p>The regression approach can be extended to categorical data. In this section we first illustrate how, for binary data, one can simply assign numeric values of 0 and 1 to the outcomes <span class="math inline">\(y\)</span>, and apply regression as if the data were continuous. We will then point out a limitation with this approach and introduce <em>logistic regression</em> as a solution. Logistic regression is a specific case of a set of <em>generalized linear models</em>. To illustrate logistic regression, we will apply it to our previous predicting sex example:</p>
<p>If we define the outcome <span class="math inline">\(Y\)</span> as 1 for females and 0 for males, and <span class="math inline">\(X\)</span> as the height, we are interested in the conditional probability:</p>
<p><span class="math display">\[
\mbox{Pr}( Y = 1 \mid X = x)
\]</span></p>
<p>As an example, let’s provide a prediction for a student that is 66 inches tall. What is the conditional probability of being female if you are 66 inches tall? In our dataset, we can estimate this by rounding to the nearest inch and computing:</p>
<pre class="r"><code>train_set %&gt;% 
  filter(round(height)==66) %&gt;%
  summarize(y_hat = mean(sex==&quot;Female&quot;))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["y_hat"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.3269231"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>To construct a prediction algorithm, we want to estimate the proportion of the population that is female for any given height <span class="math inline">\(X=x\)</span>, which we write as the conditional probability described above: <span class="math inline">\(\mbox{Pr}( Y = 1 | X=x)\)</span>. Let’s see what this looks like for several values of <span class="math inline">\(x\)</span> (we will remove strata of <span class="math inline">\(x\)</span> with few data points):</p>
<pre class="r"><code>heights %&gt;% 
  mutate(x = round(height)) %&gt;%
  group_by(x) %&gt;%
  filter(n() &gt;= 10) %&gt;%
  summarize(prop = mean(sex == &quot;Female&quot;)) %&gt;%
  ggplot(aes(x, prop)) +
  geom_point()</code></pre>
<p><img src="datsci_08_files/figure-html/height-and-sex-conditional-probabilities-1.png" width="624" /></p>
<p>Since the results from the plot above look close to linear, and it is the only approach we currently know, we will try regression. We assume that:</p>
<p><span class="math display">\[p(x) = \mbox{Pr}( Y = 1 | X=x)  = \beta_0 + \beta_1 x\]</span></p>
<p>Note: because <span class="math inline">\(p_0(x) = 1 - p_1(x)\)</span>, we will only estimate <span class="math inline">\(p_1(x)\)</span> and drop the <span class="math inline">\(_1\)</span> index.</p>
<p>If we convert the factors to 0s and 1s, we can estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> with least squares.</p>
<pre class="r"><code>lm_fit &lt;- mutate(train_set, y = as.numeric(sex == &quot;Female&quot;)) %&gt;% 
  lm(y ~ height, data = .)</code></pre>
<p>Once we have estimates <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, we can obtain an actual prediction. Our estimate of the conditional probability <span class="math inline">\(p(x)\)</span> is:</p>
<p><span class="math display">\[
\hat{p}(x) = \hat{\beta}_0+ \hat{\beta}_1 x
\]</span></p>
<p>To form a prediction, we define a <em>decision rule</em>: predict female if <span class="math inline">\(\hat{p}(x) &gt; 0.5\)</span>. We can compare our predictions to the outcomes using:</p>
<pre class="r"><code>p_hat &lt;- predict(lm_fit, test_set)
y_hat &lt;- ifelse(p_hat &gt; 0.5, &quot;Female&quot;, &quot;Male&quot;) %&gt;% factor()
confusionMatrix(y_hat, test_set$sex)[[&quot;Accuracy&quot;]]</code></pre>
<pre><code>## NULL</code></pre>
<p>We see this method does substantially better than guessing.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>The regression approach can be extended to categorical data. For example, we can try regression to estimate the conditional probability:</li>
</ul>
<p><span class="math display">\[p(x) = \mbox{Pr}( Y = 1 | X=x)  = \beta_0 + \beta_1 x\]</span></p>
<ul>
<li>Once we have estimates <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, we can obtain an actual prediction <span class="math inline">\(p(x)\)</span>. Then we can define a specific decision rule to form a prediction.</li>
</ul>
</div>
</div>
<div id="section-logistic-regression" class="section level3">
<h3>Logistic Regression</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#logistic-regression">textbook section - 31.3.1 Generalized linear models</a>.</p>
</div>
<p>The function <span class="math inline">\(\beta_0 + \beta_1 x\)</span> can take any value including negatives and values larger than 1. In fact, the estimate <span class="math inline">\(\hat{p}(x)\)</span> computed in the linear regression section does indeed become negative at around 76 inches.</p>
<pre class="r"><code>heights %&gt;% 
  mutate(x = round(height)) %&gt;%
  group_by(x) %&gt;%
  filter(n() &gt;= 10) %&gt;%
  summarize(prop = mean(sex == &quot;Female&quot;)) %&gt;%
  ggplot(aes(x, prop)) +
  geom_point() + 
  geom_abline(intercept = lm_fit$coef[1], slope = lm_fit$coef[2])</code></pre>
<p><img src="datsci_08_files/figure-html/regression-prediction-1.png" width="624" /></p>
<p>The range is:</p>
<pre class="r"><code>range(p_hat)</code></pre>
<pre><code>## [1] -0.330736  1.036463</code></pre>
<p>But we are estimating a probability: <span class="math inline">\(\mbox{Pr}( Y = 1 \mid X = x)\)</span> which is constrained between 0 and 1.</p>
<p>The idea of generalized linear models (GLM) is to 1) define a distribution of <span class="math inline">\(Y\)</span> that is consistent with it’s possible outcomes and 2) find a function <span class="math inline">\(g\)</span> so that <span class="math inline">\(g(\mbox{Pr}( Y = 1 \mid X = x))\)</span> can be modeled as a linear combination of predictors. Logistic regression is the most commonly used GLM. It is an extension of linear regression that assures that the estimate of <span class="math inline">\(\mbox{Pr}( Y = 1 \mid X = x)\)</span> is between 0 and 1. This approach makes use of the <em>logistic</em> transformation introduced in the textbook <a href="https://rafalab.github.io/dsbook/gapminder.html#logit">(Section - 9.8.1 Logistic transformation)</a>:</p>
<p><span class="math display">\[ g(p) = \log \frac{p}{1-p}\]</span></p>
<p>This logistic transformation converts probability to log odds. As discussed in the data visualization lecture, the odds tell us how much more likely it is something will happen compared to not happening. <span class="math inline">\(p=0.5\)</span> means the odds are 1 to 1, thus the odds are 1. If <span class="math inline">\(p=0.75\)</span>, the odds are 3 to 1. A nice characteristic of this transformation is that it converts probabilities to be symmetric around 0. Here is a plot of <span class="math inline">\(g(p)\)</span> versus <span class="math inline">\(p\)</span>:</p>
<p><img src="datsci_08_files/figure-html/p-versus-logistic-of-p-1.png" width="624" /></p>
<p>With <em>logistic regression</em>, we model the conditional probability directly with:</p>
<p><span class="math display">\[ 
g\left\{ \mbox{Pr}(Y = 1 \mid X=x) \right\} = \beta_0 + \beta_1 x
\]</span></p>
<p>With this model, we can no longer use least squares. Instead we compute the <em>maximum likelihood estimate</em> (MLE). You can learn more about this concept in <a href="http://www.amazon.com/Mathematical-Statistics-Analysis-Available-Enhanced/dp/0534399428">a statistical theory textbook</a>.</p>
<p>In R, we can fit the logistic regression model with the function <code>glm</code>: generalized linear models. This function is more general than logistic regression so we need to specify the model we want through the <code>family</code> parameter:</p>
<pre class="r"><code>glm_fit &lt;- train_set %&gt;% 
  mutate(y = as.numeric(sex == &quot;Female&quot;)) %&gt;%
  glm(y ~ height, data=., family = &quot;binomial&quot;)</code></pre>
<p>We can obtain prediction using the predict function:</p>
<pre class="r"><code>p_hat_logit &lt;- predict(glm_fit, newdata = test_set, type = &quot;response&quot;)</code></pre>
<p>When using <code>predict</code> with a <code>glm</code> object, we have to specify that we want <code>type="response"</code> if we want the conditional probabilities, since the default is to return the logistic transformed values.</p>
<p>This model fits the data slightly better than the line:</p>
<p><img src="datsci_08_files/figure-html/conditional-prob-glm-fit-1.png" width="624" /></p>
<p>Because we have an estimate <span class="math inline">\(\hat{p}(x)\)</span>, we can obtain predictions:</p>
<pre class="r"><code>y_hat_logit &lt;- ifelse(p_hat_logit &gt; 0.5, &quot;Female&quot;, &quot;Male&quot;) %&gt;% factor
confusionMatrix(y_hat_logit, test_set$sex)[[&quot;Accuracy&quot;]]</code></pre>
<pre><code>## NULL</code></pre>
<p>The resulting predictions are similar. This is because the two estimates of <span class="math inline">\(p(x)\)</span> are larger than 1/2 in about the same region of x:</p>
<pre class="r"><code>data.frame(x = seq(min(tmp$x), max(tmp$x))) %&gt;%
  mutate(logistic = plogis(glm_fit$coef[1] + glm_fit$coef[2]*x),
         regression = lm_fit$coef[1] + lm_fit$coef[2]*x) %&gt;%
  gather(method, p_x, -x) %&gt;%
  ggplot(aes(x, p_x, color = method)) + 
  geom_line() +
  geom_hline(yintercept = 0.5, lty = 5)</code></pre>
<p><img src="datsci_08_files/figure-html/glm-prediction-1.png" width="624" /></p>
<p>Both linear and logistic regressions provide an estimate for the conditional expectation:</p>
<p><span class="math display">\[
\mbox{E}(Y \mid X=x)
\]</span> which in the case of binary data is equivalent to the conditional probability:</p>
<p><span class="math display">\[
\mbox{Pr}(Y = 1 \mid X = x)
\]</span></p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><strong>Logistic regression</strong> is an extension of linear regression that assures that the estimate of conditional probability <span class="math inline">\(\mbox{Pr}(Y = 1 \mid X = x)\)</span> is between 0 and 1. This approach makes use of the logistic transformation:</li>
</ul>
<p><span class="math display">\[ g(p) = \log \frac{p}{1-p}\]</span></p>
<ul>
<li>With logistic regression, we model the conditional probability directly with:</li>
</ul>
<p><span class="math inline">\(g\left\{ \mbox{Pr}(Y = 1 \mid X=x) \right\} = \beta_0 + \beta_1 x\)</span></p>
<ul>
<li><p>Note that with this model, we can no longer use least squares. Instead we compute the <strong>maximum likelihood estimate (MLE)</strong>.</p></li>
<li><p>In R, we can fit the logistic regression model with the function <code>glm()</code> (generalized linear models). If we want to compute the conditional probabilities, we want <code>type="response"</code> since the default is to return the logistic transformed values.</p></li>
</ul>
</div>
</div>
<div id="section-case-study-2-or-7" class="section level3">
<h3>Case Study: 2 or 7</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#two-or-seven">textbook section - 27.8 Case study: is it a 2 or a 7?</a>.</p>
</div>
<p>In the two simple examples above, we only had one predictor. We actually do not consider these machine learning challenges, which are characterized by cases with many predictors. Let’s go back to the digits example in which we had 784 predictors. For illustrative purposes, we will start by simplifying this problem to one with two predictors and two classes. Specifically, we define the challenge as building an algorithm that can determine if a digit is a 2 or 7 from the predictors. We are not quite ready to build algorithms with 784 predictors, so we will extract two simple predictors from the 784: the proportion of dark pixels that are in the upper left quadrant (<span class="math inline">\(X_1\)</span>) and the lower right quadrant (<span class="math inline">\(X_2\)</span>).</p>
<p>We then select a random sample of 1,000 digits, 500 in the training set and 500 in the test set. We provide this dataset in the <code>dslabs</code> package:</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)
data(&quot;mnist_27&quot;)</code></pre>
<p>We can explore the data by plotting the two predictors and using colors to denote the labels:</p>
<pre class="r"><code>mnist_27$train %&gt;% ggplot(aes(x_1, x_2, color = y)) + geom_point()</code></pre>
<p><img src="datsci_08_files/figure-html/two-or-seven-scatter-1.png" width="624" /></p>
<p>We can immediately see some patterns. For example, if <span class="math inline">\(X_1\)</span> (the upper left panel) is very large, then the digit is probably a 7. Also, for smaller values of <span class="math inline">\(X_1\)</span>, the 2s appear to be in the mid range values of <span class="math inline">\(X_2\)</span>.</p>
<p>These are the images of the digits with the largest and smallest values for <span class="math inline">\(X_1\)</span>: And here are the original images corresponding to the largest and smallest value of <span class="math inline">\(X_2\)</span>:</p>
<p><img src="datsci_08_files/figure-html/two-or-seven-images-large-x1-1.png" width="100%" /></p>
<p>We can start getting a sense for why these predictors are useful, but also why the problem will be somewhat challenging.</p>
<p>We haven’t really learned any algorithms yet, so let’s try building an algorithm using regression. The model is simply:</p>
<p><span class="math display">\[
p(x_1, x_2) = \mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2) = 
\beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span></p>
<p>We fit it like this:</p>
<pre class="r"><code>fit &lt;- mnist_27$train %&gt;%
  mutate(y = ifelse(y==7, 1, 0)) %&gt;%
  lm(y ~ x_1 + x_2, data = .)</code></pre>
<p>We can now build a decision rule based on the estimate of <span class="math inline">\(\hat{p}(x_1, x_2)\)</span>:</p>
<pre class="r"><code>library(caret)
p_hat &lt;- predict(fit, newdata = mnist_27$test)
y_hat &lt;- factor(ifelse(p_hat &gt; 0.5, 7, 2))
confusionMatrix(y_hat, mnist_27$test$y)$overall[[&quot;Accuracy&quot;]]</code></pre>
<pre><code>## [1] 0.75</code></pre>
<p>We get an accuracy well above 50%. Not bad for our first try. But can we do better?</p>
<p>Because we constructed the <code>mnist_27</code> example and we had at our disposal 60,000 digits in just the MNIST dataset, we used this to build the <em>true</em> conditional distribution <span class="math inline">\(p(x_1, x_2)\)</span>. Keep in mind that this is something we don’t have access to in practice, but we include it in this example because it permits the comparison of <span class="math inline">\(\hat{p}(x_1, x_2)\)</span> to the true <span class="math inline">\(p(x_1, x_2)\)</span>. This comparison teaches us the limitations of different algorithms. Let’s do that here. We have stored the true <span class="math inline">\(p(x_1,x_2)\)</span> in the <code>mnist_27</code> object and can plot the image using the <strong>ggplot2</strong> function <code>geom_raster()</code>. We choose better colors and use the <code>stat_contour</code> function to draw a curve that separates pairs <span class="math inline">\((x_1,x_2)\)</span> for which <span class="math inline">\(p(x_1,x_2) &gt; 0.5\)</span> and pairs for which <span class="math inline">\(p(x_1,x_2) &lt; 0.5\)</span>:</p>
<pre class="r"><code>mnist_27$true_p %&gt;% ggplot(aes(x_1, x_2, z = p, fill = p)) +
  geom_raster() +
  scale_fill_gradientn(colors=c(&quot;#F8766D&quot;, &quot;white&quot;, &quot;#00BFC4&quot;)) +
  stat_contour(breaks=c(0.5), color=&quot;black&quot;)</code></pre>
<p><img src="datsci_08_files/figure-html/true-p-better-colors-1.png" width="624" /></p>
<p>Above you see a plot of the true <span class="math inline">\(p(x,y)\)</span>. To start understanding the limitations of logistic regression here, first note that with logistic regression <span class="math inline">\(\hat{p}(x,y)\)</span> has to be a plane, and as a result the boundary defined by the decision rule is given by: <span class="math inline">\(\hat{p}(x,y) = 0.5\)</span>, which implies the boundary can’t be anything other than a straight line:</p>
<p><span class="math display">\[
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0.5 \implies
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0.5  \implies
x_2 = (0.5-\hat{\beta}_0)/\hat{\beta}_2  -\hat{\beta}_1/\hat{\beta}_2 x_1
\]</span> Note that for this boundary, <span class="math inline">\(x_2\)</span> is a linear function of <span class="math inline">\(x_1\)</span>. This implies that our logistic regression approach has no chance of capturing the non-linear nature of the true <span class="math inline">\(p(x_1,x_2)\)</span>. Below is a visual representation of <span class="math inline">\(\hat{p}(x_1, x_2)\)</span>. We used the <code>squish</code> function from the <strong>scales</strong> package to constrain estimates to be between 0 and 1. We can see where the mistakes were made by also showing the data and the boundary. They mainly come from low values <span class="math inline">\(x_1\)</span> that have either high or low value of <span class="math inline">\(x_2\)</span>. Regression can’t catch this.</p>
<p><img src="datsci_08_files/figure-html/regression-p-hat-1.png" width="100%" /></p>
<p>We need something more flexible: a method that permits estimates with shapes other than a plane.</p>
<p>We are going to learn a few new algorithms based on different ideas and concepts. But what they all have in common is that they permit more flexible approaches. We will start by describing nearest neighbor and kernel approaches. To introduce the concepts behinds these approaches, we will again start with a simple one-dimensional example and describe the concept of <em>smoothing</em>.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>In this case study we apply logistic regression to classify whether a digit is two or seven. We are interested in estimating a conditional probability that depends on two variables:</li>
</ul>
<p><span class="math display">\[
g\{p(x_1, x_2)\}= g\{\mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2)\} = 
\beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span></p>
<ul>
<li>Through this case, we know that logistic regression forces our estimates to be a <strong>plane</strong> and our boundary to be a <strong>line</strong>. This implies that a logistic regression approach has no chance of capturing the <strong>non-linear</strong> nature of the true <span class="math inline">\(p(x_1, x_2)\)</span>. Therefore, we need other more flexible methods that permit other shapes.</li>
</ul>
</div>
</div>
<div id="section-comprehension-check-logistic-regression" class="section level3">
<h3>3.1 Comprehension Check: Logistic Regression</h3>
<p>Insert assessment here</p>
</div>
<div id="section-introduction-to-smoothing" class="section level3">
<h3>Introduction to Smoothing</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/smoothing.html">textbook section - Chapter 28 Smoothing</a>.</p>
</div>
<p>Before continuing learning about machine learning algorithms, we introduce the important concept of <em>smoothing</em>. Smoothing is a very powerful technique used all across data analysis. Other names given to this technique are <em>curve fitting</em> and <em>low pass filtering</em>. It is designed to detect trends in the presence of noisy data in cases in which the shape of the trend is unknown. The <em>smoothing</em> name comes from the fact that to accomplish this feat, we assume that the trend is <em>smooth</em>, as in a smooth surface. In contrast, the noise, or deviation from the trend, is unpredictably wobbly:</p>
<p><img src="datsci_08_files/figure-html/signal-plus-noise-example-1.png" width="624" /></p>
<p>Part of what we explain in this section are the assumptions that permit us to extract the trend from the noise.</p>
<p><strong>To understand why we cover this topic, remember that the concepts behind smoothing techniques are extremely useful in machine learning because conditional expectations/probabilities can be thought of as <em>trends</em> of unknown shapes that we need to estimate in the presence of uncertainty.</strong></p>
<p>To explain these concepts, we will focus first on a problem with just one predictor. Specifically, we try to estimate the time trend in the 2008 US popular vote poll margin (difference between Obama and McCain).</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)
data(&quot;polls_2008&quot;)
qplot(day, margin, data = polls_2008)</code></pre>
<p><img src="datsci_08_files/figure-html/polls-2008-data-1.png" width="624" /></p>
<p>For the purposes of this example, do not think of it as a forecasting problem. Instead, we are simply interested in learning the shape of the trend <em>after</em> the election is over.</p>
<p>We assume that for any given day <span class="math inline">\(x\)</span>, there is a true preference among the electorate <span class="math inline">\(f(x)\)</span>, but due to the uncertainty introduced by the polling, each data point comes with an error <span class="math inline">\(\varepsilon\)</span>. A mathematical model for the observed poll margin <span class="math inline">\(Y_i\)</span> is:</p>
<p><span class="math display">\[
Y_i = f(x_i) + \varepsilon_i
\]</span></p>
<p>To think of this as a machine learning problem, consider that we want to predict <span class="math inline">\(Y\)</span> given a day <span class="math inline">\(x\)</span>. If we knew the conditional expectation <span class="math inline">\(f(x) = \mbox{E}(Y \mid X=x)\)</span>, we would use it. But since we don’t know this conditional expectation, we have to estimate it. Let’s use regression, since it is the only method we have learned up to now.</p>
<p><img src="datsci_08_files/figure-html/linear-regression-not-flexible-1.png" width="624" /></p>
<p>The line we see does not appear to describe the trend very well. For example, on September 4 (day -62), the Republican Convention was held and the data suggest that it gave John McCain a boost in the polls. However, the regression line does not capture this potential trend. To see the <em>lack of fit</em> more clearly, we note that points above the fitted line (blue) and those below (red) are not evenly distributed across days. We therefore need an alternative, more flexible approach.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p><strong>Smoothing</strong> is a very powerful technique used all across data analysis. It is designed to detect trends in the presence of noisy data in cases in which the shape of the trend is unknown.</p></li>
<li><p>The concepts behind smoothing techniques are extremely useful in machine learning because <strong>conditional expectations/probabilities</strong> can be thought of as <strong>trends</strong> of unknown shapes that we need to estimate in the presence of uncertainty.</p></li>
</ul>
</div>
</div>
<div id="section-bin-smoothing-and-kernels" class="section level3">
<h3>Bin Smoothing and Kernels</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/smoothing.html#bin-smoothing">textbook section - 28.1 Bin smoothing</a> and <a href="https://rafalab.github.io/dsbook/smoothing.html#kernels">textbook section - 28.2 Kernels</a>.</p>
</div>
<p>The general idea of smoothing is to group data points into strata in which the value of <span class="math inline">\(f(x)\)</span> can be assumed to be constant. We can make this assumption because we think <span class="math inline">\(f(x)\)</span> changes slowly and, as a result, <span class="math inline">\(f(x)\)</span> is almost constant in small windows of time. An example of this idea for the <code>poll_2008</code> data is to assume that public opinion remained approximately the same within a week’s time. With this assumption in place, we have several data points with the same expected value.</p>
<p>If we fix a day to be in the center of our week, call it <span class="math inline">\(x_0\)</span>, then for any other day <span class="math inline">\(x\)</span> such that <span class="math inline">\(|x - x_0| \leq 3.5\)</span>, we assume <span class="math inline">\(f(x)\)</span> is a constant <span class="math inline">\(f(x) = \mu\)</span>. This assumption implies that: <span class="math display">\[
E[Y_i | X_i = x_i ] \approx \mu \mbox{   if   }  |x_i - x_0| \leq 3.5
\]</span></p>
<p>In smoothing, we call the size of the interval satisfying <span class="math inline">\(|x_i - x_0| \leq 3.5\)</span> the <em>window size</em>, <em>bandwidth</em> or <em>span</em>. Later we will see that we try to optimize this parameter.</p>
<p>This assumption implies that a good estimate for <span class="math inline">\(f(x)\)</span> is the average of the <span class="math inline">\(Y_i\)</span> values in the window. If we define <span class="math inline">\(A_0\)</span> as the set of indexes <span class="math inline">\(i\)</span> such that <span class="math inline">\(|x_i - x_0| \leq 3.5\)</span> and <span class="math inline">\(N_0\)</span> as the number of indexes in <span class="math inline">\(A_0\)</span>, then our estimate is:</p>
<p><span class="math display">\[
\hat{f}(x_0) = \frac{1}{N_0} \sum_{i \in A_0}  Y_i
\]</span></p>
<p>The idea behind <em>bin smoothing</em> is to make this calculation with each value of <span class="math inline">\(x\)</span> as the center. In the poll example, for each day, we would compute the average of the values within a week with that day in the center. Here are two examples: <span class="math inline">\(x_0 = -125\)</span> and <span class="math inline">\(x_0 = -55\)</span>. The blue segment represents the resulting average.</p>
<p><img src="datsci_08_files/figure-html/binsmoother-expained-1.png" width="624" /></p>
<p>By computing this mean for every point, we form an estimate of the underlying curve <span class="math inline">\(f(x)\)</span>. Below we show the procedure happening as we move from the -155 up to 0. At each value of <span class="math inline">\(x_0\)</span>, we keep the estimate <span class="math inline">\(\hat{f}(x_0)\)</span> and move on to the next point:</p>
<p><img src="images/binsmoother-animation.gif" /><!-- --></p>
<p>The final code and resulting estimate look like this:</p>
<pre class="r"><code>span &lt;- 7 
fit &lt;- with(polls_2008, 
            ksmooth(day, margin, kernel = &quot;box&quot;, bandwidth = span))

polls_2008 %&gt;% mutate(smooth = fit$y) %&gt;%
  ggplot(aes(day, margin)) +
    geom_point(size = 3, alpha = .5, color = &quot;grey&quot;) + 
  geom_line(aes(day, smooth), color=&quot;red&quot;)</code></pre>
<p><img src="datsci_08_files/figure-html/binsmoother-final-1.png" width="624" /></p>
</div>
<div id="section-kernels" class="section level3">
<h3>Kernels</h3>
<p>The final result from the bin smoother is quite wiggly. One reason for this is that each time the window moves, two points change. We can attenuate this somewhat by taking weighted averages that give the center point more weight than far away points, with the two points at the edges receiving very little weight.</p>
<p>You can think of the bin smoother approach as a weighted average:</p>
<p><span class="math display">\[
\hat{f}(x_0) = \sum_{i=1}^N w_0(x_i) Y_i
\]</span></p>
<p>in which each point receives a weight of either <span class="math inline">\(0\)</span> or <span class="math inline">\(1/N_0\)</span>, with <span class="math inline">\(N_0\)</span> the number of points in the week. In the code above, we used the argument <code>kernel="box"</code> in our call to the function <code>ksmooth</code>. This is because the weight function looks like a box. The <code>ksmooth</code> function provides a “smoother” option which uses the normal density to assign weights.</p>
<p><img src="datsci_08_files/figure-html/gaussian-kernel-1.png" width="80%" /></p>
<!--
With this animation, we see that points on the edge get less weight (the size of the point is proportional to its weight):

![](./images//kernel-animation.gif)<!-- -->
<p>–&gt;</p>
<p>The final code and resulting plot for the normal kerenl look like this:</p>
<pre class="r"><code>span &lt;- 7
fit &lt;- with(polls_2008, 
            ksmooth(day, margin, kernel = &quot;normal&quot;, bandwidth = span))

polls_2008 %&gt;% mutate(smooth = fit$y) %&gt;%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = &quot;grey&quot;) + 
  geom_line(aes(day, smooth), color=&quot;red&quot;)</code></pre>
<p><img src="datsci_08_files/figure-html/final-ksmooth-normal-kernel-1.png" width="624" /></p>
<p>Notice that the final estimate now looks smoother.</p>
<p>There are several functions in R that implement bin smoothers. One example is <code>ksmooth</code>, shown above. In practice, however, we typically prefer methods that use slightly more complex models than fitting a constant. The final result above, for example, is still somewhat wiggly in parts we don’t expect it to be (between -125 and -75, for example). Methods such as <code>loess</code>, which we explain next, improve on this.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The general idea of smoothing is to group data points into strata in which the value of <span class="math inline">\(f(x)\)</span> can be assumed to be constant. We can make this assumption because we think <span class="math inline">\(f(x)\)</span> changes slowly and, as a result, <span class="math inline">\(f(x)\)</span> is almost constant in small windows of time.</p></li>
<li><p>This assumption implies that a good estimate for <span class="math inline">\(f(x)\)</span> is the average of the <span class="math inline">\(Y_i\)</span> values in the window. The estimate is:</p></li>
</ul>
<p><span class="math display">\[
\hat{f}(x_0) = \frac{1}{N_0} \sum_{i \in A_0}  Y_i
\]</span></p>
<ul>
<li>In smoothing, we call the size of the interval <span class="math inline">\(|x - x_0|\)</span> satisfying the particular condition the window size, bandwidth or span.</li>
</ul>
</div>
</div>
<div id="section-local-weighted-regression-loess" class="section level3">
<h3>Local Weighted Regression (loess)</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/smoothing.html#local-weighted-regression-loess">textbook section - 28.3 Local weighted regression (loess)</a>.</p>
</div>
<p>A limitation of the bin smoother approach just described is that we need small windows for the approximately constant assumptions to hold. As a result, we end up with a small number of data points to average and obtain imprecise estimates <span class="math inline">\(\hat{f}(x)\)</span>. Here we describe how <em>local weighted regression</em> (loess) permits us to consider larger window sizes. To do this, we will use a mathematical result, referred to as Taylor’s theorem, which tells us that if you look closely enough at any smooth function <span class="math inline">\(f(x)\)</span>, it will look like a line. To see why this makes sense, consider the curved edges gardeners make using straight-edged spades:</p>
<p><img src="images/garden.png" width="80%" /></p>
<p>(<a href="https://www.flickr.com/photos/49707497@N06/7361631644">“Downing Street garden path edge”</a> by <a href="https://www.flickr.com/photos/number10gov/">Flckr user Number 10</a>. <a href="https://creativecommons.org/licenses/by/2.0/">CC-BY 2.0 license</a>.)</p>
<p>Instead of assuming the function is approximately constant in a window, we assume the function is locally linear. We can consider larger window sizes with the linear assumption than with a constant. Instead of the one-week window, we consider a larger one in which the trend is approximately linear. We start with a three-week window and later consider and evaluate other options:</p>
<p><span class="math display">\[
E[Y_i | X_i = x_i ] = \beta_0 + \beta_1 (x_i-x_0) \mbox{   if   }  |x_i - x_0| \leq 21
\]</span></p>
<p>For every point <span class="math inline">\(x_0\)</span>, loess defines a window and fits a line within that window. Here is an example showing the fits for <span class="math inline">\(x_0=-125\)</span> and <span class="math inline">\(x_0 = -55\)</span>:</p>
<p><img src="datsci_08_files/figure-html/loess-1.png" width="624" /></p>
<p>The fitted value at <span class="math inline">\(x_0\)</span> becomes our estimate <span class="math inline">\(\hat{f}(x_0)\)</span>. Below we show the procedure happening as we move from the -155 up to 0.</p>
<p><img src="images/loess-animation.gif" /><!-- --></p>
<p>The final result is a smoother fit than the bin smoother since we use larger sample sizes to estimate our local parameters:</p>
<pre class="r"><code>total_days &lt;- diff(range(polls_2008$day))
span &lt;- 21/total_days

fit &lt;- loess(margin ~ day, degree=1, span = span, data=polls_2008)

polls_2008 %&gt;% mutate(smooth = fit$fitted) %&gt;%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = &quot;grey&quot;) +
  geom_line(aes(day, smooth), color=&quot;red&quot;)</code></pre>
<p><img src="datsci_08_files/figure-html/final-loess-1.png" width="624" /></p>
<p>Different spans give us different estimates. We can see how different window sizes lead to different estimates:</p>
<p><img src="images/loess-multi-span-animation.gif" /><!-- --></p>
<p>Here are the final estimates:</p>
<p><img src="datsci_08_files/figure-html/loess-final-1.png" width="624" /></p>
<p>There are three other differences between <code>loess</code> and the typical bin smoother.</p>
<p>1. Rather than keeping the bin size the same, <code>loess</code> keeps the number of points used in the local fit the same. This number is controlled via the <code>span</code> argument, which expects a proportion. For example, if <code>N</code> is the number of data points and <code>span=0.5</code>, then for a given <span class="math inline">\(x\)</span>, <code>loess</code> will use the <code>0.5 * N</code> closest points to <span class="math inline">\(x\)</span> for the fit.</p>
<p>2. When fitting a line locally, <code>loess</code> uses a <em>weighted</em> approach. Basically, instead of using least squares, we minimize a weighted version:</p>
<p><span class="math display">\[
\sum_{i=1}^N w_0(x_i) \left[Y_i - \left\{\beta_0 + \beta_1 (x_i-x_0)\right\}\right]^2
\]</span></p>
<p>However, instead of the Gaussian kernel, loess uses a function called the Tukey tri-weight:</p>
<p><span class="math display">\[
W(u)= \left( 1  - |u|^3\right)^3 \mbox{ if } |u| \leq 1 \mbox{ and } W(u) = 0 \mbox{ if } |u| &gt; 1
\]</span></p>
<p>To define the weights, we denote <span class="math inline">\(2h\)</span> as the window size and define:</p>
<p><span class="math display">\[
w_0(x_i) = W\left(\frac{x_i - x_0}{h}\right)
\]</span></p>
<p>This kernel differs from the Gaussian kernel in that more points get values closer to the max:</p>
<p><img src="datsci_08_files/figure-html/triweight-kernel-1.png" width="80%" /></p>
<p>3. <code>loess</code> has the option of fitting the local model <em>robustly</em>. An iterative algorithm is implemented in which, after fitting a model in one iteration, outliers are detected and down-weighted for the next iteration. To use this option, we use the argument <code>family="symmetric"</code>.</p>
</div>
<div id="section-fitting-parabolas" class="section level3">
<h3>Fitting parabolas</h3>
<p>Taylor’s theorem also tells us that if you look at any mathematical function closely enough, it looks like a parabola. The theorem also states that you don’t have to look as closely when approximating with parabolas as you do when approximating with lines. This means we can make our windows even larger and fit parabolas instead of lines.</p>
<p><span class="math display">\[
E[Y_i | X_i = x_i ] = \beta_0 + \beta_1 (x_i-x_0) + \beta_2 (x_i-x_0)^2 \mbox{   if   }  |x_i - x_0| \leq h
\]</span></p>
<p>This is actually the default procedure of the function <code>loess</code>. You may have noticed that when we showed the code for using loess, we set <code>degree = 1</code>. This tells loess to fit polynomials of degree 1, a fancy name for lines. If you read the help page for loess, you will see that the argument <code>degree</code> defaults to 2. By default, loess fits parabolas not lines. Here is a comparison of the fitting lines (red dashed) and fitting parabolas (orange solid):</p>
<pre class="r"><code>total_days &lt;- diff(range(polls_2008$day))
span &lt;- 28/total_days
fit_1 &lt;- loess(margin ~ day, degree=1, span = span, data=polls_2008)

fit_2 &lt;- loess(margin ~ day, span = span, data=polls_2008)


polls_2008 %&gt;% mutate(smooth_1 = fit_1$fitted, smooth_2 = fit_2$fitted) %&gt;%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = &quot;grey&quot;) +
  geom_line(aes(day, smooth_1), color=&quot;red&quot;, lty = 2) +
  geom_line(aes(day, smooth_2), color=&quot;orange&quot;, lty = 1) </code></pre>
<p><img src="datsci_08_files/figure-html/polls-2008-parabola-line-loess-1.png" width="624" /></p>
<p>The <code>degree = 2</code> gives us more wiggly results. We actually prefer <code>degree = 1</code> as it is less prone to this kind of noise.</p>
</div>
<div id="section-beware-of-default-smoothing-parameters" class="section level3">
<h3>Beware of default smoothing parameters</h3>
<p><code>ggplot</code> uses loess in its <code>geom_smooth</code> function:</p>
<pre class="r"><code>polls_2008 %&gt;% ggplot(aes(day, margin)) +
  geom_point() + 
  geom_smooth()</code></pre>
<p><img src="datsci_08_files/figure-html/ggplot-loess-default-1.png" width="624" /></p>
<p>But be careful with default parameters as they are rarely optimal. However, you can conveniently change them:</p>
<pre class="r"><code>polls_2008 %&gt;% ggplot(aes(day, margin)) +
  geom_point() + 
  geom_smooth(span = 0.15, method.args = list(degree=1))</code></pre>
<p><img src="datsci_08_files/figure-html/ggplot-loess-degree-1-1.png" width="624" /></p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>A limitation of the bin smoothing approach is that we need small windows for the approximately constant assumptions to hold which may lead to imprecise estimates of <span class="math inline">\(f(x)\)</span>. <strong>Local weighted regression (loess)</strong> permits us to consider larger window sizes.</p></li>
<li><p>One important difference between loess and bin smoother is that we assume the smooth function is locally <strong>linear</strong> in a window instead of constant.</p></li>
<li><p>The result of loess is a smoother fit than bin smoothing because we use larger sample sizes to estimate our local parameters.</p></li>
</ul>
</div>
</div>
<div id="section-comprehension-check-smoothing" class="section level3">
<h3>3.2 Comprehension Check: Smoothing</h3>
<p>Insert assessment here</p>
</div>
<div id="section-matrices" class="section level3">
<h3>Matrices</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#matrix-algebra">textbook section - 33.1 Matrix algebra</a>.</p>
</div>
<p>In machine learning, situations in which all predictors are numeric, or can be converted to numeric in a meaningful way, are common. The digits data set is an example: every pixel records a number between 0 and 255. Let’s load the data:</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)
if(!exists(&quot;mnist&quot;)) mnist &lt;- read_mnist()</code></pre>
<p>In these cases, it is often convenient to save the predictors in a matrix and the outcome in a vector rather than using a data frame. You can see that the predictors are saved as a matrix:</p>
<pre class="r"><code>class(mnist$train$images)</code></pre>
<pre><code>## [1] &quot;matrix&quot; &quot;array&quot;</code></pre>
<p>This matrix represents 60,000 digits, so for the examples in this section, we will take a more manageable subset. We will take the first 1,000 predictors <code>x</code> and labels <code>y</code>:</p>
<pre class="r"><code>x &lt;- mnist$train$images[1:1000,] 
y &lt;- mnist$train$labels[1:1000]</code></pre>
<p>The main reason for using matrices is that certain mathematical operations needed to develop efficient code can be performed using techniques from a branch of mathematics called <em>linear algebra</em>. In fact, linear algebra and matrix notation are key elements of the language used in academic papers describing machine learning techniques. We will not cover linear algebra in detail here, but will demonstrate how to use matrices in R so that you can apply the linear algebra techniques already implemented in base R or other packages.</p>
<p>To motivate the use of matrices, we will pose five questions/challenges:</p>
<p>1. Do some digits require more ink than others? Study the distribution of the total pixel darkness and how it varies by digits.</p>
<p>2. Are some pixels uninformative? Study the variation of each pixel and remove predictors (columns) associated with pixels that don’t change much and thus can’t provide much information for classification.</p>
<p>3. Can we remove smudges? First, look at the distribution of all pixel values. Use this to pick a cutoff to define unwritten space. Then, set anything below that cutoff to 0.</p>
<p>4. Binarize the data. First, look at the distribution of all pixel values. Use this to pick a cutoff to distinguish between writing and no writing. Then, convert all entries into either 1 or 0, respectively.</p>
<p>5. Scale each of the predictors in each entry to have the same average and standard deviation.</p>
<p>To complete these, we will have to perform mathematical operations involving several variables. The <strong>tidyverse</strong> is not developed to perform these types of mathematical operations. For this task, it is convenient to use matrices.</p>
<p>Before we do this, we will introduce matrix notation and basic R code to define and operate on matrices.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The main reason for using matrices is that certain mathematical operations needed to develop efficient code can be performed using techniques from a branch of mathematics called linear algebra.</p></li>
<li><p>Linear algebra and matrix notation are key elements of the language used in academic papers describing machine learning techniques.</p></li>
</ul>
</div>
</div>
<div id="section-matrix-notation" class="section level3">
<h3>Matrix Notation</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#notation-2">textbook section - 33.1.1 Notation</a>.</p>
</div>
<p>In matrix algebra, we have three main types of objects: scalars, vectors, and matrices. A scalar is just one number, for example <span class="math inline">\(a = 1\)</span>. To denote scalars in matrix notation, we usually use a lower case letter and do not bold.</p>
<p>Vectors are like the numeric vectors we define in R: they include several scalar entries. For example, the column containing the first pixel:</p>
<pre class="r"><code>length(x[,1])</code></pre>
<pre><code>## [1] 1000</code></pre>
<p>has 1,000 entries. In matrix algebra, we use the following notation for a vector representing a feature/predictor:</p>
<p><span class="math display">\[ 
\begin{pmatrix}
x_1\\\
x_2\\\
\vdots\\\
x_N
\end{pmatrix}
\]</span></p>
<p>Similarly, we can use math notation to represent different features mathematically by adding an index:</p>
<p><span class="math display">\[ 
\mathbf{X}_1 = \begin{pmatrix}
x_{1,1}\\
\vdots\\
x_{N,1}
\end{pmatrix} \mbox{ and }
\mathbf{X}_2 = \begin{pmatrix}
x_{1,2}\\
\vdots\\
x_{N,2}
\end{pmatrix}
\]</span></p>
<p>If we are writing out a column, such as <span class="math inline">\(\mathbf{X}_1\)</span>, in a sentence we often use the notation: <span class="math inline">\(\mathbf{X}_1 = ( x_{1,1}, \dots x_{N,1})^\top\)</span> with <span class="math inline">\(^\top\)</span> the transpose operation that converts columns into rows and rows into columns.</p>
<p>A matrix can be defined as a series of vectors of the same size joined together as columns:</p>
<pre class="r"><code>x_1 &lt;- 1:5
x_2 &lt;- 6:10
cbind(x_1, x_2)</code></pre>
<pre><code>##      x_1 x_2
## [1,]   1   6
## [2,]   2   7
## [3,]   3   8
## [4,]   4   9
## [5,]   5  10</code></pre>
<p>Mathematically, we represent them with bold upper case letters:</p>
<p><span class="math display">\[ 
\mathbf{X} = [ \mathbf{X}_1 \mathbf{X}_2 ] = \begin{pmatrix}
x_{1,1}&amp;x_{1,2}\\
\vdots\\
x_{N,1}&amp;x_{N,2}
\end{pmatrix}
\]</span></p>
<p>The <em>dimension</em> of a matrix is often an important characteristic needed to assure that certain operations can be performed. The dimension is a two-number summary defined as the number of rows <span class="math inline">\(\times\)</span> the number of columns. In R, we can extract the dimension of a matrix with the function <code>dim</code>:</p>
<pre class="r"><code>dim(x)</code></pre>
<pre><code>## [1] 1000  784</code></pre>
<p>Vectors can be thought of as <span class="math inline">\(N\times 1\)</span> matrices. However, in R, a vector does not have dimensions:</p>
<pre class="r"><code>dim(x_1)</code></pre>
<pre><code>## NULL</code></pre>
<p>Yet we explicitly convert a vector into a matrix using the function <code>as.matrix</code>:</p>
<pre class="r"><code>dim(as.matrix(x_1))</code></pre>
<pre><code>## [1] 5 1</code></pre>
<p>We can use this notation to denote an arbitrary number of predictors with the following <span class="math inline">\(N\times p\)</span> matrix, for example, with <span class="math inline">\(p=784\)</span>:</p>
<p><span class="math display">\[
\mathbf{X} = \begin{pmatrix}
  x_{1,1}&amp;\dots &amp; x_{1,p} \\
  x_{2,1}&amp;\dots &amp; x_{2,p} \\
   &amp; \vdots &amp; \\
  x_{N,1}&amp;\dots &amp; x_{N,p} 
  \end{pmatrix}
\]</span></p>
<p>We stored this matrix in x:</p>
<pre class="r"><code>dim(x)</code></pre>
<pre><code>## [1] 1000  784</code></pre>
<p>We will now learn several useful operations related to matrix algebra. We use three of the motivating questions listed above.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>In matrix algebra, we have three main types of objects: <strong>scalars</strong>, <strong>vectors</strong>, and <strong>matrices</strong>.</p>
<ul>
<li><strong>Scalar</strong>:</li>
</ul>
<p><span class="math display">\[a = 1\]</span></p>
<ul>
<li><strong>Vector</strong>:</li>
</ul>
<p><span class="math display">\[\mathbf{X}_1 = \begin{pmatrix}x_{1,1}\\\vdots\\x_{N,1}\end{pmatrix}\]</span></p>
<ul>
<li><strong>Matrix</strong>:</li>
</ul></li>
</ul>
<p><span class="math display">\[ 
\mathbf{X} = [ \mathbf{X}_1 \mathbf{X}_2 ] = \begin{pmatrix}
x_{1,1}&amp;x_{1,2}\\
\vdots\\
x_{N,1}&amp;x_{N,2}
\end{pmatrix}
\]</span></p>
<ul>
<li>In R, we can extract the dimension of a matrix with the function <code>dim()</code>. We can convert a vector into a matrix using the function <code>as.matrix()</code>.</li>
</ul>
</div>
</div>
<div id="section-converting-a-vector-to-a-matrix" class="section level3">
<h3>Converting a Vector to a Matrix</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#converting-a-vector-to-a-matrix">textbook section - 33.1.2 Converting a vector to a matrix</a>.</p>
</div>
<p>It is often useful to convert a vector to a matrix. For example, because the variables are pixels on a grid, we can convert the rows of pixel intensities into a matrix representing this grid.</p>
<p>We can convert a vector into a matrix with the <code>matrix</code> function and specifying the number of rows and columns that the resulting matrix should have. The matrix is filled in <strong>by column</strong>: the first column is filled first, then the second and so on. This example helps illustrate:</p>
<pre class="r"><code>my_vector &lt;- 1:15
mat &lt;- matrix(my_vector, 5, 3)
mat</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    6   11
## [2,]    2    7   12
## [3,]    3    8   13
## [4,]    4    9   14
## [5,]    5   10   15</code></pre>
<p>We can fill by row by using the <code>byrow</code> argument. So, for example, to <em>transpose</em> the matrix <code>mat</code>, we can use:</p>
<pre class="r"><code>mat_t &lt;- matrix(my_vector, 3, 5, byrow = TRUE)
mat_t</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    2    3    4    5
## [2,]    6    7    8    9   10
## [3,]   11   12   13   14   15</code></pre>
<p>When we turn the columns into rows, we refer to the operations as <em>transposing</em> the matrix. The function <code>t</code> can be used to directly transpose a matrix:</p>
<pre class="r"><code>identical(t(mat), mat_t)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p><strong>Warning</strong>: The <code>matrix</code> function recycles values in the vector <strong>without warning</strong> if the product of columns and rows does not match the length of the vector:</p>
<pre class="r"><code>matrix(my_vector, 4, 5)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    5    9   13    2
## [2,]    2    6   10   14    3
## [3,]    3    7   11   15    4
## [4,]    4    8   12    1    5</code></pre>
<p>To put the pixel intensities of our, say, 3rd entry, which is a 4 into grid, we can use:</p>
<pre class="r"><code>grid &lt;- matrix(x[3,], 28, 28)</code></pre>
<p>To confirm that in fact we have done this correctly, we can use the function <code>image</code>, which shows an image of its third argument. The top of this plot is pixel 1, which is shown at the bottom so the image is flipped. To code below includes code showing how to flip it back:</p>
<pre class="r"><code>image(1:28, 1:28, grid)
image(1:28, 1:28, grid[, 28:1])</code></pre>
<p><img src="datsci_08_files/figure-html/matrix-image-1.png" width="768" /></p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>In R, we can <strong>convert a vector</strong> into a matrix with the <code>matrix()</code> function. The matrix is filled in by column, but we can fill by row by using the <code>byrow</code> argument. The function <code>t()</code> can be used to directly transpose a matrix.</p></li>
<li><p>Note that the matrix function <strong>recycles values in the vector</strong> without warning if the product of columns and rows does not match the length of the vector.</p></li>
</ul>
</div>
</div>
<div id="section-row-and-column-summaries-and-apply" class="section level3">
<h3>Row and Column Summaries and Apply</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#row-and-column-summaries">textbook section - 33.1.3 Row and column summaries</a>.</p>
</div>
<p>For the first task, related to total pixel darkness, we want to sum the values of each row and then visualize how these values vary by digit.</p>
<p>The function <code>rowSums</code> takes a matrix as input and computes the desired values:</p>
<pre class="r"><code>sums &lt;- rowSums(x)</code></pre>
<p>We can also compute the averages with <code>rowMeans</code> if we want the values to remain between 0 and 255:</p>
<pre class="r"><code>avg &lt;- rowMeans(x)</code></pre>
<p>Once we have this, we can simply generate a boxplot:</p>
<pre class="r"><code>tibble(labels = as.factor(y), row_averages = avg) %&gt;% 
  qplot(labels, row_averages, data = ., geom = &quot;boxplot&quot;) </code></pre>
<p><img src="datsci_08_files/figure-html/boxplot-of-digit-averages-1.png" width="624" /></p>
<p>From this plot we see that, not surprisingly, 1s use less ink than the other digits.</p>
<p>We can compute the column sums and averages using the function <code>colSums</code> and <code>colMeans</code>, respectively.</p>
<p>The <strong>matrixStats</strong> package adds functions that performs operations on each row or column very efficiently, including the functions <code>rowSds</code> and <code>colSds</code>.</p>
</div>
<div id="section-apply" class="section level3">
<h3><code>apply</code></h3>
<p>The functions just described are performing an operation similar to what <code>sapply</code> and the <strong>purrr</strong> function <code>map</code> do: apply the same function to a part of your object. In this case, the function is applied to either each row or each column. The <code>apply</code> function lets you apply any function, not just <code>sum</code> or <code>mean</code>, to a matrix. The first argument is the matrix, the second is the dimension, 1 for rows, 2 for columns, and the third is the function. So, for example, <code>rowMeans</code> can be written as:</p>
<pre class="r"><code>avgs &lt;- apply(x, 1, mean)</code></pre>
<p>But notice that just like with <code>sapply</code> and <code>map</code>, we can perform any function. So if we wanted the standard deviation for each column, we could write:</p>
<pre class="r"><code>sds &lt;- apply(x, 2, sd)</code></pre>
<p>The tradeoff for this flexibility is that these operations are not as fast as dedicated functions such as <code>rowMeans</code>.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The function <code>rowSums()</code> computes the sum of each row.</p></li>
<li><p>The function <code>rowMeans()</code> computes the average of each row.</p></li>
<li><p>We can compute the column sums and averages using the functions <code>colSums()</code> and <code>colMeans()</code>.</p></li>
<li><p>The <strong>matrixStats</strong> package adds functions that performs operations on each row or column very efficiently, including the functions <code>rowSds()</code> and <code>colSds()</code>.</p></li>
<li><p>The <code>apply()</code> function lets you apply any function to a matrix. The first argument is the <strong>matrix</strong>, the second is the <strong>dimension</strong> (1 for rows, 2 for columns), and the third is the <strong>function</strong>.</p></li>
</ul>
</div>
</div>
<div id="section-filtering-columns-based-on-summaries" class="section level3">
<h3>Filtering Columns Based on Summaries</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#filtering-columns-based-on-summaries">textbook section - 33.1.5 Filtering columns based on summaries</a>.</p>
</div>
<p>We now turn to task 2: studying the variation of each pixel and removing columns associated with pixels that don’t change much and thus do not inform the classification. Although a simplistic approach, we will quantify the variation of each pixel with its standard deviation across all entries. Since each column represents a pixel, we use the <code>colSds</code> function from the <strong>matrixStats</strong> package:</p>
<pre class="r"><code>library(matrixStats)
sds &lt;- colSds(x)</code></pre>
<p>A quick look at the distribution of these values shows that some pixels have very low entry to entry variability:</p>
<pre class="r"><code>qplot(sds, bins = &quot;30&quot;, color = I(&quot;black&quot;))</code></pre>
<p><img src="datsci_08_files/figure-html/sds-histogram-1.png" width="624" /></p>
<p>This makes sense since we don’t write in some parts of the box. Here is the variance plotted by location:</p>
<pre class="r"><code>image(1:28, 1:28, matrix(sds, 28, 28)[, 28:1])</code></pre>
<p><img src="datsci_08_files/figure-html/pixel-variance-1.png" width="50%" /></p>
<p>We see that there is little variation in the corners.</p>
<p>We could remove features that have no variation since these can’t help us predict. In the textbook <a href="https://rafalab.github.io/dsbook/r-basics.html#matrices">(Section - 2.4.7 Matrices)</a>, we described the operations used to extract columns:</p>
<pre class="r"><code>x[ ,c(351,352)]</code></pre>
<p>and rows:</p>
<pre class="r"><code>x[c(2,3),]</code></pre>
<p>We can also use logical indexes to determine which columns or rows to keep. So if we wanted to remove uninformative predictors from our matrix, we could write this one line of code:</p>
<pre class="r"><code>new_x &lt;- x[ ,colSds(x) &gt; 60]
dim(new_x)</code></pre>
<pre><code>## [1] 1000  314</code></pre>
<p>Only the columns for which the standard deviation is above 60 are kept, which removes over half the predictors.</p>
<p>Here we add an important warning related to subsetting matrices: if you select one column or one row, the result is no longer a matrix but a vector.</p>
<pre class="r"><code>class(x[,1])</code></pre>
<pre><code>## [1] &quot;integer&quot;</code></pre>
<pre class="r"><code>dim(x[1,])</code></pre>
<pre><code>## NULL</code></pre>
<p>However, we can preserve the matrix class by using the argument <code>drop=FALSE</code>:</p>
<pre class="r"><code>class(x[ , 1, drop=FALSE])</code></pre>
<pre><code>## [1] &quot;matrix&quot; &quot;array&quot;</code></pre>
<pre class="r"><code>dim(x[, 1, drop=FALSE])</code></pre>
<pre><code>## [1] 1000    1</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The operations used to extract columns: x[,c(351,352)].</p></li>
<li><p>The operations used to extract rows: x[c(2,3),].</p></li>
<li><p>We can also use logical indexes to determine which columns or rows to keep: new_x &lt;- x[ ,colSds(x) &gt; 60].</p></li>
<li><p><strong>Important note</strong>: if you select only one column or only one row, the result is no longer a matrix but a <strong>vector</strong>. We can <strong>preserve the matrix class</strong> by using the argument <code>drop=FALSE</code>.</p></li>
</ul>
</div>
</div>
<div id="section-indexing-with-matrices-and-binarizing-the-data" class="section level3">
<h3>Indexing with Matrices and Binarizing the Data</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#indexing-with-matrices">textbook section - 33.1.6 Indexing with matrices</a> and <a href="https://rafalab.github.io/dsbook/large-datasets.html#binarizing-the-data">textbook section - 33.1.7 Binarizing the data</a>.</p>
</div>
<p>We can quickly make a histogram of all the values in our dataset. We saw how we can turn vectors into matrices. We can also undo this and turn matrices into vectors. The operation will happen by row:</p>
<pre class="r"><code>mat &lt;- matrix(1:15, 5, 3)
as.vector(mat)</code></pre>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15</code></pre>
<p>To see a histogram of all our predictor data, we can use:</p>
<pre class="r"><code>qplot(as.vector(x), bins = 30, color = I(&quot;black&quot;))</code></pre>
<p><img src="datsci_08_files/figure-html/histogram-all-pixels-1.png" width="624" /></p>
<p>We notice a clear dichotomy which is explained as parts of the image with ink and parts without. If we think that values below, say, 50 are smudges, we can quickly make them zero using:</p>
<pre class="r"><code>new_x &lt;- x
new_x[new_x &lt; 50] &lt;- 0</code></pre>
<p>To see what this does, we look at a smaller matrix:</p>
<pre class="r"><code>mat &lt;- matrix(1:15, 5, 3)
mat[mat &lt; 3] &lt;- 0
mat</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    0    6   11
## [2,]    0    7   12
## [3,]    3    8   13
## [4,]    4    9   14
## [5,]    5   10   15</code></pre>
<p>We can also use logical operations with matrix logical:</p>
<pre class="r"><code>mat &lt;- matrix(1:15, 5, 3)
mat[mat &gt; 6 &amp; mat &lt; 12] &lt;- 0
mat</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    6    0
## [2,]    2    0   12
## [3,]    3    0   13
## [4,]    4    0   14
## [5,]    5    0   15</code></pre>
</div>
<div id="section-binarizing-the-data" class="section level3">
<h3>Binarizing the data</h3>
<p>The histogram above seems to suggest that this data is mostly binary. A pixel either has ink or does not. Using what we have learned, we can binarize the data using just matrix operations:</p>
<pre class="r"><code>bin_x &lt;- x
bin_x[bin_x &lt; 255/2] &lt;- 0 
bin_x[bin_x &gt; 255/2] &lt;- 1</code></pre>
<p>We can also convert to a matrix of logicals and then coerce to numbers like this:</p>
<pre class="r"><code>bin_X &lt;- (x &gt; 255/2)*1</code></pre>
<!--
We can see that at least the entry we looked at before does not change much:

<img src="datsci_08_files/figure-html/binarized-image-1.png" width="100%" />
-->
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>We can use logical operations with matrices:</li>
</ul>
<pre class="r"><code>mat &lt;- matrix(1:15, 5, 3)
mat[mat &gt; 6 &amp; mat &lt; 12] &lt;- 0
mat</code></pre>
<ul>
<li>We can also binarize the data using just matrix operations:</li>
</ul>
<pre class="r"><code>bin_x &lt;- x
bin_x[bin_x &lt; 255/2] &lt;- 0 
bin_x[bin_x &gt; 255/2] &lt;- 1</code></pre>
</div>
</div>
<div id="section-vectorization-for-matrices-and-matrix-algebra-operations" class="section level3">
<h3>Vectorization for Matrices and Matrix Algebra Operations</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#vectorization-for-matrices">textbook section - 33.1.8 Vectorization for matrices</a> and <a href="https://rafalab.github.io/dsbook/large-datasets.html#matrix-algebra-operations">textbook section - 33.1.9 Matrix algebra operations</a>.</p>
</div>
<p>In R, if we subtract a vector from a matrix, the first element of the vector is subtracted from the first row, the second element from the second row, and so on. Using mathematical notation, we would write it as follows:</p>
<p><span class="math display">\[
 \begin{pmatrix}
  X_{1,1}&amp;\dots &amp; X_{1,p} \\
  X_{2,1}&amp;\dots &amp; X_{2,p} \\
   &amp; \vdots &amp; \\
  X_{N,1}&amp;\dots &amp; X_{N,p} 
  \end{pmatrix}
-
\begin{pmatrix}
a_1\\\
a_2\\\
\vdots\\\
a_N
\end{pmatrix}
=
\begin{pmatrix}
  X_{1,1}-a_1&amp;\dots &amp; X_{1,p} -a_1\\
  X_{2,1}-a_2&amp;\dots &amp; X_{2,p} -a_2\\
   &amp; \vdots &amp; \\
  X_{N,1}-a_n&amp;\dots &amp; X_{N,p} -a_n
  \end{pmatrix}
\]</span></p>
<p>The same holds true for other arithmetic operations. This implies that we can scale each row of a matrix like this:</p>
<pre class="r"><code>(x - rowMeans(x)) / rowSds(x)</code></pre>
<p>If you want to scale each column, be careful since this approach does not work for columns. To perform a similar operation, we convert the columns to rows using the transpose <code>t</code>, proceed as above, and then transpose back:</p>
<pre class="r"><code>t(t(X) - colMeans(X))</code></pre>
<p>We can also use a function called <code>sweep</code> that works similarly to <code>apply</code>. It takes each entry of a vector and subtracts it from the corresponding row or column.</p>
<pre class="r"><code>X_mean_0 &lt;- sweep(x, 2, colMeans(x))</code></pre>
<p>The function <code>sweep</code> actually has another argument that lets you define the arithmetic operation. So to divide by the standard deviation, we do the following:</p>
<pre class="r"><code>x_mean_0 &lt;- sweep(x, 2, colMeans(x))
x_standardized &lt;- sweep(x_mean_0, 2, colSds(x), FUN = &quot;/&quot;)</code></pre>
</div>
<div id="section-matrix-algebra-operations" class="section level3">
<h3>Matrix algebra operations</h3>
<p>Finally, although we do not cover matrix algebra operations such as matrix multiplication, we share here the relevant commands for those that know the mathematics and want to learn the code:</p>
<p>1. Matrix multiplication is done with <code>%*%</code>. For example, the cross product is:</p>
<pre class="r"><code>t(x) %*% x</code></pre>
<p>2. We can compute the cross product directly with the function:</p>
<pre class="r"><code>crossprod(x)</code></pre>
<p>3. To compute the inverse of a function, we use <code>solve</code>. Here it is applied to the cross product:</p>
<pre class="r"><code>solve(crossprod(x))</code></pre>
<p>4. The QR decomposition is readily available by using the <code>qr</code> function:</p>
<pre class="r"><code>qr(x)</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>We can scale each row of a matrix using this line of code:</li>
</ul>
<pre class="r"><code>(x - rowMeans(x)) / rowSds(x)</code></pre>
<ul>
<li>To scale each column of a matrix, we use this code:</li>
</ul>
<pre class="r"><code>t(t(X) - colMeans(X))</code></pre>
<ul>
<li>We can also use a function called sweep() that works similarly to apply(). It takes each entry of a vector and subtracts it from the corresponding row or column:</li>
</ul>
<pre class="r"><code>X_mean_0 &lt;- sweep(x, 2, colMeans(x))</code></pre>
<ul>
<li><p>Matrix multiplication: t(x) %*% x</p></li>
<li><p>The cross product: crossprod(x)</p></li>
<li><p>The inverse of a function: solve(crossprod(x))</p></li>
<li><p>The QR decomposition: qr(x)</p></li>
</ul>
</div>
</div>
<div id="section-comprehension-check-working-with-matrices" class="section level3">
<h3>3.3 Comprehension Check: Working with Matrices</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-4-distance-knn-cross-validation-and-generative-models" class="section level2">
<h2>Section 4: Distance, Knn, Cross Validation, and Generative Models</h2>
<p>In the <strong>Distance, kNN, Cross Validation, and Generative Models</strong> section, you will learn about different types of discriminative and generative approaches for machine learning algorithms.</p>
<p>After completing this section, you will be able to:</p>
<ul>
<li><p>Use the <strong>k-nearest neighbors (kNN)</strong> algorithm.</p></li>
<li><p>Understand the problems of <strong>overtraining</strong> and <strong>oversmoothing</strong>.</p></li>
<li><p>Use <strong>cross-validation</strong> to reduce the <strong>true error</strong> and the <strong>apparent error</strong>.</p></li>
<li><p>Use <strong>generative models</strong> such as <strong>naive Bayes</strong>, <strong>quadratic discriminant analysis (qda)</strong>, and <strong>linear discriminant analysis (lda)</strong> for machine learning.</p></li>
</ul>
<p>This section has three parts: <strong>nearest neighbors</strong>, <strong>cross-validation</strong>, and <strong>generative models</strong>. There are comprehension checks periodically throughout.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your own learning.</p>
<div id="section-distance" class="section level3">
<h3>Distance</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#distance">textbook section - 33.3 Distance</a>.</p>
</div>
<p>Many of the analyses we perform with high-dimensional data relate directly or indirectly to distance. Most clustering and machine learning techniques rely on being able to define distance between observations, using features or predictors.</p>
</div>
<div id="section-euclidean-distance" class="section level3">
<h3>Euclidean distance</h3>
<p>As a review, let’s define the distance between two points, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, on a Cartesian plane.</p>
<p><img src="datsci_08_files/figure-html/euclidean-distance-1.png" width="65%" /></p>
<p>The Euclidean distance between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is simply:</p>
<p><span class="math display">\[
\mbox{dist}(A,B) = \sqrt{ (A_x-B_x)^2 + (A_y-B_y)^2}
\]</span></p>
<p>This definition applies to the case of one dimension, in which the distance between two numbers is simply the absolute value of their difference. So if our two one-dimensional numbers are <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, the distance is:</p>
<p><span class="math display">\[
\mbox{dist}(A,B) = \sqrt{ (A - B)^2 } = | A - B |
\]</span></p>
</div>
<div id="section-distance-in-higher-dimensions" class="section level3">
<h3>Distance in higher dimensions</h3>
<p>Earlier we introduced a training dataset with feature matrix measurements for 784 features. For illustrative purposes, we will look at a random sample of 2s and 7s.</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)

if(!exists(&quot;mnist&quot;)) mnist &lt;- read_mnist()

set.seed(1995)
ind &lt;- which(mnist$train$labels %in% c(2,7)) %&gt;% sample(500)
x &lt;- mnist$train$images[ind,]
y &lt;- mnist$train$labels[ind]</code></pre>
<p>The predictors are in <code>x</code> and the labels in <code>y</code>.</p>
<p>For the purposes of, for example, smoothing, we are interested in describing distance between observation; in this case, digits. Later, for the purposes of selecting features, we might also be interested in finding pixels that <em>behave similarly</em> across samples.</p>
<p>To define distance, we need to know what <em>points</em> are since mathematical distance is computed between points. With high dimensional data, points are no longer on the Cartesian plane. Instead, points are in higher dimensions. We can no longer visualize them and need to think abstractly. For example, predictors <span class="math inline">\(\mathbf{X}_i\)</span> are defined as a point in 784 dimensional space: <span class="math inline">\(\mathbf{X}_i = (x_{i,1},\dots,x_{i,784})^\top\)</span>.</p>
<p>Once we define points this way, the Euclidean distance is defined very similarly as it was for two dimensions. For example, the distance between the predictors for two observations, say observations <span class="math inline">\(i=1\)</span> and <span class="math inline">\(i=2\)</span>, is:</p>
<p><span class="math display">\[
\mbox{dist}(1,2) = \sqrt{ \sum_{j=1}^{784} (x_{1,j}-x_{2,j })^2 }
\]</span></p>
<p>This is just one non-negative number, just as it is for two dimensions.</p>
</div>
<div id="section-euclidean-distance-example" class="section level3">
<h3>Euclidean distance example</h3>
<p>The labels for the first three observations are:</p>
<pre class="r"><code>y[1:3]</code></pre>
<pre><code>## [1] 7 2 7</code></pre>
<p>The vectors of predictors for each of these observations are:</p>
<pre class="r"><code>x_1 &lt;- x[1,]
x_2 &lt;- x[2,]
x_3 &lt;- x[3,]</code></pre>
<p>The first two numbers are seven and the third one is a 2. We expect the distances between the same number:</p>
<pre class="r"><code>sqrt(sum((x_1 - x_2)^2))</code></pre>
<pre><code>## [1] 3273.385</code></pre>
<p>to be smaller than between different numbers:</p>
<pre class="r"><code>sqrt(sum((x_1 - x_3)^2))</code></pre>
<pre><code>## [1] 2311.005</code></pre>
<pre class="r"><code>sqrt(sum((x_2 - x_3)^2))</code></pre>
<pre><code>## [1] 2635.91</code></pre>
<p>As expected, the 7s are closer to each other.</p>
<p>A faster way to compute this is using matrix algebra:</p>
<pre class="r"><code>sqrt(crossprod(x_1 - x_2))</code></pre>
<pre><code>##          [,1]
## [1,] 3273.385</code></pre>
<pre class="r"><code>sqrt(crossprod(x_1 - x_3))</code></pre>
<pre><code>##          [,1]
## [1,] 2311.005</code></pre>
<pre class="r"><code>sqrt(crossprod(x_2 - x_3))</code></pre>
<pre><code>##         [,1]
## [1,] 2635.91</code></pre>
<p>We can also compute <strong>all</strong> the distances at once relatively quickly using the function <code>dist</code>, which computes the distance between each row and produces an object of class <code>dist</code>:</p>
<pre class="r"><code>d &lt;- dist(x)
class(d)</code></pre>
<pre><code>## [1] &quot;dist&quot;</code></pre>
<p>There are several machine learning related functions in R that take objects of class <code>dist</code> as input. To access the entries using row and column indices, we need to coerce it into a matrix. We can see the distance we calculated above like this:</p>
<pre class="r"><code>as.matrix(d)[1:3,1:3]</code></pre>
<pre><code>##          1        2        3
## 1    0.000 3273.385 2311.005
## 2 3273.385    0.000 2635.910
## 3 2311.005 2635.910    0.000</code></pre>
<p>We can quickly see an image of these distances using this code:</p>
<pre class="r"><code>image(as.matrix(d))</code></pre>
<p>If we order this distance by the labels, we can see that, in general, the twos are closer to each other and the sevens are closer to each other:</p>
<pre class="r"><code>image(as.matrix(d)[order(y), order(y)])</code></pre>
<p><img src="datsci_08_files/figure-html/diatance-image-ordered-1.png" width="50%" /></p>
<p>One thing we notice here is that there appears to be more uniformity in how the sevens are drawn, since they appear to be closer (more red) to other sevens than twos are to other twos.</p>
</div>
<div id="section-predictor-space" class="section level3">
<h3>Predictor space</h3>
<p><em>Predictor space</em> is a concept that is often used to describe machine learning algorithms. The term <em>space</em> refers to a mathematical definition that we don’t describe in detail here. Instead, we provide a simplified explanation to help understand the term predictor space when used in the context of machine learning algorithms.</p>
<p>The predictor space can be thought of as the collection of all possible vectors of predictors that should be considered for the machine learning challenge in question. Each member of the space is referred to as a <em>point</em>. For example, in the 2 or 7 dataset, the predictor space consists of all pairs <span class="math inline">\((x_1, x_2)\)</span> such that both <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are within 0 and 1. This particular <em>space</em> can be represented graphically as a square. In the MNIST dataset the predictor space consists of all 784-th dimensional vectors with each vector element an integer between 0 and 256. An essential element of a predictor space is that we need to define a function that provides the distance between any two points. In most cases we use Euclidean distance, but there are other possibilities. A particular case in which we can’t simply use Euclidean distance is when we have categorical predictors.</p>
<p>Defining a predictor space is useful in machine learning because we do things like define neighborhoods of points, as required by many smoothing techniques. For example, we can define a neighborhood as all the points that are within 2 units away from a predefined center. If the points are two-dimensional and we use Euclidean distance, this neighborhood is graphically represented as a circle with radius 2. In three dimensions the neighborhood is a sphere. We will soon learn about algorithms that partition the space into non-overlapping regions and then make different predictions for each region using the data in the region.</p>
</div>
<div id="section-distance-between-predictors" class="section level3">
<h3>Distance between predictors</h3>
<p>We can also compute distances between predictors. If <span class="math inline">\(N\)</span> is the number of observations, the distance between two predictors, say 1 and 2, is:</p>
<p><span class="math display">\[
\mbox{dist}(1,2) = \sqrt{ \sum_{i=1}^{N} (x_{i,1}-x_{i,2})^2 }
\]</span></p>
<p>To compute the distance between all pairs of the 784 predictors, we can transpose the matrix first and then use <code>dist</code>:</p>
<pre class="r"><code>d &lt;- dist(t(x))
dim(as.matrix(d))</code></pre>
<pre><code>## [1] 784 784</code></pre>
<!--
An interesting thing to note here is that if we pick a predictor (a pixel), we can see which pixels are close. That is, the pair of pixels either have ink in the same images (small distance) or they don't (large distance). The distance between, for example, and all other pixels is given by:


```r
d_492 <- as.matrix(d)[492,]
```
 
We can now see the spatial pattern of these distances with the following code:


```r
image(1:28, 1:28, matrix(d_492, 28, 28))
```

<img src="datsci_08_files/figure-html/distnace-rows-1.png" width="384" />

Not surprisingly, points physically nearby are mathematically closer.
-->
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Most clustering and machine learning techniques rely on being able to define distance between observations, using features or predictors.</p></li>
<li><p>With high dimensional data, a quick way to compute all the distances at once is to use the function <code>dist()</code>, which computes the distance between each row and produces an object of class <code>dist()</code>:</p></li>
</ul>
<pre class="r"><code>d &lt;- dist(x)</code></pre>
<ul>
<li>We can also compute distances between predictors. If <span class="math inline">\(N\)</span> is the number of observations, the distance between two predictors, say 1 and 2, is:</li>
</ul>
<p><span class="math display">\[
\mbox{dist}(1,2) = \sqrt{ \sum_{i=1}^{N} (x_{i,1}-x_{i,2})^2 }
\]</span></p>
<ul>
<li>To compute the distance between all pairs of the 784 predictors, we can transpose the matrix first and then use <code>dist()</code>:</li>
</ul>
<pre class="r"><code>d &lt;- dist(t(x))</code></pre>
</div>
</div>
<div id="section-comprehension-check-distance" class="section level3">
<h3>4.1 Comprehension Check: Distance</h3>
<p>Insert assessment here</p>
</div>
<div id="section-knn" class="section level3">
<h3>Knn</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/cross-validation.html#knn-cv-intro">textbook section - Chapter 29 Cross validation</a>.</p>
</div>
<p>In this section we introduce cross validation, one of the most important ideas in machine learning. Here we focus on the conceptual and mathematical aspects. We will describe how to implement cross validation in practice with the <strong>caret</strong> package later, in the textbook <a href="https://rafalab.github.io/dsbook/caret.html#caret-cv">(Section - 30.2 Cross validation)</a> in the next section. To motivate the concept, we will use the two predictor digits data presented in the textbook <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#two-or-seven">(Section - 27.8 Case study: is it a 2 or a 7?)</a> and introduce, for the first time, an actual machine learning algorithm: k-nearest neighbors (kNN).</p>
</div>
<div id="section-motivation-with-k-nearest-neighbors" class="section level3">
<h3>Motivation with k-nearest neighbors</h3>
<p>Let’s start by loading the data and showing a plot of the predictors with outcome represented with color.</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)
data(&quot;mnist_27&quot;)
mnist_27$test%&gt;% ggplot(aes(x_1, x_2, color = y)) +  geom_point()</code></pre>
<p><img src="datsci_08_files/figure-html/mnist-27-data-1.png" width="624" /></p>
<p>We will use these data to estimate the conditional probability function</p>
<p><span class="math display">\[
p(x_1, x_2) = \mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2).
\]</span> as defined in the textbook <a href="https://rafalab.github.io/dsbook/smoothing.html#smoothing-ml-connection">(Section - 28.4 Connecting smoothing to machine learning)</a>. With k-nearest neighbors (kNN) we estimate <span class="math inline">\(p(x_1, x_2)\)</span> in a similar way to bin smoothing. However, as we will see, kNN is easier to adapt to multiple dimensions. First we define the distance between all observations based on the features. Then, for any point <span class="math inline">\((x_1,x_2)\)</span> for which we want an estimate of <span class="math inline">\(p(x_1, x_2)\)</span>, we look for the <span class="math inline">\(k\)</span> nearest points to <span class="math inline">\((x_1,x_2)\)</span> and then take an average of the 0s and 1s associated with these points. We refer to the set of points used to compute the average as the <em>neighborhood</em>. Due to the connection we described earlier between conditional expectations and conditional probabilities, this gives us a <span class="math inline">\(\hat{p}(x_1,x_2)\)</span>, just like the bin smoother gave us an estimate of a trend. As with bin smoothers, we can control the flexibility of our estimate, in this case through the <span class="math inline">\(k\)</span> parameter: larger <span class="math inline">\(k\)</span>s result in smoother estimates, while smaller <span class="math inline">\(k\)</span>s result in more flexible and more wiggly estimates.</p>
<p>To implement the algorithm, we can use the <code>knn3</code> function from the <strong>caret</strong> package. Looking at the help file for this package, we see that we can call it in one of two ways. We will use the first in which we specify a <em>formula</em> and a data frame. The data frame contains all the data to be used. The formula has the form <code>outcome ~ predictor_1 + predictor_2 + predictor_3</code> and so on. Therefore, we would type <code>y ~ x_1 + x_2</code>. If we are going to use all the predictors, we can use the <code>.</code> like this <code>y ~ .</code>. The final call looks like this:</p>
<pre class="r"><code>library(caret)
knn_fit &lt;- knn3(y ~ ., data = mnist_27$train)</code></pre>
<p>For this function, we also need to pick a parameter: the number of neighbors to include. Let’s start with the default <span class="math inline">\(k=5\)</span>.</p>
<pre class="r"><code>knn_fit &lt;- knn3(y ~ ., data = mnist_27$train, k = 5)</code></pre>
<p>In this case, since our dataset is balanced and we care just as much about sensitivity as we do about specificity, we will use accuracy to quantify performance.</p>
<p>The <code>predict</code> function for <code>knn</code> produces a probability for each class. We keep the probability of being a 7 as the estimate <span class="math inline">\(\hat{p}(x_1, x_2)\)</span></p>
<pre class="r"><code>y_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = &quot;class&quot;)
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##    0.815</code></pre>
<p>In the textbook <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#two-or-seven">(Section - 27.8 Case study: is it a 2 or a 7?)</a> we used linear regression to generate an estimate.</p>
<pre class="r"><code>fit_lm &lt;- mnist_27$train %&gt;% 
  mutate(y = ifelse(y == 7, 1, 0)) %&gt;% 
  lm(y ~ x_1 + x_2, data = .)
p_hat_lm &lt;- predict(fit_lm, mnist_27$test)
y_hat_lm &lt;- factor(ifelse(p_hat_lm &gt; 0.5, 7, 2))
confusionMatrix(y_hat_lm, mnist_27$test$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##     0.75</code></pre>
<p>And we see that kNN, with the default parameter, already beats regression. To see why this is the case, we will plot <span class="math inline">\(\hat{p}(x_1, x_2)\)</span> and compare it to the true conditional probability <span class="math inline">\(p(x_1, x_2)\)</span>:</p>
<p><img src="datsci_08_files/figure-html/knn-fit-1.png" width="100%" /></p>
<p>We see that kNN better adapts to the non-linear shape of <span class="math inline">\(p(x_1, x_2)\)</span>. However, our estimate has some islands of blue in the red area, which intuitively does not make much sense. This is due to what we call <em>over-training</em>. We describe over-training in detail below. Over-training is the reason that we have higher accuracy in the train set compared to the test set:</p>
<pre class="r"><code>y_hat_knn &lt;- predict(knn_fit, mnist_27$train, type = &quot;class&quot;)
confusionMatrix(y_hat_knn, mnist_27$train$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##   0.8825</code></pre>
<pre class="r"><code>y_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = &quot;class&quot;)
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##    0.815</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p><strong>K-nearest neighbors (kNN)</strong> estimates the conditional probabilities in a similar way to bin smoothing. However, kNN is easier to adapt to multiple dimensions.</p></li>
<li><p>Using kNN, for any point <span class="math inline">\((x_1, x_2)\)</span> for which we want an estimate of <span class="math inline">\(p(x_1, x_2)\)</span>, we look for the <strong>k nearest points</strong> to <span class="math inline">\((x_1, x_2)\)</span> and take an average of the 0s and 1s associated with these points. We refer to the set of points used to compute the average as the <strong>neighborhood</strong>. Larger values of k result in smoother estimates, while smaller values of k result in more flexible and more wiggly estimates.</p></li>
<li><p>To implement the algorithm, we can use the <code>knn3()</code> function from the <strong>caret</strong> package. There are two ways to call this function:</p></li>
</ul>
<p>1. We need to specify a formula and a data frame. The formula looks like this: outcome∼predictor1+predictor2+predictor3. The <code>predict()</code> function for knn3 produces a probability for each class.</p>
<p>2. We can also call the function with the first argument being the matrix predictors and the second a vector of outcomes, like this:</p>
<pre class="r"><code>x &lt;- as.matrix(mnist_27$train[,2:3])
y &lt;- mnist_27$train$y
knn_fit &lt;- knn3(x,y)</code></pre>
</div>
</div>
<div id="section-overtraining-and-oversmoothing" class="section level3">
<h3>Overtraining and Oversmoothing</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/cross-validation.html#over-training">textbook section - 29.1.1 Over-training</a> and <a href="https://rafalab.github.io/dsbook/cross-validation.html#over-smoothing">textbook section - 29.1.2 Over-smoothing</a>.</p>
</div>
<p>Over-training is at its worst when we set <span class="math inline">\(k=1\)</span>. With <span class="math inline">\(k=1\)</span>, the estimate for each <span class="math inline">\((x_1, x_2)\)</span> in the training set is obtained with just the <span class="math inline">\(y\)</span> corresponding to that point. In this case, if the <span class="math inline">\((x_1, x_2)\)</span> are unique, we will obtain perfect accuracy in the training set because each point is used to predict itself. Remember that if the predictors are not unique and have different outcomes for at least one set of predictors, then it is impossible to predict perfectly.</p>
<p>Here we fit a kNN model with <span class="math inline">\(k=1\)</span>:</p>
<pre class="r"><code>knn_fit_1 &lt;- knn3(y ~ ., data = mnist_27$train, k = 1)
y_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$train, type = &quot;class&quot;)
confusionMatrix(y_hat_knn_1, mnist_27$train$y)$overall[[&quot;Accuracy&quot;]]</code></pre>
<pre><code>## [1] 0.9975</code></pre>
<p>However, the test set accuracy is actually worse than logistic regression:</p>
<pre class="r"><code>y_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$test, type = &quot;class&quot;)
confusionMatrix(y_hat_knn_1, mnist_27$test$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##     0.74</code></pre>
<p>We can see the over-fitting problem in this figure. <img src="datsci_08_files/figure-html/knn-1-overfit-1.png" width="100%" /></p>
<p>The black curves denote the decision rule boundaries.</p>
<p>The estimate <span class="math inline">\(\hat{p}(x_1, x_2)\)</span> follows the training data too closely (left). You can see that in the training set, boundaries have been drawn to perfectly surround a single red point in a sea of blue. Because most points <span class="math inline">\((x_1, x_2)\)</span> are unique, the prediction is either 1 or 0 and the prediction for that point is the associated label. However, once we introduce the training set (right), we see that many of these small islands now have the opposite color and we end up making several incorrect predictions.</p>
</div>
<div id="section-over-smoothing" class="section level3">
<h3>Over-smoothing</h3>
<p>Although not as badly as with the previous examples, we saw that with <span class="math inline">\(k=5\)</span> we also over-trained. Hence, we should consider a larger <span class="math inline">\(k\)</span>. Let’s try, as an example, a much larger number: <span class="math inline">\(k=401\)</span>.</p>
<pre class="r"><code>knn_fit_401 &lt;- knn3(y ~ ., data = mnist_27$train, k = 401)
y_hat_knn_401 &lt;- predict(knn_fit_401, mnist_27$test, type = &quot;class&quot;)
confusionMatrix(y_hat_knn_401, mnist_27$test$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##     0.79</code></pre>
<p>This turns out to be similar to regression: <img src="datsci_08_files/figure-html/mnist-27-glm-est-1.png" width="100%" /></p>
<p>This size of <span class="math inline">\(k\)</span> is so large that it does not permit enough flexibility. We call this <em>over-smoothing</em>.</p>
</div>
<div id="section-picking-the-k-in-knn" class="section level3">
<h3>Picking the <span class="math inline">\(k\)</span> in kNN</h3>
<p>So how do we pick <span class="math inline">\(k\)</span>? In principle we want to pick the <span class="math inline">\(k\)</span> that maximizes accuracy, or minimizes the expected MSE as defined in the textbook <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#loss-function">(Section - 27.4.8 The loss function)</a>. The goal of cross validation is to estimate these quantities for any given algorithm and set of tuning parameters such as <span class="math inline">\(k\)</span>. To understand why we need a special method to do this let’s repeat what we did above but for different values of <span class="math inline">\(k\)</span>:</p>
<pre class="r"><code>ks &lt;- seq(3, 251, 2)</code></pre>
<p>We do this using <code>map_df</code> function to repeat the above for each one.</p>
<pre class="r"><code>library(purrr)
accuracy &lt;- map_df(ks, function(k){
  fit &lt;- knn3(y ~ ., data = mnist_27$train, k = k)
  
  y_hat &lt;- predict(fit, mnist_27$train, type = &quot;class&quot;)
  cm_train &lt;- confusionMatrix(y_hat, mnist_27$train$y)
  train_error &lt;- cm_train$overall[&quot;Accuracy&quot;]
  
  y_hat &lt;- predict(fit, mnist_27$test, type = &quot;class&quot;)
  cm_test &lt;- confusionMatrix(y_hat, mnist_27$test$y)
  test_error &lt;- cm_test$overall[&quot;Accuracy&quot;]
  
  tibble(train = train_error, test = test_error)
})</code></pre>
<p>Note that we estimate accuracy by using both the training set and the test set. We can now plot the accuracy estimates for each value of <span class="math inline">\(k\)</span>:</p>
<p><img src="datsci_08_files/figure-html/accuracy-vs-k-knn-1.png" width="624" /></p>
<p>First, note that the estimate obtained on the training set is generally lower than the estimate obtained with the test set, with the difference larger for smaller values of <span class="math inline">\(k\)</span>. This is due to over-training. Also note that the accuracy versus <span class="math inline">\(k\)</span> plot is quite jagged. We do not expect this because small changes in <span class="math inline">\(k\)</span> should not affect the algorithm’s performance too much. The jaggedness is explained by the fact that the accuracy is computed on a sample and therefore is a random variable. This demonstrates why we prefer to minimize the expected loss rather than the loss we observe with one dataset.</p>
<p>If we were to use these estimates to pick the <span class="math inline">\(k\)</span> that maximizes accuracy, we would use the estimates built on the test data:</p>
<pre class="r"><code>ks[which.max(accuracy$test)]</code></pre>
<pre><code>## [1] 41</code></pre>
<pre class="r"><code>max(accuracy$test)</code></pre>
<pre><code>## [1] 0.86</code></pre>
<p>Another reason we need a better estimate of accuracy is that if we use the test set to pick this <span class="math inline">\(k\)</span>, we should not expect the accompanying accuracy estimate to extrapolate to the real world. This is because even here we broke a golden rule of machine learning: we selected the <span class="math inline">\(k\)</span> using the test set. Cross validation also provides an estimate that takes this into account.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p><strong>Over-training</strong> is the reason that we have higher accuracy in the train set compared to the test set. Over-training is at its worst when we set <span class="math inline">\(k=1\)</span>. With <span class="math inline">\(k=1\)</span>, the estimate for each <span class="math inline">\(x_1, x_2\)</span> in the training set is obtained with just the <span class="math inline">\(y\)</span> corresponding to that point.</p></li>
<li><p>When we try a larger <span class="math inline">\(k\)</span>, the <span class="math inline">\(k\)</span> might be so large that it does not permit enough flexibility. We call this <strong>over-smoothing</strong>.</p></li>
<li><p>Note that if we use the test set to pick this <span class="math inline">\(k\)</span>, we should not expect the accompanying accuracy estimate to extrapolate to the real world. This is because even here we broke a golden rule of machine learning: <strong>we selected the <span class="math inline">\(k\)</span> using the test set</strong>. <strong>Cross validation</strong> also provides an estimate that takes this into account.</p></li>
</ul>
</div>
</div>
<div id="section-comprehension-check-nearest-neighbors" class="section level3">
<h3>4.1 Comprehension Check: Nearest Neighbors</h3>
<p>Insert assessment here</p>
</div>
<div id="section-k-fold-cross-validation" class="section level3">
<h3>k-fold Cross-validation</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/cross-validation.html#mathematical-description-of-cross-validation">textbook section - 29.2 Mathematical description of cross validation</a> and <a href="https://rafalab.github.io/dsbook/cross-validation.html#k-fold-cross-validation">textbook section - 29.3 K-fold cross validation</a>.</p>
</div>
<p>In the textbook <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#loss-function">(Section - 27.4.8 The loss function)</a>, we described that a common goal of machine learning is to find an algorithm that produces predictors <span class="math inline">\(\hat{Y}\)</span> for an outcome <span class="math inline">\(Y\)</span> that minimizes the MSE:</p>
<p><span class="math display">\[
\mbox{MSE} = \mbox{E}\left\{ \frac{1}{N}\sum_{i=1}^N (\hat{Y}_i - Y_i)^2 \right\}
\]</span> When all we have at our disposal is one dataset, we can estimate the MSE with the observed MSE like this:</p>
<p><span class="math display">\[
\hat{\mbox{MSE}} = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
\]</span> These two are often referred to as the <em>true error</em> and <em>apparent error</em>, respectively.</p>
<p>There are two important characteristics of the apparent error we should always keep in mind:</p>
<p>1. Because our data is random, the apparent error is a random variable. For example, the dataset we have may be a random sample from a larger population. An algorithm may have a lower apparent error than another algorithm due to luck.</p>
<p>2. If we train an algorithm on the same dataset that we use to compute the apparent error, we might be overtraining. In general, when we do this, the apparent error will be an underestimate of the true error. We will see an extreme example of this with k-nearest neighbors.</p>
<p>Cross validation is a technique that permits us to alleviate both these problems. To understand cross validation, it helps to think of the true error, a theoretical quantity, as the average of many apparent errors obtained by applying the algorithm to <span class="math inline">\(B\)</span> new random samples of the data, none of them used to train the algorithm. As shown in a previous section, we think of the true error as:</p>
<p><span class="math display">\[
\frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b - y_i^b\right)^2 
\]</span> with <span class="math inline">\(B\)</span> a large number that can be thought of as practically infinite. As already mentioned, this is a theoretical quantity because we only have available one set of outcomes: <span class="math inline">\(y_1, \dots, y_n\)</span>. Cross validation is based on the idea of imitating the theoretical setup above as best we can with the data we have. To do this, we have to generate a series of different random samples. There are several approaches we can use, but the general idea for all of them is to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error.</p>
</div>
<div id="section-k-fold-cross-validation-1" class="section level3">
<h3>K-fold cross validation</h3>
<p>The first one we describe is <em>K-fold cross validation</em>. Generally speaking, a machine learning challenge starts with a dataset (blue in the image below). We need to build an algorithm using this dataset that will eventually be used in completely independent datasets (yellow).</p>
<p><img src="images/cv-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>But we don’t get to see these independent datasets.</p>
<p><img src="images/cv-2.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>So to imitate this situation, we carve out a piece of our dataset and pretend it is an independent dataset: we divide the dataset into a <em>training set</em> (blue) and a <em>test set</em> (red). We will train our algorithm exclusively on the training set and use the test set only for evaluation purposes.</p>
<p>We usually try to select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large so that we obtain a stable estimate of the loss without fitting an impractical number of models. Typical choices are to use 10%-20% of the data for testing.</p>
<p><img src="images/cv-3.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Let’s reiterate that it is indispensable that we not use the test set at all: not for filtering out rows, not for selecting features, nothing!</p>
<p>Now this presents a new problem because for most machine learning algorithms we need to select parameters, for example the number of neighbors <span class="math inline">\(k\)</span> in k-nearest neighbors. Here, we will refer to the set of parameters as <span class="math inline">\(\lambda\)</span>. We need to optimize algorithm parameters without using our test set and we know that if we optimize and evaluate on the same dataset, we will overtrain. This is where cross validation is most useful.</p>
<p>For each set of algorithm parameters being considered, we want an estimate of the MSE and then we will choose the parameters with the smallest MSE. Cross validation provides this estimate.</p>
<p>First, before we start the cross validation procedure, it is important to fix all the algorithm parameters. Although we will train the algorithm on the set of training sets, the parameters <span class="math inline">\(\lambda\)</span> will be the same across all training sets. We will use <span class="math inline">\(\hat{y}_i(\lambda)\)</span> to denote the predictors obtained when we use parameters <span class="math inline">\(\lambda\)</span>.</p>
<p>So, if we are going to imitate this definition:</p>
<p><span class="math display">\[
\mbox{MSE}(\lambda) = \frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b(\lambda) - y_i^b\right)^2 
\]</span></p>
<p>we want to consider datasets that can be thought of as an independent random sample and we want to do this several times. With K-fold cross validation, we do it <span class="math inline">\(K\)</span> times. In the cartoons, we are showing an example that uses <span class="math inline">\(K=5\)</span>.</p>
<p>We will eventually end up with <span class="math inline">\(K\)</span> samples, but let’s start by describing how to construct the first: we simply pick <span class="math inline">\(M=N/K\)</span> observations at random (we round if <span class="math inline">\(M\)</span> is not a round number) and think of these as a random sample <span class="math inline">\(y_1^b, \dots, y_M^b\)</span>, with <span class="math inline">\(b=1\)</span>. We call this the validation set:</p>
<p><img src="images/cv-4.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Now we can fit the model in the training set, then compute the apparent error on the independent set:</p>
<p><span class="math display">\[
\hat{\mbox{MSE}}_b(\lambda) = \frac{1}{M}\sum_{i=1}^M \left(\hat{y}_i^b(\lambda) - y_i^b\right)^2 
\]</span></p>
<p>Note that this is just one sample and will therefore return a noisy estimate of the true error. This is why we take <span class="math inline">\(K\)</span> samples, not just one. In K-cross validation, we randomly split the observations into <span class="math inline">\(K\)</span> non-overlapping sets:</p>
<p><img src="images/cv-5.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Now we repeat the calculation above for each of these sets <span class="math inline">\(b=1,\dots,K\)</span> and obtain <span class="math inline">\(\hat{\mbox{MSE}}_1(\lambda),\dots, \hat{\mbox{MSE}}_K(\lambda)\)</span>. Then, for our final estimate, we compute the average:</p>
<p><span class="math display">\[
\hat{\mbox{MSE}}(\lambda) = \frac{1}{B} \sum_{b=1}^K \hat{\mbox{MSE}}_b(\lambda)
\]</span></p>
<p>and obtain an estimate of our loss. A final step would be to select the <span class="math inline">\(\lambda\)</span> that minimizes the MSE.</p>
<p>We have described how to use cross validation to optimize parameters. However, we now have to take into account the fact that the optimization occurred on the training data and therefore we need an estimate of our final algorithm based on data that was not used to optimize the choice. Here is where we use the test set we separated early on:</p>
<p><img src="images/cv-6.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>We can do cross validation again:</p>
<p><img src="images/cv-7.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>and obtain a final estimate of our expected loss. However, note that this means that our entire compute time gets multiplied by <span class="math inline">\(K\)</span>. You will soon learn that performing this task takes time because we are performing many complex computations. As a result, we are always looking for ways to reduce this time. For the final evaluation, we often just use the one test set.</p>
<p>Once we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the optimized parameters.</p>
<p><img src="images/cv-8.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Now how do we pick the cross validation <span class="math inline">\(K\)</span>? Large values of <span class="math inline">\(K\)</span> are preferable because the training data better imitates the original dataset. However, larger values of <span class="math inline">\(K\)</span> will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason, the choices of <span class="math inline">\(K=5\)</span> and <span class="math inline">\(K=10\)</span> are popular.</p>
<p>One way we can improve the variance of our final estimate is to take more samples. To do this, we would no longer require the training set to be partitioned into non-overlapping sets. Instead, we would just pick <span class="math inline">\(K\)</span> sets of some size at random.</p>
<p>One popular version of this technique, at each fold, picks observations at random with replacement (which means the same observation can appear twice). This approach has some advantages (not discussed here) and is generally referred to as the <em>bootstrap</em>. In fact, this is the default approach in the <strong>caret</strong> package. We describe how to implement cross validation with the <strong>caret</strong> package in the next section. In the next section, we include an explanation of how the bootstrap works in general.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>For <strong><span class="math inline">\(k\)</span>-fold cross validation</strong>, we divide the dataset into a training set and a test set. We train our algorithm exclusively on the training set and use the test set only for evaluation purposes.</p></li>
<li><p>For each set of algorithm parameters being considered, we want an <strong>estimate of the MSE and then we will choose the parameters with the smallest MSE</strong>. In <span class="math inline">\(k\)</span>-fold cross validation, we randomly split the observations into <span class="math inline">\(k\)</span> non-overlapping sets, and repeat the calculation for MSE for each of these sets. Then, we compute the average MSE and obtain an estimate of our loss. Finally, we can select the optimal parameter that minimized the MSE.</p></li>
<li><p>In terms of how to select <span class="math inline">\(k\)</span> for cross validation, <strong>larger values of <span class="math inline">\(k\)</span> are preferable but they will also take much more</strong> computational time. For this reason, the choices of <span class="math inline">\(k\)</span>=5 and <span class="math inline">\(k\)</span>=10 are common.</p></li>
</ul>
</div>
</div>
<div id="section-comprehension-check-cross-validation" class="section level3">
<h3>4.2 Comprehension Check: Cross-validation</h3>
<p>Insert assessment here</p>
</div>
<div id="section-bootstrap" class="section level3">
<h3>Bootstrap</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/cross-validation.html#bootstrap">textbook section - 29.5 Bootstrap</a>.</p>
</div>
<p>Suppose the income distribution of your population is as follows:</p>
<pre class="r"><code>set.seed(1995)
n &lt;- 10^6
income &lt;- 10^(rnorm(n, log10(45000), log10(3)))
qplot(log10(income), bins = 30, color = I(&quot;black&quot;))</code></pre>
<p><img src="datsci_08_files/figure-html/income-distribution-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>The population median is:</p>
<pre class="r"><code>m &lt;- median(income)
m</code></pre>
<pre><code>## [1] 44938.54</code></pre>
<p>Suppose we don’t have access to the entire population, but want to estimate the median <span class="math inline">\(m\)</span>. We take a sample of 100 and estimate the population median <span class="math inline">\(m\)</span> with the sample median <span class="math inline">\(M\)</span>:</p>
<pre class="r"><code>N &lt;- 100
X &lt;- sample(income, N)
median(X)</code></pre>
<pre><code>## [1] 38461.33</code></pre>
<p>Can we construct a confidence interval? What is the distribution of <span class="math inline">\(M\)</span> ?</p>
<p>Because we are simulating the data, we can use a Monte Carlo simulation to learn the distribution of <span class="math inline">\(M\)</span>.</p>
<pre class="r"><code>library(gridExtra)
B &lt;- 10^4
M &lt;- replicate(B, {
  X &lt;- sample(income, N)
  median(X)
})
p1 &lt;- qplot(M, bins = 30, color = I(&quot;black&quot;))
p2 &lt;- qplot(sample = scale(M), xlab = &quot;theoretical&quot;, ylab = &quot;sample&quot;) + 
  geom_abline()
grid.arrange(p1, p2, ncol = 2)</code></pre>
<p><img src="datsci_08_files/figure-html/median-is-normal-1.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>If we know this distribution, we can construct a confidence interval. The problem here is that, as we have already described, in practice we do not have access to the distribution. In the past, we have used the Central Limit Theorem, but the CLT we studied applies to averages and here we are interested in the median. We can see that the 95% confidence interval based on CLT</p>
<pre class="r"><code>median(X) + 1.96 * sd(X) / sqrt(N) * c(-1, 1)</code></pre>
<pre><code>## [1] 21017.93 55904.72</code></pre>
<p>is quite different from the confidence interval we would generate if we know the actual distribution of <span class="math inline">\(M\)</span>:</p>
<pre class="r"><code>quantile(M, c(0.025, 0.975))</code></pre>
<pre><code>##     2.5%    97.5% 
## 34437.72 59049.59</code></pre>
<p>The bootstrap permits us to approximate a Monte Carlo simulation without access to the entire distribution. The general idea is relatively simple. We act as if the observed sample is the population. We then sample (with replacement) datasets, of the same sample size as the original dataset. Then we compute the summary statistic, in this case the median, on these <em>bootstrap samples</em>.</p>
<p>Theory tells us that, in many situations, the distribution of the statistics obtained with bootstrap samples approximate the distribution of our actual statistic. This is how we construct bootstrap samples and an approximate distribution:</p>
<pre class="r"><code>B &lt;- 10^4
M_star &lt;- replicate(B, {
  X_star &lt;- sample(X, N, replace = TRUE)
  median(X_star)
})</code></pre>
<p>Note a confidence interval constructed with the bootstrap is much closer to one constructed with the theoretical distribution:</p>
<pre class="r"><code>quantile(M_star, c(0.025, 0.975))</code></pre>
<pre><code>##     2.5%    97.5% 
## 30252.82 56908.62</code></pre>
<p>For more on the Bootstrap, including corrections one can apply to improve these confidence intervals, please consult the book <em>An introduction to the bootstrap</em> by Efron, B., &amp; Tibshirani, R. J.</p>
<p><em>Note that we can use ideas similar to those used in the bootstrap in cross validation: instead of dividing the data into equal partitions, we simply bootstrap many times.</em></p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>When we don’t have access to the entire population, we can use <strong>bootstrap</strong> to estimate the population median <span class="math inline">\(m\)</span>.</p></li>
<li><p>The bootstrap permits us to <strong>approximate a Monte Carlo simulation</strong> without access to the entire distribution. The general idea is relatively simple. We act as if the observed sample is the population. We then sample datasets (with replacement) of the same sample size as the original dataset. Then we compute the summary statistic, in this case the median, on this bootstrap sample.</p></li>
<li><p>Note that we can use ideas similar to those used in the bootstrap in <strong>cross validation</strong>: instead of dividing the data into equal partitions, we simply bootstrap many times.</p></li>
</ul>
</div>
</div>
<div id="section-comprehension-check-bootstrap" class="section level3">
<h3>4.2 Comprehension Check: Bootstrap</h3>
<p>Insert assessment here</p>
</div>
<div id="section-generative-models" class="section level3">
<h3>Generative Models</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#generative-models">textbook section - 31.7 Generative models</a>.</p>
</div>
<p>We have described how, when using squared loss, the conditional expectation/probabilities provide the best approach to developing a decision rule. In a binary case, the smallest true error we can achieve is determined by Bayes’ rule, which is a decision rule based on the true conditional probability:</p>
<p><span class="math display">\[
p(\mathbf{x}) = \mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x}) 
\]</span></p>
<p>We have described several approaches to estimating <span class="math inline">\(p(\mathbf{x})\)</span>. In all these approaches, we estimate the conditional probability directly and do not consider the distribution of the predictors. In machine learning, these are referred to as <em>discriminative</em> approaches.</p>
<p>However, Bayes’ theorem tells us that knowing the distribution of the predictors <span class="math inline">\(\mathbf{X}\)</span> may be useful. Methods that model the joint distribution of <span class="math inline">\(Y\)</span> and <span class="math inline">\(\mathbf{X}\)</span> are referred to as <em>generative models</em> (we model how the entire data, <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(Y\)</span>, are generated). We start by describing the most general generative model, Naive Bayes, and then proceed to describe two specific cases, quadratic discriminant analysis (QDA) and linear discriminant analysis (LDA).</p>
</div>
<div id="section-naive-bayes" class="section level3">
<h3>Naive Bayes</h3>
<p>Recall that Bayes rule tells us that we can rewrite <span class="math inline">\(p(\mathbf{x})\)</span> like this:</p>
<p><span class="math display">\[
p(\mathbf{x}) = \mbox{Pr}(Y=1|\mathbf{X}=\mathbf{x}) = \frac{f_{\mathbf{X}|Y=1}(\mathbf{x}) \mbox{Pr}(Y=1)}
{ f_{\mathbf{X}|Y=0}(\mathbf{x})\mbox{Pr}(Y=0)  + f_{\mathbf{X}|Y=1}(\mathbf{x})\mbox{Pr}(Y=1) }
\]</span></p>
<p>with <span class="math inline">\(f_{\mathbf{X}|Y=1}\)</span> and <span class="math inline">\(f_{\mathbf{X}|Y=0}\)</span> representing the distribution functions of the predictor <span class="math inline">\(\mathbf{X}\)</span> for the two classes <span class="math inline">\(Y=1\)</span> and <span class="math inline">\(Y=0\)</span>. The formula implies that if we can estimate these conditional distributions of the predictors, we can develop a powerful decision rule. However, this is a big <em>if</em>. As we go forward, we will encounter examples in which <span class="math inline">\(\mathbf{X}\)</span> has many dimensions and we do not have much information about the distribution. In these cases, Naive Bayes will be practically impossible to implement. However, there are instances in which we have a small number of predictors (not much more than 2) and many categories in which generative models can be quite powerful. We describe two specific examples and use our previously described case studies to illustrate them.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p><strong>Discriminative approaches</strong> estimate the conditional probability directly and do not consider the distribution of the predictors.</p></li>
<li><p><strong>Generative models</strong> are methods that model the joint distribution and <span class="math inline">\(X\)</span> (we model how the entire data, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, are generated).</p></li>
</ul>
</div>
</div>
<div id="section-naive-bayes-1" class="section level3">
<h3>Naive Bayes</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#generative-models">textbook section - 31.7 Generative models</a>.</p>
</div>
<p>Let’s start with a very simple and uninteresting, yet illustrative, case: the example related to predicting sex from height.</p>
<pre class="r"><code>library(tidyverse)
library(caret)

library(dslabs)
data(&quot;heights&quot;)

y &lt;- heights$height
set.seed(1995)
test_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set &lt;- heights %&gt;% slice(-test_index)
test_set &lt;- heights %&gt;% slice(test_index)</code></pre>
<p>In this case, the Naive Bayes approach is particularly appropriate because we know that the normal distribution is a good approximation for the conditional distributions of height given sex for both classes <span class="math inline">\(Y=1\)</span> (female) and <span class="math inline">\(Y=0\)</span> (male). This implies that we can approximate the conditional distributions <span class="math inline">\(f_{X|Y=1}\)</span> and <span class="math inline">\(f_{X|Y=0}\)</span> by simply estimating averages and standard deviations from the data:</p>
<pre class="r"><code>params &lt;- train_set %&gt;% 
  group_by(sex) %&gt;% 
  summarize(avg = mean(height), sd = sd(height))
params</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sex"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["avg"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["sd"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"Female","2":"64.76467","3":"4.138041"},{"1":"Male","2":"69.22692","3":"3.569045"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The prevalence, which we will denote with <span class="math inline">\(\pi = \mbox{Pr}(Y=1)\)</span>, can be estimated from the data with:</p>
<pre class="r"><code>pi &lt;- train_set %&gt;% summarize(pi=mean(sex==&quot;Female&quot;)) %&gt;% pull(pi)
pi</code></pre>
<pre><code>## [1] 0.2118321</code></pre>
<p>Now we can use our estimates of average and standard deviation to get an actual rule:</p>
<pre class="r"><code>x &lt;- test_set$height

f0 &lt;- dnorm(x, params$avg[2], params$sd[2])
f1 &lt;- dnorm(x, params$avg[1], params$sd[1])

p_hat_bayes &lt;- f1*pi / (f1*pi + f0*(1 - pi))</code></pre>
<p>Our Naive Bayes estimate <span class="math inline">\(\hat{p}(x)\)</span> looks a lot like our logistic regression estimate:</p>
<p><img src="datsci_08_files/figure-html/conditional-prob-glm-fit-2-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>In fact, we can show that the Naive Bayes approach is similar to the logistic regression prediction mathematically. However, we leave the demonstration to a more advanced text, such as the <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Elements of Statistical Learning</a>. We can see that they are similar empirically by comparing the two resulting curves.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>Bayes’ rule:</li>
</ul>
<p><span class="math display">\[
p(\mathbf{x}) = \mbox{Pr}(Y=1|\mathbf{X}=\mathbf{x}) = \frac{f_{\mathbf{X}|Y=1}(\mathbf{x}) \mbox{Pr}(Y=1)}
{ f_{\mathbf{X}|Y=0}(\mathbf{x})\mbox{Pr}(Y=0)  + f_{\mathbf{X}|Y=1}(\mathbf{x})\mbox{Pr}(Y=1) }
\]</span></p>
<p>with <span class="math inline">\(f_{\mathbf{X}|Y=1}\)</span> and <span class="math inline">\(f_{\mathbf{X}|Y=0}\)</span>representing the distribution functions of the predictor <span class="math inline">\(X\)</span> for the two classes <span class="math inline">\(Y=1\)</span> and <span class="math inline">\(Y=0\)</span>.</p>
</div>
</div>
<div id="section-controlling-prevalence" class="section level3">
<h3>Controlling Prevalence</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#controlling-prevalence">textbook section - 31.7.2 Controlling prevalence</a>.</p>
</div>
<p>One useful feature of the Naive Bayes approach is that it includes a parameter to account for differences in prevalence. Using our sample, we estimated <span class="math inline">\(f_{X|Y=1}\)</span>, <span class="math inline">\(f_{X|Y=0}\)</span> and <span class="math inline">\(\pi\)</span>. If we use hats to denote the estimates, we can write <span class="math inline">\(\hat{p}(x)\)</span> as:</p>
<p><span class="math display">\[
\hat{p}(x)= \frac{\hat{f}_{X|Y=1}(x) \hat{\pi}}
{ \hat{f}_{X|Y=0}(x)(1-\hat{\pi}) + \hat{f}_{X|Y=1}(x)\hat{\pi} }
\]</span></p>
<p>As we discussed earlier, our sample has a much lower prevalence, 0.21, than the general population. So if we use the rule <span class="math inline">\(\hat{p}(x)&gt;0.5\)</span> to predict females, our accuracy will be affected due to the low sensitivity:</p>
<pre class="r"><code>y_hat_bayes &lt;- ifelse(p_hat_bayes &gt; 0.5, &quot;Female&quot;, &quot;Male&quot;)
sensitivity(data = factor(y_hat_bayes), reference = factor(test_set$sex))</code></pre>
<pre><code>## [1] 0.2125984</code></pre>
<p>Again, this is because the algorithm gives more weight to specificity to account for the low prevalence:</p>
<pre class="r"><code>specificity(data = factor(y_hat_bayes), reference = factor(test_set$sex))</code></pre>
<pre><code>## [1] 0.9674185</code></pre>
<p>This is due mainly to the fact that <span class="math inline">\(\hat{\pi}\)</span> is substantially less than 0.5, so we tend to predict <code>Male</code> more often. It makes sense for a machine learning algorithm to do this in our sample because we do have a higher percentage of males. But if we were to extrapolate this to a general population, our overall accuracy would be affected by the low sensitivity.</p>
<p>The Naive Bayes approach gives us a direct way to correct this since we can simply force <span class="math inline">\(\hat{\pi}\)</span> to be whatever value we want it to be. So to balance specificity and sensitivity, instead of changing the cutoff in the decision rule, we could simply change <span class="math inline">\(\hat{\pi}\)</span> to 0.5 like this:</p>
<pre class="r"><code>p_hat_bayes_unbiased &lt;- f1 * 0.5 / (f1 * 0.5 + f0 * (1 - 0.5)) 
y_hat_bayes_unbiased &lt;- ifelse(p_hat_bayes_unbiased&gt; 0.5, &quot;Female&quot;, &quot;Male&quot;)</code></pre>
<p>Note the difference in sensitivity with a better balance:</p>
<pre class="r"><code>sensitivity(factor(y_hat_bayes_unbiased), factor(test_set$sex))</code></pre>
<pre><code>## [1] 0.6929134</code></pre>
<pre class="r"><code>specificity(factor(y_hat_bayes_unbiased), factor(test_set$sex))</code></pre>
<pre><code>## [1] 0.8320802</code></pre>
<p>The new rule also gives us a very intuitive cutoff between 66-67, which is about the middle of the female and male average heights:</p>
<pre class="r"><code>qplot(x, p_hat_bayes_unbiased, geom = &quot;line&quot;) + 
  geom_hline(yintercept = 0.5, lty = 2) + 
  geom_vline(xintercept = 67, lty = 2)</code></pre>
<p><img src="datsci_08_files/figure-html/naive-with-good-prevalence-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>The Naive Bayes approach includes a parameter to account for differences in prevalence <span class="math inline">\(\pi=\mbox{Pr}(Y=1)\)</span>. If we use hats to denote the estimates, we can write <span class="math inline">\(\hat{p}(x)\)</span> as:</li>
</ul>
<p><span class="math display">\[
\hat{p}(x)=\mbox{Pr}(Y=1|\mathbf{X}=\mathbf{x}) = \frac{\hat{f}_{X|Y=1}(x) \hat{\pi}}
{ \hat{f}_{X|Y=0}(x)(1-\hat{\pi}) + \hat{f}_{X|Y=1}(x)\mbox{Pr}(Y=1) }
\]</span></p>
<ul>
<li>The Naive Bayes approach gives us a direct way to correct the imbalance between sensitivity and specificity by simply forcing <span class="math inline">\(\hat{\pi}\)</span> to be whatever value we want it to be in order to better balance specificity and sensitivity.</li>
</ul>
</div>
</div>
<div id="section-qda-and-lda" class="section level3">
<h3>qda and lda</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#quadratic-discriminant-analysis">textbook section - 31.7.3 Quadratic discriminant analysis</a> and <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#linear-discriminant-analysis">textbook section - 31.7.4 Linear discriminant analysis</a>.</p>
</div>
<p>Quadratic Discriminant Analysis (QDA) is a version of Naive Bayes in which we assume that the distributions <span class="math inline">\(p_{\mathbf{X}|Y=1}(x)\)</span> and <span class="math inline">\(p_{\mathbf{X}|Y=0}(\mathbf{x})\)</span> are multivariate normal. The simple example we described in the previous section is actually QDA. Let’s now look at a slightly more complicated case: the 2 or 7 example.</p>
<pre class="r"><code>data(&quot;mnist_27&quot;)</code></pre>
<p>In this case, we have two predictors so we assume each one is bivariate normal. This implies that we need to estimate two averages, two standard deviations, and a correlation for each case <span class="math inline">\(Y=1\)</span> and <span class="math inline">\(Y=0\)</span>. Once we have these, we can approximate the distributions <span class="math inline">\(f_{X_1,X_2|Y=1}\)</span> and <span class="math inline">\(f_{X_1, X_2|Y=0}\)</span>. We can easily estimate parameters from the data:</p>
<pre class="r"><code>params &lt;- mnist_27$train %&gt;% 
  group_by(y) %&gt;% 
  summarize(avg_1 = mean(x_1), avg_2 = mean(x_2), 
            sd_1= sd(x_1), sd_2 = sd(x_2), 
            r = cor(x_1, x_2))
params</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["y"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["avg_1"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["avg_2"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["sd_1"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["sd_2"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["r"],"name":[6],"type":["dbl"],"align":["right"]}],"data":[{"1":"2","2":"0.1287798","3":"0.2831542","4":"0.07017481","5":"0.05779893","6":"0.4007501"},{"1":"7","2":"0.2341133","3":"0.2881862","4":"0.07188006","5":"0.10479565","6":"0.4551607"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Here we provide a visual way of showing the approach. We plot the data and use contour plots to give an idea of what the two estimated normal densities look like (we show the curve representing a region that includes 95% of the points):</p>
<pre class="r"><code>mnist_27$train %&gt;% mutate(y = factor(y)) %&gt;% 
  ggplot(aes(x_1, x_2, fill = y, color=y)) + 
  geom_point(show.legend = FALSE) + 
  stat_ellipse(type=&quot;norm&quot;, lwd = 1.5)</code></pre>
<p><img src="datsci_08_files/figure-html/qda-explained-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>This defines the following estimate of <span class="math inline">\(f(x_1, x_2)\)</span>.</p>
<p>We can use the <code>train</code> function from the <strong>caret</strong> package to fit the model and obtain predictors:</p>
<pre class="r"><code>library(caret)
train_qda &lt;- train(y ~ ., method = &quot;qda&quot;, data = mnist_27$train)</code></pre>
<p>We see that we obtain relatively good accuracy:</p>
<pre class="r"><code>y_hat &lt;- predict(train_qda, mnist_27$test)
confusionMatrix(y_hat, mnist_27$test$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##     0.82</code></pre>
<p>The estimated conditional probability looks relatively good, although it does not fit as well as the kernel smoothers:</p>
<p><img src="datsci_08_files/figure-html/qda-estimate-1.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>One reason QDA does not work as well as the kernel methods is perhaps because the assumption of normality does not quite hold. Although for the 2s it seems reasonable, for the 7s it does seem to be off. Notice the slight curvature in the points for the 7s:</p>
<pre class="r"><code>mnist_27$train %&gt;% mutate(y = factor(y)) %&gt;% 
  ggplot(aes(x_1, x_2, fill = y, color=y)) + 
  geom_point(show.legend = FALSE) + 
  stat_ellipse(type=&quot;norm&quot;) +
  facet_wrap(~y)</code></pre>
<p><img src="datsci_08_files/figure-html/qda-does-not-fit-1.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>QDA can work well here, but it becomes harder to use as the number of predictors increases. Here we have 2 predictors and had to compute 4 means, 4 SDs, and 2 correlations. How many parameters would we have if instead of 2 predictors, we had 10? The main problem comes from estimating correlations for 10 predictors. With 10, we have 45 correlations for each class. In general, the formula is <span class="math inline">\(K\times p(p-1)/2\)</span>, which gets big fast. Once the number of parameters approaches the size of our data, the method becomes impractical due to overfitting.</p>
</div>
<div id="section-linear-discriminant-analysis" class="section level3">
<h3>Linear discriminant analysis</h3>
<p>A relatively simple solution to the problem of having too many parameters is to assume that the correlation structure is the same for all classes, which reduces the number of parameters we need to estimate.</p>
<p>In this case, we would compute just one pair of standard deviations and one correlation, <!--so the parameters would look something like this:


```r
params <- mnist_27$train %>% 
  group_by(y) %>% 
  summarize(avg_1 = mean(x_1), avg_2 = mean(x_2), 
            sd_1= sd(x_1), sd_2 = sd(x_2), 
            r = cor(x_1,x_2))

params <- params %>% mutate(sd_1 = mean(sd_1), sd_2=mean(sd_2), r=mean(r))
params 
```

<div data-pagedtable="false">
  <script data-pagedtable-source type="application/json">
{"columns":[{"label":["y"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["avg_1"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["avg_2"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["sd_1"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["sd_2"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["r"],"name":[6],"type":["dbl"],"align":["right"]}],"data":[{"1":"2","2":"0.1287798","3":"0.2831542","4":"0.07102743","5":"0.08129729","6":"0.4279554"},{"1":"7","2":"0.2341133","3":"0.2881862","4":"0.07102743","5":"0.08129729","6":"0.4279554"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
--> and the distributions looks like this:</p>
<p><img src="datsci_08_files/figure-html/lda-explaine1-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Now the size of the ellipses as well as the angle are the same. This is because they have the same standard deviations and correlations.</p>
<p>We can fit the LDA model using <strong>caret</strong>:</p>
<pre class="r"><code>train_lda &lt;- train(y ~ ., method = &quot;lda&quot;, data = mnist_27$train)
y_hat &lt;- predict(train_lda, mnist_27$test)
confusionMatrix(y_hat, mnist_27$test$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##     0.75</code></pre>
<p>When we force this assumption, we can show mathematically that the boundary is a line, just as with logistic regression. For this reason, we call the method <em>linear</em> discriminant analysis (LDA). Similarly, for QDA, we can show that the boundary must be a quadratic function.</p>
<p><img src="datsci_08_files/figure-html/lda-estimate-1.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>In the case of LDA, the lack of flexibility does not permit us to capture the non-linearity in the true conditional probability function.</p>
</div>
<div id="section-connection-to-distance" class="section level3">
<h3>Connection to distance</h3>
<p>The normal density is:</p>
<p><span class="math display">\[
p(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp\left\{ - \frac{(x-\mu)^2}{\sigma^2}\right\}
\]</span></p>
<p>If we remove the constant <span class="math inline">\(1/(\sqrt{2\pi} \sigma)\)</span> and then take the log, we get:</p>
<p><span class="math display">\[
- \frac{(x-\mu)^2}{\sigma^2}
\]</span></p>
<p>which is the negative of a distance squared scaled by the standard deviation. For higher dimensions, the same is true except the scaling is more complex and involves correlations.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p><strong>Quadratic discriminant analysis (QDA)</strong> is a version of Naive Bayes in which we assume that the distributions <span class="math inline">\(p_{\mathbf{X}|Y=1}(x)\)</span> and <span class="math inline">\(p_{\mathbf{X}|Y=0}(\mathbf{x})\)</span> are multivariate normal.</p></li>
<li><p>QDA can work well with a few predictors, but it becomes <strong>harder to use as the number of predictors increases</strong>. Once the number of parameters approaches the size of our data, the method becomes impractical due to overfitting.</p></li>
<li><p>Forcing the assumption that all predictors share the same standard deviations and correlations, the boundary will be a line, just as with logistic regression. For this reason, we call the method <strong>linear discriminant analysis (LDA)</strong>.</p></li>
<li><p>In the case of LDA, the lack of flexibility <strong>does not permit us to capture the non-linearity</strong> in the true conditional probability function.</p></li>
</ul>
</div>
</div>
<div id="section-case-study-more-than-three-classes" class="section level3">
<h3>Case Study: More than Three Classes</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#case-study-more-than-three-classes">textbook section - 31.8 Case study: more than three classes</a>.</p>
</div>
<p>We can generate an example with three categories like this:</p>
<pre class="r"><code>if(!exists(&quot;mnist&quot;)) mnist &lt;- read_mnist()
set.seed(3456)
index_127 &lt;- sample(which(mnist$train$labels %in% c(1,2,7)), 2000)
y &lt;- mnist$train$labels[index_127] 
x &lt;- mnist$train$images[index_127,]
index_train &lt;- createDataPartition(y, p=0.8, list = FALSE)
## get the quadrants
row_column &lt;- expand.grid(row=1:28, col=1:28) 
upper_left_ind &lt;- which(row_column$col &lt;= 14 &amp; row_column$row &lt;= 14)
lower_right_ind &lt;- which(row_column$col &gt; 14 &amp; row_column$row &gt; 14)
## binarize the values. Above 200 is ink, below is no ink
x &lt;- x &gt; 200 
## proportion of pixels in lower right quadrant
x &lt;- cbind(rowSums(x[ ,upper_left_ind])/rowSums(x), 
           rowSums(x[ ,lower_right_ind])/rowSums(x)) 
##save data
train_set &lt;- data.frame(y = factor(y[index_train]),
                        x_1 = x[index_train,1], x_2 = x[index_train,2])
test_set &lt;- data.frame(y = factor(y[-index_train]),
                       x_1 = x[-index_train,1], x_2 = x[-index_train,2])</code></pre>
<p>Here is the training data:</p>
<pre class="r"><code>train_set %&gt;% ggplot(aes(x_1, x_2, color=y)) + geom_point()</code></pre>
<p><img src="datsci_08_files/figure-html/mnist-27-training-data-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>We can use the <strong>caret</strong> package to train the QDA model:</p>
<pre class="r"><code>train_qda &lt;- train(y ~ ., method = &quot;qda&quot;, data = train_set)</code></pre>
<p>Now we estimate three conditional probabilities (although they have to add to 1):</p>
<pre class="r"><code>predict(train_qda, test_set, type = &quot;prob&quot;) %&gt;% head()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["1"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["2"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["7"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.76552737","2":"0.230426894","3":"0.004045739","_rn_":"1"},{"1":"0.20310581","2":"0.725142233","3":"0.071751954","_rn_":"2"},{"1":"0.53959543","2":"0.459086283","3":"0.001318291","_rn_":"3"},{"1":"0.03925568","2":"0.094193777","3":"0.866550547","_rn_":"4"},{"1":"0.96000924","2":"0.009356292","3":"0.030634472","_rn_":"5"},{"1":"0.98652887","2":"0.007239538","3":"0.006231591","_rn_":"6"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Our predictions are one of the three classes:</p>
<pre class="r"><code>predict(train_qda, test_set) %&gt;% head()</code></pre>
<pre><code>## [1] 1 2 1 7 1 1
## Levels: 1 2 7</code></pre>
<p>The confusion matrix is therefore a 3 by 3 table:</p>
<pre class="r"><code>confusionMatrix(predict(train_qda, test_set), test_set$y)$table</code></pre>
<pre><code>##           Reference
## Prediction   1   2   7
##          1 111   9  11
##          2  10  86  21
##          7  21  28 102</code></pre>
<p>The accuracy is 0.7493734</p>
<p>Note that for sensitivity and specificity, we have a pair of values for <strong>each</strong> class. To define these terms, we need a binary outcome. We therefore have three columns: one for each class as the positives and the other two as the negatives.</p>
<p>To visualize what parts of the region are called 1, 2, and 7 we now need three colors:</p>
<p><img src="datsci_08_files/figure-html/three-classes-plot-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>The accuracy for LDA, 0.6290727, is much worse because the model is more rigid. This is what the decision rule looks like:</p>
<p><img src="datsci_08_files/figure-html/lda-too-rigid-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>The results for kNN</p>
<pre class="r"><code>train_knn &lt;- train(y ~ ., method = &quot;knn&quot;, data = train_set,
                   tuneGrid = data.frame(k = seq(15, 51, 2)))</code></pre>
<p>are much better with an accuracy of 0.7493734. The decision rule looks like this:</p>
<p><img src="datsci_08_files/figure-html/three-classes-knn-better-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Note that one of the limitations of generative models here is due to the lack of fit of the normal assumption, in particular for class 1.</p>
<pre class="r"><code>train_set %&gt;% mutate(y = factor(y)) %&gt;% 
  ggplot(aes(x_1, x_2, fill = y, color=y)) + 
  geom_point(show.legend = FALSE) + 
  stat_ellipse(type=&quot;norm&quot;) </code></pre>
<p><img src="datsci_08_files/figure-html/three-classes-lack-of-fit-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Generative models can be very powerful, but only when we are able to successfully approximate the joint distribution of predictors conditioned on each class.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>In this case study, we will briefly give a slightly more complex example: one with <strong>3 classes instead of 2</strong>. Then we will fit QDA, LDA, and KNN models for prediction.</p></li>
<li><p>Generative models can be very powerful, but only when we are able to <strong>successfully approximate the joint distribution</strong> of predictors conditioned on each class.</p></li>
</ul>
</div>
</div>
<div id="section-comprehension-check-generative-models" class="section level3">
<h3>4.3 Comprehension Check: Generative Models</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-5-classification-with-more-than-two-classes-and-the-caret-package" class="section level2">
<h2>Section 5: Classification with More than Two Classes and the Caret Package</h2>
<p>In the <strong>Classification with More than Two Classes and the Caret Package</strong> section, you will learn how to overcome the curse of dimensionality using methods that adapt to higher dimensions and how to use the caret package to implement many different machine learning algorithms.</p>
<p>After completing this section, you will be able to:</p>
<ul>
<li><p>Use <strong>classification and regression trees</strong>.</p></li>
<li><p>Use <strong>classification (decision) trees</strong>.</p></li>
<li><p>Apply <strong>random forests</strong> to address the shortcomings of decision trees.</p></li>
<li><p>Use the <strong>caret</strong> package to implement a variety of machine learning algorithms.</p></li>
</ul>
<p>This section has three parts: <strong>classification with more than two classes</strong>, <strong>caret package</strong>, and a <strong>set of exercises</strong> on the Titanic. There are comprehension checks after the first two parts and an assessment in the third part.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your own learning.</p>
<div id="section-trees-motivation" class="section level3">
<h3>Trees Motivation</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#the-curse-of-dimensionality">textbook section - 31.10.1 The curse of dimensionality</a>.</p>
</div>
</div>
<div id="section-the-curse-of-dimensionality" class="section level3">
<h3>The curse of dimensionality</h3>
<p>We described how methods such as LDA and QDA are not meant to be used with many predictors <span class="math inline">\(p\)</span> because the number of parameters that we need to estimate becomes too large. For example, with the digits example <span class="math inline">\(p=784\)</span>, we would have over 600,000 parameters with LDA, and we would multiply that by the number of classes for QDA. Kernel methods such as kNN or local regression do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to as the <em>curse of dimensionality</em>. The <em>dimension</em> here refers to the fact that when we have <span class="math inline">\(p\)</span> predictors, the distance between two observations is computed in <span class="math inline">\(p\)</span>-dimensional space.</p>
<p>A useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility.</p>
<p>For example, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10th of data. Then it’s easy to see that our windows have to be of size 0.1:</p>
<p><img src="datsci_08_files/figure-html/curse-of-dim-1.png" width="50%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Now, for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point. If we want to include 10% of the data, then we need to increase the size of each side of the square to <span class="math inline">\(\sqrt{.10} \approx .316\)</span>:</p>
<p><img src="datsci_08_files/figure-html/curse-of-dim-2-1.png" width="50%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Using the same logic, if we want to include 10% of the data in a three-dimensional space, then the side of each cube is <span class="math inline">\(\sqrt[3]{.10} \approx 0.464\)</span>. In general, to include 10% of the data in a case with <span class="math inline">\(p\)</span> dimensions, we need an interval with each side of size <span class="math inline">\(\sqrt[p]{.10}\)</span> of the total. This proportion gets close to 1 quickly, and if the proportion is 1 it means we include all the data and are no longer smoothing.</p>
<pre class="r"><code>library(tidyverse)
p &lt;- 1:100
qplot(p, .1^(1/p), ylim = c(0,1))</code></pre>
<p><img src="datsci_08_files/figure-html/curse-of-dim-4-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>By the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset.</p>
<p>Here we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes while still producing models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on regression and decision trees and their extension to random forests.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>LDA and QDA are <strong>not meant to be used with many predictors <span class="math inline">\(p\)</span></strong> because the number of parameters needed to be estimated becomes too large.</p></li>
<li><p><strong>Curse of dimensionality</strong>: For kernel methods such as kNN or local regression, when they have multiple predictors used, the span/neighborhood/window made to include a given percentage of the data become large. With larger neighborhoods, our methods lose flexibility. The dimension here refers to the fact that when we have <span class="math inline">\(p\)</span> predictors, the distance between two observations is computed in <span class="math inline">\(p\)</span>-dimensional space.</p></li>
</ul>
</div>
</div>
<div id="section-classification-and-regression-trees-cart" class="section level3">
<h3>Classification and Regression Trees (CART)</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#cart-motivation">textbook section - 31.10.2 CART motivation</a>.</p>
</div>
<p>To motivate this section, we will use a new dataset that includes the breakdown of the composition of olive oil into 8 fatty acids:</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)
data(&quot;olive&quot;)
names(olive)</code></pre>
<pre><code>##  [1] &quot;region&quot;      &quot;area&quot;        &quot;palmitic&quot;    &quot;palmitoleic&quot; &quot;stearic&quot;    
##  [6] &quot;oleic&quot;       &quot;linoleic&quot;    &quot;linolenic&quot;   &quot;arachidic&quot;   &quot;eicosenoic&quot;</code></pre>
<p>For illustrative purposes, we will try to predict the region using the fatty acid composition values as predictors.</p>
<pre class="r"><code>table(olive$region)</code></pre>
<pre><code>## 
## Northern Italy       Sardinia Southern Italy 
##            151             98            323</code></pre>
<p>We remove the <code>area</code> column because we won’t use it as a predictor.</p>
<pre class="r"><code>olive &lt;- select(olive, -area)</code></pre>
<p>Let’s very quickly try to predict the region using kNN:</p>
<pre class="r"><code>library(caret)
fit &lt;- train(region ~ .,  method = &quot;knn&quot;, 
             tuneGrid = data.frame(k = seq(1, 15, 2)), 
             data = olive)
ggplot(fit)</code></pre>
<p><img src="datsci_08_files/figure-html/olive-knn-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>We see that using just one neighbor, we can predict relatively well. However, a bit of data exploration reveals that we should be able to do even better. For example, if we look at the distribution of each predictor stratified by region we see that eicosenoic is only present in Southern Italy and that linoleic separates Northern Italy from Sardinia.</p>
<pre class="r"><code>olive %&gt;% gather(fatty_acid, percentage, -region) %&gt;%
  ggplot(aes(region, percentage, fill = region)) +
  geom_boxplot() +
  facet_wrap(~fatty_acid, scales = &quot;free&quot;, ncol = 4) +
  theme(axis.text.x = element_blank(), legend.position=&quot;bottom&quot;)</code></pre>
<p><img src="datsci_08_files/figure-html/olive-eda-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>This implies that we should be able to build an algorithm that predicts perfectly! We can see this clearly by plotting the values for eicosenoic and linoleic.</p>
<pre class="r"><code>olive %&gt;% 
  ggplot(aes(eicosenoic, linoleic, color = region)) + 
  geom_point() +
  geom_vline(xintercept = 0.065, lty = 2) + 
  geom_segment(x = -0.2, y = 10.54, xend = 0.065, yend = 10.54, 
               color = &quot;black&quot;, lty = 2)</code></pre>
<p><img src="datsci_08_files/figure-html/olive-two-predictors-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>In the textbook <a href="https://rafalab.github.io/dsbook/large-datasets.html#predictor-space">(Section - 33.3.4 Predictor space)</a> we define predictor spaces. The predictor space here consists of eight-dimensional points with values between 0 and 100. In the plot above, we show the space defined by the two predictors eicosenoic and linoleic, and, by eye, we can construct a prediction rule that partitions the predictor space so that each partition contains only outcomes of a one category. This in turn can be used to define an algorithm with perfect accuracy. Specifically, we define the following decision rule. If eicosenoic is larger than 0.065, predict Southern Italy. If not, then if linoleic is larger than <span class="math inline">\(10.535\)</span>, predict Sardinia, and if lower, predict Northern Italy. We can draw this decision tree like this:</p>
<p><img src="datsci_08_files/figure-html/olive-tree-1.png" width="50%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Decision trees like this are often used in practice. For example, to decide on a person’s risk of poor outcome after having a heart attack, doctors use the following:</p>
<p><img src="images/Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png" width="50%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>(Source: <a href="https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&amp;mirid=1&amp;type=2">Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-184</a>.)</p>
<p>A tree is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as <em>nodes</em>. Regression and decision trees operate by predicting an outcome variable <span class="math inline">\(Y\)</span> by partitioning the predictors.</p>
</div>
<div id="section-regression-trees" class="section level3">
<h3>Regression trees</h3>
<p>When the outcome is continuous, we call the method a <em>regression</em> tree. To introduce regression trees, we will use the 2008 poll data used in previous sections to describe the basic idea of how we build these algorithms. As with other machine learning algorithms, we will try to estimate the conditional expectation <span class="math inline">\(f(x) = \mbox{E}(Y | X = x)\)</span> with <span class="math inline">\(Y\)</span> the poll margin and <span class="math inline">\(x\)</span> the day.</p>
<pre class="r"><code>data(&quot;polls_2008&quot;)
qplot(day, margin, data = polls_2008)</code></pre>
<p><img src="datsci_08_files/figure-html/polls-2008-again-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>The general idea here is to build a decision tree and, at the end of each <em>node</em>, obtain a predictor <span class="math inline">\(\hat{y}\)</span>. A mathematical way to describe this is to say that we are partitioning the predictor space into <span class="math inline">\(J\)</span> non-overlapping regions, <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span>, and then for any predictor <span class="math inline">\(x\)</span> that falls within region <span class="math inline">\(R_j\)</span>, estimate <span class="math inline">\(f(x)\)</span> with the average of the training observations <span class="math inline">\(y_i\)</span> for which the associated predictor <span class="math inline">\(x_i\)</span> is also in <span class="math inline">\(R_j\)</span>.</p>
<p>But how do we decide on the partition <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span> and how do we choose <span class="math inline">\(J\)</span>? Here is where the algorithm gets a bit complicated.</p>
<p>Regression trees create partitions recursively. We start the algorithm with one partition, the entire predictor space. In our simple first example, this space is the interval [-155, 1]. But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on. We describe how we pick the partition to further partition, and when to stop, later.</p>
<p>Once we select a partition <span class="math inline">\(\mathbf{x}\)</span> to split in order to create the new partitions, we find a predictor <span class="math inline">\(j\)</span> and value <span class="math inline">\(s\)</span> that define two new partitions, which we will call <span class="math inline">\(R_1(j,s)\)</span> and <span class="math inline">\(R_2(j,s)\)</span>, that split our observations in the current partition by asking if <span class="math inline">\(x_j\)</span> is bigger than <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[
R_1(j,s) = \{\mathbf{x} \mid x_j &lt; s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
\]</span></p>
<p>In our current example we only have one predictor, so we will always choose <span class="math inline">\(j=1\)</span>, but in general this will not be the case. Now, after we define the new partitions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>, and we decide to stop the partitioning, we compute predictors by taking the average of all the observations <span class="math inline">\(y\)</span> for which the associated <span class="math inline">\(\mathbf{x}\)</span> is in <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>. We refer to these two as <span class="math inline">\(\hat{y}_{R_1}\)</span> and <span class="math inline">\(\hat{y}_{R_2}\)</span> respectively.</p>
<p>But how do we pick <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>? Basically we find the pair that minimizes the residual sum of square (RSS): <span class="math display">\[
\sum_{i:\, x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2
\]</span></p>
<p>This is then applied recursively to the new regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>. We describe how we stop later, but once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region.</p>
<p>Let’s take a look at what this algorithm does on the 2008 presidential election poll data. We will use the <code>rpart</code> function in the <strong>rpart</strong> package.</p>
<pre class="r"><code>library(rpart)
fit &lt;- rpart(margin ~ ., data = polls_2008)</code></pre>
<p>Here, there is only one predictor. Thus we do not have to decide which predictor <span class="math inline">\(j\)</span> to split by, we simply have to decide what value <span class="math inline">\(s\)</span> we use to split. We can visually see where the splits were made:</p>
<pre class="r"><code>plot(fit, margin = 0.1)
text(fit, cex = 0.75)</code></pre>
<p><img src="datsci_08_files/figure-html/polls-2008-tree-1.png" width="60%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>The first split is made on day 39.5. One of those regions is then split at day 86.5. The two resulting new partitions are split on days 49.5 and 117.5, respectively, and so on. We end up with 8 partitions. The final estimate <span class="math inline">\(\hat{f}(x)\)</span> looks like this:</p>
<pre class="r"><code>polls_2008 %&gt;% 
  mutate(y_hat = predict(fit)) %&gt;% 
  ggplot() +
  geom_point(aes(day, margin)) +
  geom_step(aes(day, y_hat), col=&quot;red&quot;)</code></pre>
<p><img src="datsci_08_files/figure-html/polls-2008-tree-fit-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Note that the algorithm stopped partitioning at 8. Now we explain how this decision is made.</p>
<p>First we need to define the term <em>complexity parameter</em> (cp). Every time we split and define two new partitions, our training set RSS decreases. This is because with more partitions, our model has more flexibility to adapt to the training data. In fact, if you split until every point is its own partition, then RSS goes all the way down to 0 since the average of one value is that same value. To avoid this, the algorithm sets a minimum for how much the RSS must improve for another partition to be added. This parameter is referred to as the <em>complexity parameter</em> (cp). The RSS must improve by a factor of cp for the new partition to be added. Large values of cp will therefore force the algorithm to stop earlier which results in fewer nodes.</p>
<p>However, cp is not the only parameter used to decide if we should partition a current partition or not. Another common parameter is the minimum number of observations required in a partition before partitioning it further. The argument used in the <code>rpart</code> function is <code>minsplit</code> and the default is 20. The <code>rpart</code> implementation of regression trees also permits users to determine a minimum number of observations in each node. The argument is <code>minbucket</code> and defaults to <code>round(minsplit/3)</code>.</p>
<p>As expected, if we set <code>cp = 0</code> and <code>minsplit = 2</code>, then our prediction is as flexible as possible and our predictor is our original data:</p>
<pre class="r"><code>fit &lt;- rpart(margin ~ ., data = polls_2008, 
             control = rpart.control(cp = 0, minsplit = 2))
polls_2008 %&gt;% 
  mutate(y_hat = predict(fit)) %&gt;% 
  ggplot() +
  geom_point(aes(day, margin)) +
  geom_step(aes(day, y_hat), col=&quot;red&quot;)</code></pre>
<p><img src="datsci_08_files/figure-html/polls-2008-tree-over-fit-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Intuitively we know that this is not a good approach as it will generally result in over-training. These <code>cp</code>, <code>minsplit</code>, and <code>minbucket</code>, three parameters can be used to control the variability of the final predictors. The larger these values are the more data is averaged to compute a predictor and thus reduce variability. The drawback is that it restricts flexibility.</p>
<p>So how do we pick these parameters? We can use cross validation, described in the textbook <a href="https://rafalab.github.io/dsbook/cross-validation.html">(Section - 29 Cross validation)</a>, just like with any tuning parameter. Here is an example of using cross validation to chose cp.</p>
<pre class="r"><code>library(caret)
train_rpart &lt;- train(margin ~ ., 
                     method = &quot;rpart&quot;,
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)),
                     data = polls_2008)
ggplot(train_rpart)</code></pre>
<p><img src="datsci_08_files/figure-html/polls-2008-tree-train-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>To see the resulting tree, we access the <code>finalModel</code> and plot it:</p>
<pre class="r"><code>plot(train_rpart$finalModel, margin = 0.1)
text(train_rpart$finalModel, cex = 0.75)</code></pre>
<p><img src="datsci_08_files/figure-html/polls-2008-final-model-1.png" width="80%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>And because we only have one predictor, we can actually plot <span class="math inline">\(\hat{f}(x)\)</span>:</p>
<pre class="r"><code>polls_2008 %&gt;% 
  mutate(y_hat = predict(train_rpart)) %&gt;% 
  ggplot() +
  geom_point(aes(day, margin)) +
  geom_step(aes(day, y_hat), col=&quot;red&quot;)</code></pre>
<p><img src="datsci_08_files/figure-html/polls-2008-final-fit-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Note that if we already have a tree and want to apply a higher cp value, we can use the <code>prune</code> function. We call this <em>pruning</em> a tree because we are snipping off partitions that do not meet a <code>cp</code> criterion. We previously created a tree that used a <code>cp = 0</code> and saved it to <code>fit</code>. We can prune it like this:</p>
<pre class="r"><code>pruned_fit &lt;- prune(fit, cp = 0.01)</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>A tree is basically a <strong>flow chart of yes or no questions</strong>. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as nodes.</p></li>
<li><p>When the outcome is continuous, we call the decision tree method a <strong>regression tree</strong>.</p></li>
<li><p>Regression and decision trees operate by predicting an outcome variable <span class="math inline">\(Y\)</span> by <strong>partitioning the predictors</strong>.</p></li>
<li><p>The general idea here is to <strong>build a decision tree</strong> and, at end of each node, obtain a predictor <span class="math inline">\(\hat{y}\)</span>. Mathematically, we are <strong>partitioning the predictor space</strong> into <span class="math inline">\(J\)</span> non-overlapping regions, <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span> and then for any predictor <span class="math inline">\(x\)</span> that falls within region <span class="math inline">\(R_j\)</span>, estimate <span class="math inline">\(f(x)\)</span> with the average of the training observations <span class="math inline">\(y_i\)</span> for which the associated predictor <span class="math inline">\(x_j\)</span> in also in <span class="math inline">\(R_j\)</span>.</p></li>
<li><p>To pick <span class="math inline">\(j\)</span> and its value <span class="math inline">\(j\)</span>, we find the pair that <strong>minimizes the residual sum of squares (RSS)</strong>:</p></li>
</ul>
<p><span class="math display">\[
\sum_{i:\, x_i \ R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \ R_2(j,s)} (y_i - \hat{y}_{R_2})^2
\]</span> - To fit the regression tree model, we can use the <code>rpart()</code> function in the rpart package.</p>
<ul>
<li><p>Two common parameters used for partition decision are the <strong>complexity parameter</strong> (cp) and the <strong>minimum number of observations required in a partition</strong> before partitioning it further (<code>minsplit</code> in the <strong>rpart</strong> package).</p></li>
<li><p>If we already have a tree and want to apply a higher cp value, we can use the <code>prune()</code> function. We call this pruning a tree because we are snipping off partitions that do not meet a cp criterion.</p></li>
</ul>
</div>
</div>
<div id="section-classification-decision-trees" class="section level3">
<h3>Classification (Decision) Trees</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#classification-decision-trees">textbook section - 31.10.4 Classification (decision) trees</a>.</p>
</div>
<p>Classification trees, or decision trees, are used in prediction problems where the outcome is categorical. We use the same partitioning principle with some differences to account for the fact that we are now working with a categorical outcome.</p>
<p>The first difference is that we form predictions by calculating which class is the most common among the training set observations within the partition, rather than taking the average in each partition (as we can’t take the average of categories).</p>
<p>The second is that we can no longer use RSS to choose the partition. While we could use the naive approach of looking for partitions that minimize training error, better performing approaches use more sophisticated metrics. Two of the more popular ones are the <em>Gini Index</em> and <em>Entropy</em>.</p>
<p>In a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The <em>Gini Index</em> is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define <span class="math inline">\(\hat{p}_{j,k}\)</span> as the proportion of observations in partition <span class="math inline">\(j\)</span> that are of class <span class="math inline">\(k\)</span>. The Gini Index is defined as</p>
<p><span class="math display">\[
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
\]</span></p>
<p>If you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above.</p>
<p><em>Entropy</em> is a very similar quantity, defined as</p>
<p><span class="math display">\[
\mbox{entropy}(j) = -\sum_{k=1}^K \hat{p}_{j,k}\log(\hat{p}_{j,k}), \mbox{ with } 0 \times \log(0) \mbox{ defined as }0
\]</span></p>
<p>Let us look at how a classification tree performs on the digits example we examined before:</p>
<p>We can use this code to run the algorithm and plot the resulting tree:</p>
<pre class="r"><code>train_rpart &lt;- train(y ~ .,
                     method = &quot;rpart&quot;,
                     tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 25)),
                     data = mnist_27$train)
plot(train_rpart)</code></pre>
<p><img src="datsci_08_files/figure-html/mnist-27-tree-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>The accuracy achieved by this approach is better than what we got with regression, but is not as good as what we achieved with kernel methods:</p>
<pre class="r"><code>y_hat &lt;- predict(train_rpart, mnist_27$test)
confusionMatrix(y_hat, mnist_27$test$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##     0.82</code></pre>
<p>The plot of the estimated conditional probability shows us the limitations of classification trees:</p>
<p><img src="datsci_08_files/figure-html/rf-cond-prob-1.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Note that with decision trees, it is difficult to make the boundaries smooth since each partition creates a discontinuity.</p>
<p>Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough). Finally, they can model human decision processes and don’t require use of dummy predictors for categorical variables. On the other hand, the approach via recursive partitioning can easily over-train and is therefore a bit harder to train than, for example, linear regression or kNN. Furthermore, in terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data. Random forests, explained next, improve on several of these shortcomings.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p><strong>Classification trees</strong>, or decision trees, are used in prediction problems where the <strong>outcome is categorical</strong>.</p></li>
<li><p>Decision trees form predictions by calculating <strong>which class is the most common</strong> among the training set observations within the partition, rather than taking the average in each partition.</p></li>
<li><p>Two of the more popular metrics to choose the partitions are the <strong>Gini index</strong> and <strong>entropy</strong>.</p></li>
</ul>
<p><span class="math display">\[
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
\]</span></p>
<p><span class="math display">\[
\mbox{entropy}(j) = -\sum_{k=1}^K \hat{p}_{j,k}\log(\hat{p}_{j,k}), \mbox{ with } 0 \times \log(0) \mbox{ defined as }0
\]</span></p>
<ul>
<li><p>Pros: Classification trees are highly interpretable and easy to visualize.They can model human decision processes and don’t require use of dummy predictors for categorical variables.</p></li>
<li><p>Cons: The approach via recursive partitioning can easily over-train and is therefore a bit harder to train than. Furthermore, in terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data.</p></li>
</ul>
</div>
</div>
<div id="section-random-forests" class="section level3">
<h3>Random Forests</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/examples-of-algorithms.html#random-forests">textbook section - 31.11 Random forests</a>.</p>
</div>
<p>Random forests are a <strong>very popular</strong> machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by <em>averaging</em> multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.</p>
<p>The first step is <em>bootstrap aggregation</em> or <em>bagging</em>. The general idea is to generate many predictors, each using regression or classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees <strong>randomly</strong> different, and the combination of trees is the <strong>forest</strong>. The specific steps are as follows.</p>
<p>1. Build <span class="math inline">\(B\)</span> decision trees using the training set. We refer to the fitted models as <span class="math inline">\(T_1, T_2, \dots, T_B\)</span>. We later explain how we ensure they are different.</p>
<p>2. For every observation in the test set, form a prediction <span class="math inline">\(\hat{y}_j\)</span> using tree <span class="math inline">\(T_j\)</span>.</p>
<p>3. For continuous outcomes, form a final prediction with the average <span class="math inline">\(\hat{y} = \frac{1}{B} \sum_{j=1}^B \hat{y}_j\)</span>. For categorical data classification, predict <span class="math inline">\(\hat{y}\)</span> with majority vote (most frequent class among <span class="math inline">\(\hat{y}_1, \dots, \hat{y}_T\)</span>).</p>
<p>So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let <span class="math inline">\(N\)</span> be the number of observations in the training set. To create <span class="math inline">\(T_j, \, j=1,\ldots,B\)</span> from the training set we do the following:</p>
<p>1. Create a bootstrap training set by sampling <span class="math inline">\(N\)</span> observations from the training set <strong>with replacement</strong>. This is the first way to induce randomness.</p>
<p>2. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy.</p>
<p>To illustrate how the first steps can result in smoother estimates we will demonstrate by fitting a random forest to the 2008 polls data. We will use the <code>randomForest</code> function in the <strong>randomForest</strong> package:</p>
<pre class="r"><code>library(randomForest)
fit &lt;- randomForest(margin~., data = polls_2008) </code></pre>
<p>Note that if we apply the function <code>plot</code> to the resulting object, stored in <code>fit</code>, we see how the error rate of our algorithm changes as we add trees.</p>
<pre class="r"><code>rafalib::mypar()
plot(fit)</code></pre>
<p><img src="datsci_08_files/figure-html/more-trees-better-fit-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>We can see that in this case, the accuracy improves as we add more trees until about 30 trees where accuracy stabilizes.</p>
<p>The resulting estimate for this random forest can be seen like this:</p>
<pre class="r"><code>polls_2008 %&gt;%
  mutate(y_hat = predict(fit, newdata = polls_2008)) %&gt;% 
  ggplot() +
  geom_point(aes(day, margin)) +
  geom_line(aes(day, y_hat), col=&quot;red&quot;)</code></pre>
<p><img src="datsci_08_files/figure-html/polls-2008-rf-fit-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Notice that the random forest estimate is much smoother than what we achieved with the regression tree in the previous section. This is possible because the average of many step functions can be smooth. We can see this by visually examining how the estimate changes as we add more trees. In the following figure you see each of the bootstrap samples for several values of <span class="math inline">\(b\)</span> and for each one we see the tree that is fitted in grey, the previous trees that were fitted in lighter grey, and the result of averaging all the trees estimated up to that point.</p>
<p><img src="images/rf.gif" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Here is the random forest fit for our digits example based on two predictors:</p>
<pre class="r"><code>library(randomForest)
train_rf &lt;- randomForest(y ~ ., data=mnist_27$train)

confusionMatrix(predict(train_rf, mnist_27$test),
                mnist_27$test$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##     0.79</code></pre>
<p>Here is what the conditional probabilities look like:</p>
<p><img src="datsci_08_files/figure-html/cond-prob-rf-1.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Visualizing the estimate shows that, although we obtain high accuracy, it appears that there is room for improvement by making the estimate smoother. This could be achieved by changing the parameter that controls the minimum number of data points in the nodes of the tree. The larger this minimum, the smoother the final estimate will be. We can train the parameters of the random forest. Below, we use the <strong>caret</strong> package to optimize over the minimum node size. Because, this is not one of the parameters that the <strong>caret</strong> package optimizes by default we will write our own code:</p>
<pre class="r"><code>nodesize &lt;- seq(1, 51, 10)
acc &lt;- sapply(nodesize, function(ns){
  train(y ~ ., method = &quot;rf&quot;, data = mnist_27$train,
               tuneGrid = data.frame(mtry = 2),
               nodesize = ns)$results$Accuracy
})
qplot(nodesize, acc)</code></pre>
<p><img src="datsci_08_files/figure-html/acc-versus-nodesize-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>We can now fit the random forest with the optimized minimun node size to the entire training data and evaluate performance on the test data.</p>
<pre class="r"><code>train_rf_2 &lt;- randomForest(y ~ ., data=mnist_27$train,
                           nodesize = nodesize[which.max(acc)])

confusionMatrix(predict(train_rf_2, mnist_27$test),
                mnist_27$test$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##    0.815</code></pre>
<p>The selected model improves accuracy and provides a smoother estimate.</p>
<p><img src="datsci_08_files/figure-html/cond-prob-final-rf-1.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Note that we can avoid writing our own code by using other random forest implementations as described in the <a href="http://topepo.github.io/caret/available-models.html"><strong>caret</strong> manual</a>.</p>
<p>Random forest performs better in all the examples we have considered. However, a disadvantage of random forests is that we lose interpretability. An approach that helps with interpretability is to examine <em>variable importance</em>. To define <em>variable importance</em> we count how often a predictor is used in the individual trees. You can learn more about <em>variable importance</em> in an <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">advanced machine learning book</a>. The <strong>caret</strong> package includes the function <code>varImp</code> that extracts variable importance from any model in which the calculation is implemented. We give an example on how we use variable importance in the next section.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p><strong>Random forests</strong> are a very popular machine learning approach that addresses the shortcomings of decision trees. The goal is to improve prediction performance and reduce instability by <strong>averaging multiple decision trees</strong> (a forest of trees constructed with randomness).</p></li>
<li><p>The general idea of random forests is to generate many predictors, each using regression or classification trees, and then <strong>forming a final prediction based on the average prediction of all these tree</strong>s. To assure that the individual trees are not the same, we use the <strong>bootstrap to induce randomness</strong>.</p></li>
<li><p>A <strong>disadvantage</strong> of random forests is that we <strong>lose interpretability</strong>.</p></li>
<li><p>An approach that helps with interpretability is to examine <strong>variable importance</strong>. To define variable importance we <strong>count how often a predictor is used in the individual trees</strong>. The <strong>caret</strong> package includes the function <strong>varImp</strong> that extracts variable importance from any model in which the calculation is implemented.</p></li>
</ul>
</div>
</div>
<div id="section-comprehension-check-trees-and-random-forests" class="section level3">
<h3>5.1 Comprehension Check: Trees and Random Forests</h3>
<p>Insert assessment here</p>
</div>
<div id="section-caret-package" class="section level3">
<h3>Caret Package</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/caret.html">textbook section - Chapter 30 The caret package</a>.</p>
</div>
<p>We have already learned about regression and kNN as machine learning algorithms. In later sections, we learn several others, and this is just a small subset of all the algorithms out there. Many of these algorithms are implemented in R. However, they are distributed via different packages, developed by different authors, and often use different syntax. The <strong>caret</strong> package tries to consolidate these differences and provide consistency. It currently includes 237 different methods which are summarized in the <a href="https://topepo.github.io/caret/available-models.html"><strong>caret</strong> package manual</a>. Keep in mind that <strong>caret</strong> does not include the needed packages and, to implement a package through <strong>caret</strong>, you still need to install the library. The required packages for each method are described in the package manual.</p>
<p>The <strong>caret</strong> package also provides a function that performs cross validation for us. Here we provide some examples showing how we use this incredibly helpful package. We will use the 2 or 7 example to illustrate:</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)
data(&quot;mnist_27&quot;)</code></pre>
</div>
<div id="section-the-caret-train-functon" class="section level3">
<h3>The caret <code>train</code> functon</h3>
<p>The <strong>caret</strong> <code>train</code> function lets us train different algorithms using similar syntax. So, for example, we can type:</p>
<pre class="r"><code>library(caret)
train_glm &lt;- train(y ~ ., method = &quot;glm&quot;, data = mnist_27$train)
train_knn &lt;- train(y ~ ., method = &quot;knn&quot;, data = mnist_27$train)</code></pre>
<p>To make predictions, we can use the output of this function directly without needing to look at the specifics of <code>predict.glm</code> and <code>predict.knn</code>. Instead, we can learn how to obtain predictions from <code>predict.train</code>.</p>
<p>The code looks the same for both methods:</p>
<pre class="r"><code>y_hat_glm &lt;- predict(train_glm, mnist_27$test, type = &quot;raw&quot;)
y_hat_knn &lt;- predict(train_knn, mnist_27$test, type = &quot;raw&quot;)</code></pre>
<p>This permits us to quickly compare the algorithms. For example, we can compare the accuracy like this:</p>
<pre class="r"><code>confusionMatrix(y_hat_glm, mnist_27$test$y)$overall[[&quot;Accuracy&quot;]]</code></pre>
<pre><code>## [1] 0.75</code></pre>
<pre class="r"><code>confusionMatrix(y_hat_knn, mnist_27$test$y)$overall[[&quot;Accuracy&quot;]]</code></pre>
<pre><code>## [1] 0.84</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>The caret package helps provides a uniform interface and standardized syntax for the many different machine learning packages in R. Note that caret does not automatically install the packages needed.</li>
</ul>
<p><strong>Caret package links:</strong></p>
<p><a href="http://topepo.github.io/caret/available-models.htm">External link</a></p>
<p><a href="http://topepo.github.io/caret/train-models-by-tag.html">External link</a></p>
</div>
</div>
<div id="section-tuning-parameters-with-caret" class="section level3">
<h3>Tuning Parameters with Caret</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/caret.html#caret-cv">textbook section - 30.2 Cross validation</a>.</p>
</div>
<p>When an algorithm includes a tuning parameter, <code>train</code> automatically uses cross validation to decide among a few default values. To find out what parameter or parameters are optimized, you can <a href="http://topepo.github.io/caret/available-models.html">read the manual</a> or study the output of:</p>
<pre class="r"><code>getModelInfo(&quot;knn&quot;)</code></pre>
<p>We can also use a quick lookup like this:</p>
<pre class="r"><code>modelLookup(&quot;knn&quot;)</code></pre>
<p>If we run it with default values:</p>
<pre class="r"><code>train_knn &lt;- train(y ~ ., method = &quot;knn&quot;, data = mnist_27$train)</code></pre>
<p>you can quickly see the results of the cross validation using the <code>ggplot</code> function. The argument <code>highlight</code> highlights the max:</p>
<pre class="r"><code>ggplot(train_knn, highlight = TRUE)</code></pre>
<p><img src="datsci_08_files/figure-html/caret-highlight-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>By default, the cross validation is performed by taking 25 bootstrap samples comprised of 25% of the observations. For the <code>kNN</code> method, the default is to try <span class="math inline">\(k=5,7,9\)</span>. We change this using the <code>tuneGrid</code> parameter. The grid of values must be supplied by a data frame with the parameter names as specified in the <code>modelLookup</code> output.</p>
<p>Here, we present an example where we try out 30 values between 9 and 67. To do this with <strong>caret</strong>, we need to define a column named <code>k</code>, so we use this: <code>data.frame(k = seq(9, 67, 2))</code>.</p>
<p>Note that when running this code, we are fitting 30 versions of kNN to 25 bootstrapped samples. Since we are fitting <span class="math inline">\(30 \times 25 = 750\)</span> kNN models, running this code will take several seconds. We set the seed because cross validation is a random procedure and we want to make sure the result here is reproducible.</p>
<pre class="r"><code>set.seed(2008)
train_knn &lt;- train(y ~ ., method = &quot;knn&quot;, 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(9, 71, 2)))
ggplot(train_knn, highlight = TRUE)</code></pre>
<p><img src="datsci_08_files/figure-html/train-knn-plot-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>To access the parameter that maximized the accuracy, you can use this:</p>
<pre class="r"><code>train_knn$bestTune</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["k"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"27","_rn_":"10"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>and the best performing model like this:</p>
<pre class="r"><code>train_knn$finalModel</code></pre>
<pre><code>## 27-nearest neighbor model
## Training set outcome distribution:
## 
##   2   7 
## 379 421</code></pre>
<p>The function <code>predict</code> will use this best performing model. Here is the accuracy of the best model when applied to the test set, which we have not used at all yet because the cross validation was done on the training set:</p>
<pre class="r"><code>confusionMatrix(predict(train_knn, mnist_27$test, type = &quot;raw&quot;),
                mnist_27$test$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##    0.835</code></pre>
<p>If we want to change how we perform cross validation, we can use the <code>trainControl</code> function. We can make the code above go a bit faster by using, for example, 10-fold cross validation. This means we have 10 samples using 10% of the observations each. We accomplish this using the following code:</p>
<pre class="r"><code>control &lt;- trainControl(method = &quot;cv&quot;, number = 10, p = .9)
train_knn_cv &lt;- train(y ~ ., method = &quot;knn&quot;, 
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(9, 71, 2)),
                   trControl = control)
ggplot(train_knn_cv, highlight = TRUE)</code></pre>
<p><img src="datsci_08_files/figure-html/cv-10-fold-accuracy-estimate-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>We notice that the accuracy estimates are more variable, which is expected since we changed the number of samples used to estimate accuracy.</p>
<p>Note that <code>results</code> component of the <code>train</code> output includes several summary statistics related to the variability of the cross validation estimates:</p>
<pre class="r"><code>names(train_knn$results)</code></pre>
<pre><code>## [1] &quot;k&quot;          &quot;Accuracy&quot;   &quot;Kappa&quot;      &quot;AccuracySD&quot; &quot;KappaSD&quot;</code></pre>
<!--We can also see the standard deviation bars obtained from the cross validation samples:


```r
train_knn$results %>% 
  ggplot(aes(x = k, y = Accuracy)) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(x = k, 
                    ymin = Accuracy - AccuracySD, 
                    ymax = Accuracy + AccuracySD))
```

<img src="datsci_08_files/figure-html/accuracy-with-sd-bars-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" />
-->
</div>
<div id="section-example-fitting-with-loess" class="section level3">
<h3>Example: fitting with loess</h3>
<p>The best fitting kNN model approximates the true conditional probability: <img src="datsci_08_files/figure-html/mnist27-optimal-knn-fit-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>However, we do see that the boundary is somewhat wiggly. This is because kNN, like the basic bin smoother, does not use a kernel. To improve this we could try loess. By reading through the available models part of the <a href="https://topepo.github.io/caret/available-models.html">manual</a> we see that we can use the <code>gamLoess</code> method. In the <a href="https://topepo.github.io/caret/train-models-by-tag.html">manual</a> we also see that we need to install the <strong>gam</strong> package if we have not done so already:</p>
<pre class="r"><code>install.packages(&quot;gam&quot;)</code></pre>
<p>Then we see that we have two parameters to optimize:</p>
<pre class="r"><code>modelLookup(&quot;gamLoess&quot;)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["model"],"name":[1],"type":["chr"],"align":["left"]},{"label":["parameter"],"name":[2],"type":["chr"],"align":["left"]},{"label":["label"],"name":[3],"type":["chr"],"align":["left"]},{"label":["forReg"],"name":[4],"type":["lgl"],"align":["right"]},{"label":["forClass"],"name":[5],"type":["lgl"],"align":["right"]},{"label":["probModel"],"name":[6],"type":["lgl"],"align":["right"]}],"data":[{"1":"gamLoess","2":"span","3":"Span","4":"TRUE","5":"TRUE","6":"TRUE","_rn_":"1"},{"1":"gamLoess","2":"degree","3":"Degree","4":"TRUE","5":"TRUE","6":"TRUE","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>We will stick to a degree of 1. But to try out different values for the span, we still have to include a column in the table with the name <code>degree</code> so we can do this:</p>
<pre class="r"><code>grid &lt;- expand.grid(span = seq(0.15, 0.65, len = 10), degree = 1)</code></pre>
<p>We will use the default cross validation control parameters.</p>
<pre class="r"><code>train_loess &lt;- train(y ~ ., 
                   method = &quot;gamLoess&quot;, 
                   tuneGrid=grid,
                   data = mnist_27$train)
ggplot(train_loess, highlight = TRUE)</code></pre>
<p><img src="datsci_08_files/figure-html/loess-accuracy-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>We can see that the method performs similar to kNN:</p>
<pre class="r"><code>confusionMatrix(data = predict(train_loess, mnist_27$test), 
                reference = mnist_27$test$y)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##     0.85</code></pre>
<p>and produces a smoother estimate of the conditional probability:</p>
<p><img src="datsci_08_files/figure-html/gam-smooth-1.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The <code>train()</code> function automatically uses cross-validation to decide among a few default values of a tuning parameter.</p></li>
<li><p>The <code>getModelInfo()</code> and <code>modelLookup()</code> functions can be used to learn more about a model and the parameters that can be optimized.</p></li>
<li><p>We can use the <code>tunegrid()</code> parameter in the <code>train()</code> function to select a grid of values to be compared.</p></li>
<li><p>The trControl parameter and <code>trainControl()</code> function can be used to change the way cross-validation is performed.</p></li>
</ul>
<p>-Note that <strong>not all parameters in machine learning algorithms are tuned</strong>. We use the <code>train()</code> function to only optimize parameters that are tunable.</p>
<p><strong>Caret package links:</strong></p>
<p><a href="https://topepo.github.io/caret/available-models.html">External link</a></p>
<p><a href="http://topepo.github.io/caret/train-models-by-tag.html">External link</a></p>
</div>
</div>
<div id="section-comprehension-check-caret-package" class="section level3">
<h3>5.2 Comprehension Check: Caret Package</h3>
<p>Insert assessment here</p>
</div>
<div id="section-assessment-titanic-exercises-part-1" class="section level3">
<h3>5.3 Assessment: Titanic Exercises Part 1</h3>
<p>Insert assessment here</p>
</div>
<div id="section-assessment-titanic-exercises-part-2" class="section level3">
<h3>5.3 Assessment: Titanic Exercises Part 2</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-6-model-fitting-and-recommendation-systems" class="section level2">
<h2>Section 6: Model Fitting and Recommendation Systems</h2>
<p>In the <strong>Model Fitting and Recommendation Systems</strong> section, you will learn how to apply the machine learning algorithms you have learned.</p>
<p>After completing this section, you will be able to:</p>
<ul>
<li><p>Apply the methods we have learned to an example, the <strong>MNIST digits</strong>.</p></li>
<li><p>Build a <strong>movie recommendation system</strong> using machine learning.</p></li>
<li><p>Penalize large estimates that come from small sample sizes using <strong>regularization</strong>.</p></li>
</ul>
<p>This section has three parts: <strong>case study: MNIST</strong>, <strong>recommendation systems</strong>, and <strong>regularization</strong>. There are comprehension checks throughout.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your own learning.</p>
<div id="section-case-study-mnist" class="section level3">
<h3>Case Study: MNIST</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/machine-learning-in-practice.html">textbook section - Chapter 32 Machine learning in practice</a>.</p>
</div>
<p>Now that we have learned several methods and explored them with illustrative examples, we are going to try them out on a real example: the MNIST digits.</p>
<p>We can load this data using the following <strong>dslabs</strong> package:</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)
mnist &lt;- read_mnist()</code></pre>
<p>The dataset includes two components, a training set and test set:</p>
<pre class="r"><code>names(mnist)</code></pre>
<pre><code>## [1] &quot;train&quot; &quot;test&quot;</code></pre>
<p>Each of these components includes a matrix with features in the columns:</p>
<pre class="r"><code>dim(mnist$train$images)</code></pre>
<pre><code>## [1] 60000   784</code></pre>
<p>and vector with the classes as integers:</p>
<pre class="r"><code>class(mnist$train$labels)</code></pre>
<pre><code>## [1] &quot;integer&quot;</code></pre>
<pre class="r"><code>table(mnist$train$labels)</code></pre>
<pre><code>## 
##    0    1    2    3    4    5    6    7    8    9 
## 5923 6742 5958 6131 5842 5421 5918 6265 5851 5949</code></pre>
<p>Because we want this example to run on a small laptop and in less than one hour, we will consider a subset of the dataset. We will sample 10,000 random rows from the training set and 1,000 random rows from the test set:</p>
<pre class="r"><code>set.seed(1990)
index &lt;- sample(nrow(mnist$train$images), 10000)
x &lt;- mnist$train$images[index,]
y &lt;- factor(mnist$train$labels[index])

index &lt;- sample(nrow(mnist$test$images), 1000)
x_test &lt;- mnist$test$images[index,]
y_test &lt;- factor(mnist$test$labels[index])</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>We will apply what we have learned in the course on the Modified National Institute of Standards and Technology database (MNIST) digits, a popular dataset used in machine learning competitions.</li>
</ul>
</div>
</div>
<div id="section-preprocessing-mnist-data" class="section level3">
<h3>Preprocessing MNIST Data</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/machine-learning-in-practice.html#preprocessing">textbook section - 32.1 Preprocessing</a>.</p>
</div>
<p>In machine learning, we often transform predictors before running the machine algorithm. We also remove predictors that are clearly not useful. We call these steps <em>preprocessing</em>.</p>
<p>Examples of preprocessing include standardizing the predictors, taking the log transform of some predictors, removing predictors that are highly correlated with others, and removing predictors with very few non-unique values or close to zero variation. We show an example below.</p>
<p>We can run the <code>nearZero</code> function from the <strong>caret</strong> package to see that several features do not vary much from observation to observation. We can see that there is a large number of features with 0 variability:</p>
<pre class="r"><code>library(matrixStats)
sds &lt;- colSds(x)
qplot(sds, bins = 256)</code></pre>
<p><img src="datsci_08_files/figure-html/pixel-sds-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>This is expected because there are parts of the image that rarely contain writing (dark pixels).</p>
<p>The <strong>caret</strong> packages includes a function that recommends features to be removed due to <em>near zero variance</em>:</p>
<pre class="r"><code>library(caret)
nzv &lt;- nearZeroVar(x)</code></pre>
<p>We can see the columns recommended for removal:</p>
<pre class="r"><code>image(matrix(1:784 %in% nzv, 28, 28))</code></pre>
<pre class="r"><code>rafalib::mypar()
image(matrix(1:784 %in% nzv, 28, 28))</code></pre>
<p><img src="datsci_08_files/figure-html/near-zero-image-1.png" width="50%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>So we end up keeping this number of columns:</p>
<pre class="r"><code>col_index &lt;- setdiff(1:ncol(x), nzv)
length(col_index)</code></pre>
<pre><code>## [1] 252</code></pre>
<p>Now we are ready to fit some models. Before we start, we need to add column names to the feature matrices as these are required by <strong>caret</strong>:</p>
<pre class="r"><code>colnames(x) &lt;- 1:ncol(mnist$train$images)
colnames(x_test) &lt;- colnames(x)</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Common preprocessing steps include:</p>
<ul>
<li><p>standardizing or transforming predictors and</p></li>
<li><p>removing predictors that are not useful, are highly correlated with others, have very few non-unique values, or have close to zero variation.</p></li>
</ul></li>
</ul>
</div>
</div>
<div id="section-model-fitting-for-mnist-data" class="section level3">
<h3>Model Fitting for MNIST Data</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/machine-learning-in-practice.html#k-nearest-neighbor-and-random-forest">textbook section - 32.2 k-nearest neighbor and random forest</a>.</p>
</div>
<p>Let’s start with kNN. The first step is to optimize for <span class="math inline">\(k\)</span>. Keep in mind that when we run the algorithm, we will have to compute a distance between each observation in the test set and each observation in the training set. There are a lot of computations. We will therefore use k-fold cross validation to improve speed.</p>
<p>If we run the following code, the computing time on a standard laptop will be several minutes.</p>
<pre class="r"><code>control &lt;- trainControl(method = &quot;cv&quot;, number = 10, p = .9)
train_knn &lt;- train(x[ ,col_index], y, 
                   method = &quot;knn&quot;, 
                   tuneGrid = data.frame(k = c(3,5,7)),
                   trControl = control)
train_knn</code></pre>
<p>In general, it is a good idea to try a test run with a subset of the data to get an idea of timing before we start running code that might take hours to complete. We can do this as follows:</p>
<pre class="r"><code>n &lt;- 1000
b &lt;- 2
index &lt;- sample(nrow(x), n)
control &lt;- trainControl(method = &quot;cv&quot;, number = b, p = .9)
train_knn &lt;- train(x[index, col_index], y[index], 
                   method = &quot;knn&quot;, 
                   tuneGrid = data.frame(k = c(3,5,7)),
                   trControl = control)</code></pre>
<p>We can then increase <code>n</code> and <code>b</code> and try to establish a pattern of how they affect computing time to get an idea of how long the fitting process will take for larger values of <code>n</code> and <code>b</code>. You want to know if a function is going to take hours, or even days, before you run it.</p>
<p>Once we optimize our algorithm, we can fit it to the entire dataset:</p>
<pre class="r"><code>fit_knn &lt;- knn3(x[, col_index], y,  k = 3)</code></pre>
<p>The accuracy is almost 0.95!</p>
<pre class="r"><code>y_hat_knn &lt;- predict(fit_knn, x_test[, col_index], type=&quot;class&quot;)
cm &lt;- confusionMatrix(y_hat_knn, factor(y_test))
cm$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##    0.953</code></pre>
<p>We now achieve an accuracy of about 0.95. From the specificity and sensitivity, we also see that 8s are the hardest to detect and the most commonly incorrectly predicted digit is 7.</p>
<pre class="r"><code>cm$byClass[,1:2]</code></pre>
<pre><code>##          Sensitivity Specificity
## Class: 0   0.9895833   0.9955752
## Class: 1   1.0000000   0.9931896
## Class: 2   0.9651163   0.9967177
## Class: 3   0.9500000   0.9988889
## Class: 4   0.9300000   0.9966667
## Class: 5   0.9207921   0.9933259
## Class: 6   0.9767442   0.9956236
## Class: 7   0.9557522   0.9887260
## Class: 8   0.8865979   0.9988926
## Class: 9   0.9509804   0.9899777</code></pre>
<p>Now let’s see if we can do even better with the random forest algorithm.</p>
<p>With random forest, computation time is a challenge. For each forest, we need to build hundreds of trees. We also have several parameters we can tune.</p>
<p>Because with random forest the fitting is the slowest part of the procedure rather than the predicting (as with kNN), we will use only five-fold cross validation. We will also reduce the number of trees that are fit since we are not yet building our final model.</p>
<p>Finally, to compute on a smaller dataset, we will take a random sample of the observations when constructing each tree. We can change this number with the <code>nSamp</code> argument.</p>
<pre class="r"><code>library(randomForest)
control &lt;- trainControl(method=&quot;cv&quot;, number = 5)
grid &lt;- data.frame(mtry = c(1, 5, 10, 25, 50, 100))

train_rf &lt;-  train(x[, col_index], y, 
                   method = &quot;rf&quot;, 
                   ntree = 150,
                   trControl = control,
                   tuneGrid = grid,
                   nSamp = 5000)

ggplot(train_rf)</code></pre>
<p><img src="datsci_08_files/figure-html/mnist-rf-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<pre class="r"><code>train_rf$bestTune</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["mtry"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"10","_rn_":"3"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Now that we have optimized our algorithm, we are ready to fit our final model:</p>
<pre class="r"><code>fit_rf &lt;- randomForest(x[, col_index], y, 
                       minNode = train_rf$bestTune$mtry)</code></pre>
<p>To check that we ran enough trees we can use the plot function:</p>
<pre class="r"><code>plot(fit_rf)</code></pre>
<p>We see that we achieve high accuracy:</p>
<pre class="r"><code>y_hat_rf &lt;- predict(fit_rf, x_test[ ,col_index])
cm &lt;- confusionMatrix(y_hat_rf, y_test)
cm$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##    0.952</code></pre>
<!--
Here are some examples of the original images and our calls:
<img src="datsci_08_files/figure-html/mnist-examples-of-calls-1.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" />
-->
<p>With some further tuning, we can get even higher accuracy.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The caret package requires that we add column names to the feature matrices.</p></li>
<li><p>In general, it is a good idea to test out a small subset of the data first to get an idea of how long your code will take to run.</p></li>
</ul>
</div>
</div>
<div id="section-variable-importance" class="section level3">
<h3>Variable Importance</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/machine-learning-in-practice.html#variable-importance">textbook section - 32.3 Variable importance</a> and <a href="https://rafalab.github.io/dsbook/machine-learning-in-practice.html#visual-assessments">textbook section - 32.4 Visual assessments</a>.</p>
</div>
<p>The following function computes the importance of each feature:</p>
<pre class="r"><code>imp &lt;- importance(fit_rf)</code></pre>
<p>We can see which features are being used most by plotting an image:</p>
<pre class="r"><code>mat &lt;- rep(0, ncol(x))
mat[col_index] &lt;- imp
image(matrix(mat, 28, 28))</code></pre>
<pre class="r"><code>rafalib::mypar()
mat &lt;- rep(0, ncol(x))
mat[col_index] &lt;- imp
image(matrix(mat, 28, 28))</code></pre>
<p><img src="datsci_08_files/figure-html/importance-image-1.png" width="50%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
</div>
<div id="section-visual-assessments" class="section level3">
<h3>Visual assessments</h3>
<p>An important part of data analysis is visualizing results to determine why we are failing. How we do this depends on the application. Below we show the images of digits for which we made an incorrect prediction. We can compare what we get with kNN to random forest.</p>
<!--Here are some errors for kNN:

<img src="datsci_08_files/figure-html/knn-images-1.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" />
 
And -->
<p>Here are some errors for the random forest:</p>
<p><img src="datsci_08_files/figure-html/rf-images,-1.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>By examining errors like this we often find specific weaknesses to algorithms or parameter choices and can try to correct them.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The <strong>Rborist</strong> package does not currently support variable importance calculations, but the <strong>randomForest</strong> package does.</p></li>
<li><p>An important part of data science is visualizing results to determine why we are failing.</p></li>
</ul>
</div>
</div>
<div id="section-ensembles" class="section level3">
<h3>Ensembles</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/machine-learning-in-practice.html#ensembles">textbook section - 32.5 Ensembles</a>.</p>
</div>
<p>The idea of an ensemble is similar to the idea of combining data from different pollsters to obtain a better estimate of the true support for each candidate.</p>
<p>In machine learning, one can usually greatly improve the final results by combining the results of different algorithms.</p>
<p>Here is a simple example where we compute new class probabilities by taking the average of random forest and kNN. We can see that the accuracy improves to 0.96:</p>
<pre class="r"><code>p_rf &lt;- predict(fit_rf, x_test[,col_index], type = &quot;prob&quot;)  
p_rf&lt;- p_rf / rowSums(p_rf)
p_knn  &lt;- predict(fit_knn, x_test[,col_index])
p &lt;- (p_rf + p_knn)/2
y_pred &lt;- factor(apply(p, 1, which.max)-1)
confusionMatrix(y_pred, y_test)$overall[&quot;Accuracy&quot;]</code></pre>
<pre><code>## Accuracy 
##    0.962</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><strong>Ensembles</strong> combine multiple machine learning algorithms into one model to improve predictions.</li>
</ul>
</div>
</div>
<div id="section-comprehension-check-ensembles" class="section level3">
<h3>6.1 Comprehension Check: Ensembles</h3>
<p>Insert assessment here</p>
</div>
<div id="section-recommendation-systems" class="section level3">
<h3>Recommendation Systems</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#recommendation-systems">textbook section - 33.7 Recommendation systems</a>.</p>
</div>
<p>Recommendation systems use ratings that <em>users</em> have given <em>items</em> to make specific recommendations. Companies that sell many products to many customers and permit these customers to rate their products, like Amazon, are able to collect massive datasets that can be used to predict what rating a particular user will give a specific item. Items for which a high rating is predicted for a given user are then recommended to that user.</p>
<p>Netflix uses a recommendation system to predict how many <em>stars</em> a user will give a specific movie. One star suggests it is not a good movie, whereas five stars suggests it is an excellent movie. Here, we provide the basics of how these recommendations are made, motivated by some of the approaches taken by the winners of the <em>Netflix challenges</em>.</p>
<p>In October 2006, Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% and win a million dollars. In September 2009, <a href="http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/">the winners were announced</a>. You can read a good summary of how the winning algorithm was put together here: <a href="http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/">http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/</a> and a more detailed explanation here: <a href="http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf">http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf</a>. We will now show you some of the data analysis strategies used by the winning team.</p>
</div>
<div id="section-movielens-data" class="section level3">
<h3>Movielens data</h3>
<p>The Netflix data is not publicly available, but the <a href="https://grouplens.org/">GroupLens research lab</a> generated their own database with over 20 million ratings for over 27,000 movies by more than 138,000 users. We make a small subset of this data available via the <strong>dslabs</strong> package:</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)
data(&quot;movielens&quot;)</code></pre>
<p>We can see this table is in tidy format with thousands of rows:</p>
<pre class="r"><code>movielens %&gt;% as_tibble()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["movieId"],"name":[1],"type":["int"],"align":["right"]},{"label":["title"],"name":[2],"type":["chr"],"align":["left"]},{"label":["year"],"name":[3],"type":["int"],"align":["right"]},{"label":["genres"],"name":[4],"type":["fctr"],"align":["left"]},{"label":["userId"],"name":[5],"type":["int"],"align":["right"]},{"label":["rating"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["timestamp"],"name":[7],"type":["int"],"align":["right"]}],"data":[{"1":"31","2":"Dangerous Minds","3":"1995","4":"Drama","5":"1","6":"2.5","7":"1260759144"},{"1":"1029","2":"Dumbo","3":"1941","4":"Animation|Children|Drama|Musical","5":"1","6":"3.0","7":"1260759179"},{"1":"1061","2":"Sleepers","3":"1996","4":"Thriller","5":"1","6":"3.0","7":"1260759182"},{"1":"1129","2":"Escape from New York","3":"1981","4":"Action|Adventure|Sci-Fi|Thriller","5":"1","6":"2.0","7":"1260759185"},{"1":"1172","2":"Cinema Paradiso (Nuovo cinema Paradiso)","3":"1989","4":"Drama","5":"1","6":"4.0","7":"1260759205"},{"1":"1263","2":"Deer Hunter, The","3":"1978","4":"Drama|War","5":"1","6":"2.0","7":"1260759151"},{"1":"1287","2":"Ben-Hur","3":"1959","4":"Action|Adventure|Drama","5":"1","6":"2.0","7":"1260759187"},{"1":"1293","2":"Gandhi","3":"1982","4":"Drama","5":"1","6":"2.0","7":"1260759148"},{"1":"1339","2":"Dracula (Bram Stoker's Dracula)","3":"1992","4":"Fantasy|Horror|Romance|Thriller","5":"1","6":"3.5","7":"1260759125"},{"1":"1343","2":"Cape Fear","3":"1991","4":"Thriller","5":"1","6":"2.0","7":"1260759131"},{"1":"1371","2":"Star Trek: The Motion Picture","3":"1979","4":"Adventure|Sci-Fi","5":"1","6":"2.5","7":"1260759135"},{"1":"1405","2":"Beavis and Butt-Head Do America","3":"1996","4":"Adventure|Animation|Comedy|Crime","5":"1","6":"1.0","7":"1260759203"},{"1":"1953","2":"French Connection, The","3":"1971","4":"Action|Crime|Thriller","5":"1","6":"4.0","7":"1260759191"},{"1":"2105","2":"Tron","3":"1982","4":"Action|Adventure|Sci-Fi","5":"1","6":"4.0","7":"1260759139"},{"1":"2150","2":"Gods Must Be Crazy, The","3":"1980","4":"Adventure|Comedy","5":"1","6":"3.0","7":"1260759194"},{"1":"2193","2":"Willow","3":"1988","4":"Action|Adventure|Fantasy","5":"1","6":"2.0","7":"1260759198"},{"1":"2294","2":"Antz","3":"1998","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"1","6":"2.0","7":"1260759108"},{"1":"2455","2":"Fly, The","3":"1986","4":"Drama|Horror|Sci-Fi|Thriller","5":"1","6":"2.5","7":"1260759113"},{"1":"2968","2":"Time Bandits","3":"1981","4":"Adventure|Comedy|Fantasy|Sci-Fi","5":"1","6":"1.0","7":"1260759200"},{"1":"3671","2":"Blazing Saddles","3":"1974","4":"Comedy|Western","5":"1","6":"3.0","7":"1260759117"},{"1":"10","2":"GoldenEye","3":"1995","4":"Action|Adventure|Thriller","5":"2","6":"4.0","7":"835355493"},{"1":"17","2":"Sense and Sensibility","3":"1995","4":"Drama|Romance","5":"2","6":"5.0","7":"835355681"},{"1":"39","2":"Clueless","3":"1995","4":"Comedy|Romance","5":"2","6":"5.0","7":"835355604"},{"1":"47","2":"Seven (a.k.a. Se7en)","3":"1995","4":"Mystery|Thriller","5":"2","6":"4.0","7":"835355552"},{"1":"50","2":"Usual Suspects, The","3":"1995","4":"Crime|Mystery|Thriller","5":"2","6":"4.0","7":"835355586"},{"1":"52","2":"Mighty Aphrodite","3":"1995","4":"Comedy|Drama|Romance","5":"2","6":"3.0","7":"835356031"},{"1":"62","2":"Mr. Holland's Opus","3":"1995","4":"Drama","5":"2","6":"3.0","7":"835355749"},{"1":"110","2":"Braveheart","3":"1995","4":"Action|Drama|War","5":"2","6":"4.0","7":"835355532"},{"1":"144","2":"Brothers McMullen, The","3":"1995","4":"Comedy","5":"2","6":"3.0","7":"835356016"},{"1":"150","2":"Apollo 13","3":"1995","4":"Adventure|Drama|IMAX","5":"2","6":"5.0","7":"835355395"},{"1":"153","2":"Batman Forever","3":"1995","4":"Action|Adventure|Comedy|Crime","5":"2","6":"4.0","7":"835355441"},{"1":"161","2":"Crimson Tide","3":"1995","4":"Drama|Thriller|War","5":"2","6":"3.0","7":"835355493"},{"1":"165","2":"Die Hard: With a Vengeance","3":"1995","4":"Action|Crime|Thriller","5":"2","6":"3.0","7":"835355441"},{"1":"168","2":"First Knight","3":"1995","4":"Action|Drama|Romance","5":"2","6":"3.0","7":"835355710"},{"1":"185","2":"Net, The","3":"1995","4":"Action|Crime|Thriller","5":"2","6":"3.0","7":"835355511"},{"1":"186","2":"Nine Months","3":"1995","4":"Comedy|Romance","5":"2","6":"3.0","7":"835355664"},{"1":"208","2":"Waterworld","3":"1995","4":"Action|Adventure|Sci-Fi","5":"2","6":"3.0","7":"835355511"},{"1":"222","2":"Circle of Friends","3":"1995","4":"Drama|Romance","5":"2","6":"5.0","7":"835355840"},{"1":"223","2":"Clerks","3":"1994","4":"Comedy","5":"2","6":"1.0","7":"835355749"},{"1":"225","2":"Disclosure","3":"1994","4":"Drama|Thriller","5":"2","6":"3.0","7":"835355552"},{"1":"235","2":"Ed Wood","3":"1994","4":"Comedy|Drama","5":"2","6":"3.0","7":"835355664"},{"1":"248","2":"Houseguest","3":"1994","4":"Comedy","5":"2","6":"3.0","7":"835355896"},{"1":"253","2":"Interview with the Vampire: The Vampire Chronicles","3":"1994","4":"Drama|Horror","5":"2","6":"4.0","7":"835355511"},{"1":"261","2":"Little Women","3":"1994","4":"Drama","5":"2","6":"4.0","7":"835355681"},{"1":"265","2":"Like Water for Chocolate (Como agua para chocolate)","3":"1992","4":"Drama|Fantasy|Romance","5":"2","6":"5.0","7":"835355697"},{"1":"266","2":"Legends of the Fall","3":"1994","4":"Drama|Romance|War|Western","5":"2","6":"5.0","7":"835355586"},{"1":"272","2":"Madness of King George, The","3":"1994","4":"Comedy|Drama","5":"2","6":"3.0","7":"835355767"},{"1":"273","2":"Mary Shelley's Frankenstein (Frankenstein)","3":"1994","4":"Drama|Horror|Sci-Fi","5":"2","6":"4.0","7":"835355779"},{"1":"292","2":"Outbreak","3":"1995","4":"Action|Drama|Sci-Fi|Thriller","5":"2","6":"3.0","7":"835355492"},{"1":"296","2":"Pulp Fiction","3":"1994","4":"Comedy|Crime|Drama|Thriller","5":"2","6":"4.0","7":"835355395"},{"1":"300","2":"Quiz Show","3":"1994","4":"Drama","5":"2","6":"3.0","7":"835355532"},{"1":"314","2":"Secret of Roan Inish, The","3":"1994","4":"Children|Drama|Fantasy|Mystery","5":"2","6":"4.0","7":"835356044"},{"1":"317","2":"Santa Clause, The","3":"1994","4":"Comedy|Drama|Fantasy","5":"2","6":"2.0","7":"835355551"},{"1":"319","2":"Shallow Grave","3":"1994","4":"Comedy|Drama|Thriller","5":"2","6":"1.0","7":"835355918"},{"1":"339","2":"While You Were Sleeping","3":"1995","4":"Comedy|Romance","5":"2","6":"3.0","7":"835355492"},{"1":"349","2":"Clear and Present Danger","3":"1994","4":"Action|Crime|Drama|Thriller","5":"2","6":"4.0","7":"835355441"},{"1":"350","2":"Client, The","3":"1994","4":"Drama|Mystery|Thriller","5":"2","6":"4.0","7":"835355697"},{"1":"356","2":"Forrest Gump","3":"1994","4":"Comedy|Drama|Romance|War","5":"2","6":"3.0","7":"835355628"},{"1":"357","2":"Four Weddings and a Funeral","3":"1994","4":"Comedy|Romance","5":"2","6":"3.0","7":"835355749"},{"1":"364","2":"Lion King, The","3":"1994","4":"Adventure|Animation|Children|Drama|Musical|IMAX","5":"2","6":"3.0","7":"835355604"},{"1":"367","2":"Mask, The","3":"1994","4":"Action|Comedy|Crime|Fantasy","5":"2","6":"3.0","7":"835355619"},{"1":"370","2":"Naked Gun 33 1/3: The Final Insult","3":"1994","4":"Action|Comedy","5":"2","6":"2.0","7":"835355932"},{"1":"371","2":"Paper, The","3":"1994","4":"Comedy|Drama","5":"2","6":"3.0","7":"835355968"},{"1":"372","2":"Reality Bites","3":"1994","4":"Comedy|Drama|Romance","5":"2","6":"3.0","7":"835356094"},{"1":"377","2":"Speed","3":"1994","4":"Action|Romance|Thriller","5":"2","6":"3.0","7":"835355710"},{"1":"382","2":"Wolf","3":"1994","4":"Drama|Horror|Romance|Thriller","5":"2","6":"3.0","7":"835356165"},{"1":"405","2":"Highlander III: The Sorcerer (a.k.a. Highlander: The Final Dimension)","3":"1994","4":"Action|Fantasy","5":"2","6":"2.0","7":"835356246"},{"1":"410","2":"Addams Family Values","3":"1993","4":"Children|Comedy|Fantasy","5":"2","6":"3.0","7":"835355532"},{"1":"454","2":"Firm, The","3":"1993","4":"Drama|Thriller","5":"2","6":"4.0","7":"835355604"},{"1":"457","2":"Fugitive, The","3":"1993","4":"Thriller","5":"2","6":"3.0","7":"835355511"},{"1":"468","2":"Englishman Who Went Up a Hill But Came Down a Mountain, The","3":"1995","4":"Comedy|Romance","5":"2","6":"4.0","7":"835355790"},{"1":"474","2":"In the Line of Fire","3":"1993","4":"Action|Thriller","5":"2","6":"2.0","7":"835355828"},{"1":"480","2":"Jurassic Park","3":"1993","4":"Action|Adventure|Sci-Fi|Thriller","5":"2","6":"4.0","7":"835355643"},{"1":"485","2":"Last Action Hero","3":"1993","4":"Action|Adventure|Comedy|Fantasy","5":"2","6":"3.0","7":"835355918"},{"1":"497","2":"Much Ado About Nothing","3":"1993","4":"Comedy|Romance","5":"2","6":"3.0","7":"835355880"},{"1":"500","2":"Mrs. Doubtfire","3":"1993","4":"Comedy|Drama","5":"2","6":"4.0","7":"835355731"},{"1":"508","2":"Philadelphia","3":"1993","4":"Drama","5":"2","6":"4.0","7":"835355860"},{"1":"509","2":"Piano, The","3":"1993","4":"Drama|Romance","5":"2","6":"4.0","7":"835355719"},{"1":"515","2":"Remains of the Day, The","3":"1993","4":"Drama|Romance","5":"2","6":"4.0","7":"835355817"},{"1":"527","2":"Schindler's List","3":"1993","4":"Drama|War","5":"2","6":"4.0","7":"835355731"},{"1":"537","2":"Sirens","3":"1994","4":"Drama","5":"2","6":"4.0","7":"835356199"},{"1":"539","2":"Sleepless in Seattle","3":"1993","4":"Comedy|Drama|Romance","5":"2","6":"3.0","7":"835355767"},{"1":"550","2":"Threesome","3":"1994","4":"Comedy|Romance","5":"2","6":"3.0","7":"835356109"},{"1":"551","2":"Nightmare Before Christmas, The","3":"1993","4":"Animation|Children|Fantasy|Musical","5":"2","6":"5.0","7":"835355767"},{"1":"552","2":"Three Musketeers, The","3":"1993","4":"Action|Adventure|Comedy|Romance","5":"2","6":"3.0","7":"835355860"},{"1":"585","2":"Brady Bunch Movie, The","3":"1995","4":"Comedy","5":"2","6":"5.0","7":"835355817"},{"1":"586","2":"Home Alone","3":"1990","4":"Children|Comedy","5":"2","6":"3.0","7":"835355790"},{"1":"587","2":"Ghost","3":"1990","4":"Comedy|Drama|Fantasy|Romance|Thriller","5":"2","6":"3.0","7":"835355779"},{"1":"588","2":"Aladdin","3":"1992","4":"Adventure|Animation|Children|Comedy|Musical","5":"2","6":"3.0","7":"835355441"},{"1":"589","2":"Terminator 2: Judgment Day","3":"1991","4":"Action|Sci-Fi","5":"2","6":"5.0","7":"835355697"},{"1":"590","2":"Dances with Wolves","3":"1990","4":"Adventure|Drama|Western","5":"2","6":"5.0","7":"835355395"},{"1":"592","2":"Batman","3":"1989","4":"Action|Crime|Thriller","5":"2","6":"5.0","7":"835355395"},{"1":"593","2":"Silence of the Lambs, The","3":"1991","4":"Crime|Horror|Thriller","5":"2","6":"3.0","7":"835355511"},{"1":"616","2":"Aristocats, The","3":"1970","4":"Animation|Children","5":"2","6":"3.0","7":"835355932"},{"1":"661","2":"James and the Giant Peach","3":"1996","4":"Adventure|Animation|Children|Fantasy|Musical","5":"2","6":"4.0","7":"835356141"},{"1":"720","2":"Wallace & Gromit: The Best of Aardman Animation","3":"1996","4":"Adventure|Animation|Comedy","5":"2","6":"4.0","7":"835355978"},{"1":"60","2":"Indian in the Cupboard, The","3":"1995","4":"Adventure|Children|Fantasy","5":"3","6":"3.0","7":"1298861675"},{"1":"110","2":"Braveheart","3":"1995","4":"Action|Drama|War","5":"3","6":"4.0","7":"1298922049"},{"1":"247","2":"Heavenly Creatures","3":"1994","4":"Crime|Drama","5":"3","6":"3.5","7":"1298861637"},{"1":"267","2":"Major Payne","3":"1995","4":"Comedy","5":"3","6":"3.0","7":"1298861761"},{"1":"296","2":"Pulp Fiction","3":"1994","4":"Comedy|Crime|Drama|Thriller","5":"3","6":"4.5","7":"1298862418"},{"1":"318","2":"Shawshank Redemption, The","3":"1994","4":"Crime|Drama","5":"3","6":"5.0","7":"1298862121"},{"1":"355","2":"Flintstones, The","3":"1994","4":"Children|Comedy|Fantasy","5":"3","6":"2.5","7":"1298861589"},{"1":"356","2":"Forrest Gump","3":"1994","4":"Comedy|Drama|Romance|War","5":"3","6":"5.0","7":"1298862167"},{"1":"377","2":"Speed","3":"1994","4":"Action|Romance|Thriller","5":"3","6":"2.5","7":"1298923242"},{"1":"527","2":"Schindler's List","3":"1993","4":"Drama|War","5":"3","6":"3.0","7":"1298862528"},{"1":"588","2":"Aladdin","3":"1992","4":"Adventure|Animation|Children|Comedy|Musical","5":"3","6":"3.0","7":"1298922100"},{"1":"592","2":"Batman","3":"1989","4":"Action|Crime|Thriller","5":"3","6":"3.0","7":"1298923247"},{"1":"593","2":"Silence of the Lambs, The","3":"1991","4":"Crime|Horror|Thriller","5":"3","6":"3.0","7":"1298921840"},{"1":"595","2":"Beauty and the Beast","3":"1991","4":"Animation|Children|Fantasy|Musical|Romance|IMAX","5":"3","6":"2.0","7":"1298923260"},{"1":"736","2":"Twister","3":"1996","4":"Action|Adventure|Romance|Thriller","5":"3","6":"3.5","7":"1298932787"},{"1":"778","2":"Trainspotting","3":"1996","4":"Comedy|Crime|Drama","5":"3","6":"4.0","7":"1298863157"},{"1":"866","2":"Bound","3":"1996","4":"Crime|Drama|Romance|Thriller","5":"3","6":"3.0","7":"1298861687"},{"1":"1197","2":"Princess Bride, The","3":"1987","4":"Action|Adventure|Comedy|Fantasy|Romance","5":"3","6":"5.0","7":"1298932770"},{"1":"1210","2":"Star Wars: Episode VI - Return of the Jedi","3":"1983","4":"Action|Adventure|Sci-Fi","5":"3","6":"3.0","7":"1298921795"},{"1":"1235","2":"Harold and Maude","3":"1971","4":"Comedy|Drama|Romance","5":"3","6":"4.0","7":"1298861628"},{"1":"1271","2":"Fried Green Tomatoes","3":"1991","4":"Comedy|Crime|Drama","5":"3","6":"3.0","7":"1298861605"},{"1":"1378","2":"Young Guns","3":"1988","4":"Action|Comedy|Western","5":"3","6":"4.0","7":"1298861658"},{"1":"1580","2":"Men in Black (a.k.a. MIB)","3":"1997","4":"Action|Comedy|Sci-Fi","5":"3","6":"3.5","7":"1298922089"},{"1":"1721","2":"Titanic","3":"1997","4":"Drama|Romance","5":"3","6":"4.5","7":"1298923236"},{"1":"1884","2":"Fear and Loathing in Las Vegas","3":"1998","4":"Adventure|Comedy|Drama","5":"3","6":"4.0","7":"1298863143"},{"1":"2028","2":"Saving Private Ryan","3":"1998","4":"Action|Drama|War","5":"3","6":"4.0","7":"1298921862"},{"1":"2318","2":"Happiness","3":"1998","4":"Comedy|Drama","5":"3","6":"4.0","7":"1298861753"},{"1":"2513","2":"Pet Sematary","3":"1989","4":"Horror","5":"3","6":"3.0","7":"1298861789"},{"1":"2694","2":"Big Daddy","3":"1999","4":"Comedy","5":"3","6":"3.0","7":"1298862710"},{"1":"2702","2":"Summer of Sam","3":"1999","4":"Drama","5":"3","6":"3.5","7":"1298861796"},{"1":"2716","2":"Ghostbusters (a.k.a. Ghost Busters)","3":"1984","4":"Action|Comedy|Sci-Fi","5":"3","6":"3.0","7":"1298924017"},{"1":"2762","2":"Sixth Sense, The","3":"1999","4":"Drama|Horror|Mystery","5":"3","6":"3.5","7":"1298922057"},{"1":"2841","2":"Stir of Echoes","3":"1999","4":"Horror|Mystery|Thriller","5":"3","6":"4.0","7":"1298861733"},{"1":"2858","2":"American Beauty","3":"1999","4":"Drama|Romance","5":"3","6":"4.0","7":"1298921825"},{"1":"2959","2":"Fight Club","3":"1999","4":"Action|Crime|Drama|Thriller","5":"3","6":"5.0","7":"1298862874"},{"1":"3243","2":"Encino Man","3":"1992","4":"Comedy","5":"3","6":"3.0","7":"1298861968"},{"1":"3510","2":"Frequency","3":"2000","4":"Drama|Thriller","5":"3","6":"4.0","7":"1298861633"},{"1":"3949","2":"Requiem for a Dream","3":"2000","4":"Drama","5":"3","6":"5.0","7":"1298863174"},{"1":"5349","2":"Spider-Man","3":"2002","4":"Action|Adventure|Sci-Fi|Thriller","5":"3","6":"3.0","7":"1298923266"},{"1":"5669","2":"Bowling for Columbine","3":"2002","4":"Documentary","5":"3","6":"3.5","7":"1298862672"},{"1":"6377","2":"Finding Nemo","3":"2003","4":"Adventure|Animation|Children|Comedy","5":"3","6":"3.0","7":"1298922080"},{"1":"7153","2":"Lord of the Rings: The Return of the King, The","3":"2003","4":"Action|Adventure|Drama|Fantasy","5":"3","6":"2.5","7":"1298921787"},{"1":"7361","2":"Eternal Sunshine of the Spotless Mind","3":"2004","4":"Drama|Romance|Sci-Fi","5":"3","6":"3.0","7":"1298922065"},{"1":"8622","2":"Fahrenheit 9/11","3":"2004","4":"Documentary","5":"3","6":"3.5","7":"1298861650"},{"1":"8636","2":"Spider-Man 2","3":"2004","4":"Action|Adventure|Sci-Fi|IMAX","5":"3","6":"3.0","7":"1298932766"},{"1":"27369","2":"Daria: Is It Fall Yet?","3":"2000","4":"Animation|Comedy","5":"3","6":"3.5","7":"1298862555"},{"1":"44191","2":"V for Vendetta","3":"2006","4":"Action|Sci-Fi|Thriller|IMAX","5":"3","6":"3.5","7":"1298932740"},{"1":"48783","2":"Flags of Our Fathers","3":"2006","4":"Drama|War","5":"3","6":"4.5","7":"1298862361"},{"1":"50068","2":"Letters from Iwo Jima","3":"2006","4":"Drama|War","5":"3","6":"4.5","7":"1298862467"},{"1":"58559","2":"Dark Knight, The","3":"2008","4":"Action|Crime|Drama|IMAX","5":"3","6":"3.0","7":"1298922071"},{"1":"84236","2":"White Stripes Under Great White Northern Lights, The","3":"2009","4":"Documentary","5":"3","6":"4.0","7":"1298922130"},{"1":"10","2":"GoldenEye","3":"1995","4":"Action|Adventure|Thriller","5":"4","6":"4.0","7":"949810645"},{"1":"34","2":"Babe","3":"1995","4":"Children|Drama","5":"4","6":"5.0","7":"949919556"},{"1":"112","2":"Rumble in the Bronx (Hont faan kui)","3":"1995","4":"Action|Adventure|Comedy|Crime","5":"4","6":"5.0","7":"949810582"},{"1":"141","2":"Birdcage, The","3":"1996","4":"Comedy","5":"4","6":"5.0","7":"949919681"},{"1":"153","2":"Batman Forever","3":"1995","4":"Action|Adventure|Comedy|Crime","5":"4","6":"4.0","7":"949811346"},{"1":"173","2":"Judge Dredd","3":"1995","4":"Action|Crime|Sci-Fi","5":"4","6":"3.0","7":"949811346"},{"1":"185","2":"Net, The","3":"1995","4":"Action|Crime|Thriller","5":"4","6":"3.0","7":"949920047"},{"1":"260","2":"Star Wars: Episode IV - A New Hope","3":"1977","4":"Action|Adventure|Sci-Fi","5":"4","6":"5.0","7":"949779042"},{"1":"289","2":"Only You","3":"1994","4":"Comedy|Romance","5":"4","6":"4.0","7":"949778802"},{"1":"296","2":"Pulp Fiction","3":"1994","4":"Comedy|Crime|Drama|Thriller","5":"4","6":"5.0","7":"949895708"},{"1":"329","2":"Star Trek: Generations","3":"1994","4":"Adventure|Drama|Sci-Fi","5":"4","6":"3.0","7":"949810618"},{"1":"349","2":"Clear and Present Danger","3":"1994","4":"Action|Crime|Drama|Thriller","5":"4","6":"5.0","7":"949810582"},{"1":"356","2":"Forrest Gump","3":"1994","4":"Comedy|Drama|Romance|War","5":"4","6":"5.0","7":"949919763"},{"1":"357","2":"Four Weddings and a Funeral","3":"1994","4":"Comedy|Romance","5":"4","6":"5.0","7":"949919681"},{"1":"364","2":"Lion King, The","3":"1994","4":"Adventure|Animation|Children|Drama|Musical|IMAX","5":"4","6":"5.0","7":"949949538"},{"1":"367","2":"Mask, The","3":"1994","4":"Action|Comedy|Crime|Fantasy","5":"4","6":"4.0","7":"949895887"},{"1":"380","2":"True Lies","3":"1994","4":"Action|Adventure|Comedy|Romance|Thriller","5":"4","6":"3.0","7":"949810534"},{"1":"410","2":"Addams Family Values","3":"1993","4":"Children|Comedy|Fantasy","5":"4","6":"3.0","7":"949919883"},{"1":"431","2":"Carlito's Way","3":"1993","4":"Crime|Drama","5":"4","6":"3.0","7":"949895772"},{"1":"434","2":"Cliffhanger","3":"1993","4":"Action|Adventure|Thriller","5":"4","6":"4.0","7":"949810688"},{"1":"435","2":"Coneheads","3":"1993","4":"Comedy|Sci-Fi","5":"4","6":"1.0","7":"949920135"},{"1":"440","2":"Dave","3":"1993","4":"Comedy|Romance","5":"4","6":"4.0","7":"949919802"},{"1":"442","2":"Demolition Man","3":"1993","4":"Action|Adventure|Sci-Fi","5":"4","6":"4.0","7":"949920028"},{"1":"464","2":"Hard Target","3":"1993","4":"Action|Adventure|Crime|Thriller","5":"4","6":"4.0","7":"949811315"},{"1":"480","2":"Jurassic Park","3":"1993","4":"Action|Adventure|Sci-Fi|Thriller","5":"4","6":"5.0","7":"949810582"},{"1":"541","2":"Blade Runner","3":"1982","4":"Action|Sci-Fi|Thriller","5":"4","6":"5.0","7":"949779091"},{"1":"588","2":"Aladdin","3":"1992","4":"Adventure|Animation|Children|Comedy|Musical","5":"4","6":"5.0","7":"949949486"},{"1":"589","2":"Terminator 2: Judgment Day","3":"1991","4":"Action|Sci-Fi","5":"4","6":"5.0","7":"949919938"},{"1":"590","2":"Dances with Wolves","3":"1990","4":"Adventure|Drama|Western","5":"4","6":"3.0","7":"949810534"},{"1":"594","2":"Snow White and the Seven Dwarfs","3":"1937","4":"Animation|Children|Drama|Fantasy|Musical","5":"4","6":"5.0","7":"949949538"},{"1":"596","2":"Pinocchio","3":"1940","4":"Animation|Children|Fantasy|Musical","5":"4","6":"5.0","7":"949949638"},{"1":"610","2":"Heavy Metal","3":"1981","4":"Action|Adventure|Animation|Horror|Sci-Fi","5":"4","6":"4.0","7":"949982238"},{"1":"616","2":"Aristocats, The","3":"1970","4":"Animation|Children","5":"4","6":"5.0","7":"949949444"},{"1":"858","2":"Godfather, The","3":"1972","4":"Crime|Drama","5":"4","6":"5.0","7":"949779022"},{"1":"903","2":"Vertigo","3":"1958","4":"Drama|Mystery|Romance|Thriller","5":"4","6":"5.0","7":"949919189"},{"1":"910","2":"Some Like It Hot","3":"1959","4":"Comedy|Crime","5":"4","6":"4.0","7":"949919306"},{"1":"913","2":"Maltese Falcon, The","3":"1941","4":"Film-Noir|Mystery","5":"4","6":"5.0","7":"949919247"},{"1":"919","2":"Wizard of Oz, The","3":"1939","4":"Adventure|Children|Fantasy|Musical","5":"4","6":"5.0","7":"949949396"},{"1":"1011","2":"Herbie Rides Again","3":"1974","4":"Children|Comedy|Fantasy|Romance","5":"4","6":"4.0","7":"949919454"},{"1":"1016","2":"Shaggy Dog, The","3":"1959","4":"Children|Comedy","5":"4","6":"4.0","7":"949919322"},{"1":"1022","2":"Cinderella","3":"1950","4":"Animation|Children|Fantasy|Musical|Romance","5":"4","6":"5.0","7":"949949638"},{"1":"1028","2":"Mary Poppins","3":"1964","4":"Children|Comedy|Fantasy|Musical","5":"4","6":"5.0","7":"949949638"},{"1":"1030","2":"Pete's Dragon","3":"1977","4":"Adventure|Animation|Children|Musical","5":"4","6":"5.0","7":"949896377"},{"1":"1031","2":"Bedknobs and Broomsticks","3":"1971","4":"Adventure|Children|Musical","5":"4","6":"5.0","7":"949896377"},{"1":"1032","2":"Alice in Wonderland","3":"1951","4":"Adventure|Animation|Children|Fantasy|Musical","5":"4","6":"5.0","7":"949949538"},{"1":"1033","2":"Fox and the Hound, The","3":"1981","4":"Animation|Children|Drama","5":"4","6":"5.0","7":"949949638"},{"1":"1036","2":"Die Hard","3":"1988","4":"Action|Crime|Thriller","5":"4","6":"5.0","7":"949896244"},{"1":"1073","2":"Willy Wonka & the Chocolate Factory","3":"1971","4":"Children|Comedy|Fantasy|Musical","5":"4","6":"5.0","7":"949919372"},{"1":"1079","2":"Fish Called Wanda, A","3":"1988","4":"Comedy|Crime","5":"4","6":"5.0","7":"949811523"},{"1":"1089","2":"Reservoir Dogs","3":"1992","4":"Crime|Mystery|Thriller","5":"4","6":"5.0","7":"949895732"},{"1":"1097","2":"E.T. the Extra-Terrestrial","3":"1982","4":"Children|Drama|Sci-Fi","5":"4","6":"5.0","7":"949778771"},{"1":"1125","2":"Return of the Pink Panther, The","3":"1975","4":"Comedy|Crime","5":"4","6":"5.0","7":"949919399"},{"1":"1127","2":"Abyss, The","3":"1989","4":"Action|Adventure|Sci-Fi|Thriller","5":"4","6":"5.0","7":"949896275"},{"1":"1136","2":"Monty Python and the Holy Grail","3":"1975","4":"Adventure|Comedy|Fantasy","5":"4","6":"5.0","7":"949919372"},{"1":"1194","2":"Cheech and Chong's Up in Smoke","3":"1978","4":"Comedy","5":"4","6":"5.0","7":"949919419"},{"1":"1196","2":"Star Wars: Episode V - The Empire Strikes Back","3":"1980","4":"Action|Adventure|Sci-Fi","5":"4","6":"5.0","7":"949779173"},{"1":"1197","2":"Princess Bride, The","3":"1987","4":"Action|Adventure|Comedy|Fantasy|Romance","5":"4","6":"5.0","7":"949811490"},{"1":"1198","2":"Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark)","3":"1981","4":"Action|Adventure","5":"4","6":"5.0","7":"949779173"},{"1":"1200","2":"Aliens","3":"1986","4":"Action|Adventure|Horror|Sci-Fi","5":"4","6":"5.0","7":"949896244"},{"1":"1206","2":"Clockwork Orange, A","3":"1971","4":"Crime|Drama|Sci-Fi|Thriller","5":"4","6":"5.0","7":"949896159"},{"1":"1208","2":"Apocalypse Now","3":"1979","4":"Action|Drama|War","5":"4","6":"5.0","7":"949779173"},{"1":"1210","2":"Star Wars: Episode VI - Return of the Jedi","3":"1983","4":"Action|Adventure|Sci-Fi","5":"4","6":"5.0","7":"949778714"},{"1":"1213","2":"Goodfellas","3":"1990","4":"Crime|Drama","5":"4","6":"5.0","7":"949895708"},{"1":"1214","2":"Alien","3":"1979","4":"Horror|Sci-Fi","5":"4","6":"5.0","7":"949810261"},{"1":"1219","2":"Psycho","3":"1960","4":"Crime|Horror","5":"4","6":"5.0","7":"949779173"},{"1":"1220","2":"Blues Brothers, The","3":"1980","4":"Action|Comedy|Musical","5":"4","6":"5.0","7":"949811523"},{"1":"1222","2":"Full Metal Jacket","3":"1987","4":"Drama|War","5":"4","6":"5.0","7":"949918693"},{"1":"1225","2":"Amadeus","3":"1984","4":"Drama","5":"4","6":"5.0","7":"949779173"},{"1":"1230","2":"Annie Hall","3":"1977","4":"Comedy|Romance","5":"4","6":"5.0","7":"949919372"},{"1":"1240","2":"Terminator, The","3":"1984","4":"Action|Sci-Fi|Thriller","5":"4","6":"5.0","7":"949896244"},{"1":"1243","2":"Rosencrantz and Guildenstern Are Dead","3":"1990","4":"Comedy|Drama","5":"4","6":"5.0","7":"949919519"},{"1":"1257","2":"Better Off Dead...","3":"1985","4":"Comedy|Romance","5":"4","6":"5.0","7":"949811602"},{"1":"1258","2":"Shining, The","3":"1980","4":"Horror","5":"4","6":"5.0","7":"949918743"},{"1":"1259","2":"Stand by Me","3":"1986","4":"Adventure|Drama","5":"4","6":"4.0","7":"949811559"},{"1":"1265","2":"Groundhog Day","3":"1993","4":"Comedy|Fantasy|Romance","5":"4","6":"5.0","7":"949919591"},{"1":"1270","2":"Back to the Future","3":"1985","4":"Adventure|Comedy|Sci-Fi","5":"4","6":"5.0","7":"949811523"},{"1":"1278","2":"Young Frankenstein","3":"1974","4":"Comedy|Fantasy","5":"4","6":"5.0","7":"949919372"},{"1":"1282","2":"Fantasia","3":"1940","4":"Animation|Children|Fantasy|Musical","5":"4","6":"5.0","7":"949949396"},{"1":"1285","2":"Heathers","3":"1989","4":"Comedy","5":"4","6":"4.0","7":"949811559"},{"1":"1288","2":"This Is Spinal Tap","3":"1984","4":"Comedy","5":"4","6":"5.0","7":"949811490"},{"1":"1291","2":"Indiana Jones and the Last Crusade","3":"1989","4":"Action|Adventure","5":"4","6":"5.0","7":"949918743"},{"1":"1298","2":"Pink Floyd: The Wall","3":"1982","4":"Drama|Musical","5":"4","6":"4.0","7":"949918927"},{"1":"1307","2":"When Harry Met Sally...","3":"1989","4":"Comedy|Romance","5":"4","6":"4.0","7":"949811602"},{"1":"1332","2":"Believers, The","3":"1987","4":"Horror|Thriller","5":"4","6":"4.0","7":"949896275"},{"1":"1334","2":"Blob, The","3":"1958","4":"Horror|Sci-Fi","5":"4","6":"5.0","7":"949982274"},{"1":"1344","2":"Cape Fear","3":"1962","4":"Crime|Drama|Thriller","5":"4","6":"5.0","7":"949919247"},{"1":"1356","2":"Star Trek: First Contact","3":"1996","4":"Action|Adventure|Sci-Fi|Thriller","5":"4","6":"4.0","7":"949810582"},{"1":"1371","2":"Star Trek: The Motion Picture","3":"1979","4":"Adventure|Sci-Fi","5":"4","6":"4.0","7":"949810302"},{"1":"1372","2":"Star Trek VI: The Undiscovered Country","3":"1991","4":"Action|Mystery|Sci-Fi","5":"4","6":"3.0","7":"949920004"},{"1":"1374","2":"Star Trek II: The Wrath of Khan","3":"1982","4":"Action|Adventure|Sci-Fi|Thriller","5":"4","6":"4.0","7":"949918787"},{"1":"1376","2":"Star Trek IV: The Voyage Home","3":"1986","4":"Adventure|Comedy|Sci-Fi","5":"4","6":"3.0","7":"949918904"},{"1":"1377","2":"Batman Returns","3":"1992","4":"Action|Crime","5":"4","6":"3.0","7":"949810688"},{"1":"1380","2":"Grease","3":"1978","4":"Comedy|Musical|Romance","5":"4","6":"5.0","7":"949896377"},{"1":"1387","2":"Jaws","3":"1975","4":"Action|Horror","5":"4","6":"5.0","7":"949810261"},{"1":"1388","2":"Jaws 2","3":"1978","4":"Horror|Thriller","5":"4","6":"4.0","7":"949810302"},{"1":"1396","2":"Sneakers","3":"1992","4":"Action|Comedy|Crime|Drama|Sci-Fi","5":"4","6":"5.0","7":"949895772"},{"1":"1544","2":"Lost World: Jurassic Park, The","3":"1997","4":"Action|Adventure|Sci-Fi|Thriller","5":"4","6":"3.0","7":"949811230"},{"1":"1580","2":"Men in Black (a.k.a. MIB)","3":"1997","4":"Action|Comedy|Sci-Fi","5":"4","6":"5.0","7":"949919978"},{"1":"1663","2":"Stripes","3":"1981","4":"Comedy|War","5":"4","6":"4.0","7":"949811559"},{"1":"1674","2":"Witness","3":"1985","4":"Drama|Romance|Thriller","5":"4","6":"5.0","7":"949896244"},{"1":"1805","2":"Wild Things","3":"1998","4":"Crime|Drama|Mystery|Thriller","5":"4","6":"1.0","7":"949895864"},{"1":"1858","2":"Mr. Nice Guy (Yat goh ho yan)","3":"1997","4":"Action|Comedy","5":"4","6":"5.0","7":"949919738"},{"1":"1917","2":"Armageddon","3":"1998","4":"Action|Romance|Sci-Fi|Thriller","5":"4","6":"4.0","7":"949810688"},{"1":"1918","2":"Lethal Weapon 4","3":"1998","4":"Action|Comedy|Crime|Thriller","5":"4","6":"2.0","7":"949895956"},{"1":"1953","2":"French Connection, The","3":"1971","4":"Action|Crime|Thriller","5":"4","6":"5.0","7":"949810261"},{"1":"1954","2":"Rocky","3":"1976","4":"Drama","5":"4","6":"5.0","7":"949810261"},{"1":"1961","2":"Rain Man","3":"1988","4":"Drama","5":"4","6":"5.0","7":"949918743"},{"1":"1967","2":"Labyrinth","3":"1986","4":"Adventure|Fantasy|Musical","5":"4","6":"3.0","7":"949949538"},{"1":"1968","2":"Breakfast Club, The","3":"1985","4":"Comedy|Drama","5":"4","6":"5.0","7":"949811559"},{"1":"1994","2":"Poltergeist","3":"1982","4":"Horror|Thriller","5":"4","6":"5.0","7":"949896309"},{"1":"2000","2":"Lethal Weapon","3":"1987","4":"Action|Comedy|Crime|Drama","5":"4","6":"5.0","7":"949811602"},{"1":"2002","2":"Lethal Weapon 3","3":"1992","4":"Action|Comedy|Crime|Drama","5":"4","6":"3.0","7":"949895907"},{"1":"2003","2":"Gremlins","3":"1984","4":"Comedy|Horror","5":"4","6":"2.0","7":"949918970"},{"1":"2005","2":"Goonies, The","3":"1985","4":"Action|Adventure|Children|Comedy|Fantasy","5":"4","6":"5.0","7":"949896070"},{"1":"2014","2":"Freaky Friday","3":"1977","4":"Children|Comedy|Fantasy","5":"4","6":"4.0","7":"949919454"},{"1":"2018","2":"Bambi","3":"1942","4":"Animation|Children|Drama","5":"4","6":"5.0","7":"949778771"},{"1":"2020","2":"Dangerous Liaisons","3":"1988","4":"Drama|Romance","5":"4","6":"4.0","7":"949811738"},{"1":"2021","2":"Dune","3":"1984","4":"Adventure|Sci-Fi","5":"4","6":"4.0","7":"949896114"},{"1":"2033","2":"Black Cauldron, The","3":"1985","4":"Adventure|Animation|Children|Fantasy","5":"4","6":"4.0","7":"949982239"},{"1":"2034","2":"Black Hole, The","3":"1979","4":"Children|Sci-Fi","5":"4","6":"4.0","7":"949896183"},{"1":"2046","2":"Flight of the Navigator","3":"1986","4":"Adventure|Children|Sci-Fi","5":"4","6":"5.0","7":"949810618"},{"1":"2054","2":"Honey, I Shrunk the Kids","3":"1989","4":"Adventure|Children|Comedy|Fantasy|Sci-Fi","5":"4","6":"3.0","7":"949896114"},{"1":"2064","2":"Roger & Me","3":"1989","4":"Documentary","5":"4","6":"5.0","7":"949918648"},{"1":"2078","2":"Jungle Book, The","3":"1967","4":"Animation|Children|Comedy|Musical","5":"4","6":"5.0","7":"949949444"},{"1":"2080","2":"Lady and the Tramp","3":"1955","4":"Animation|Children|Comedy|Romance","5":"4","6":"5.0","7":"949919306"},{"1":"2081","2":"Little Mermaid, The","3":"1989","4":"Animation|Children|Comedy|Musical|Romance","5":"4","6":"4.0","7":"949982239"},{"1":"2085","2":"101 Dalmatians (One Hundred and One Dalmatians)","3":"1961","4":"Adventure|Animation|Children","5":"4","6":"5.0","7":"949949444"},{"1":"2086","2":"One Magic Christmas","3":"1985","4":"Drama|Fantasy","5":"4","6":"5.0","7":"949896114"},{"1":"2087","2":"Peter Pan","3":"1953","4":"Animation|Children|Fantasy|Musical","5":"4","6":"5.0","7":"949949638"},{"1":"2091","2":"Return from Witch Mountain","3":"1978","4":"Children|Sci-Fi","5":"4","6":"5.0","7":"949896183"},{"1":"2094","2":"Rocketeer, The","3":"1991","4":"Action|Adventure|Sci-Fi","5":"4","6":"4.0","7":"949920028"},{"1":"2096","2":"Sleeping Beauty","3":"1959","4":"Animation|Children|Musical","5":"4","6":"5.0","7":"949949538"},{"1":"2100","2":"Splash","3":"1984","4":"Comedy|Fantasy|Romance","5":"4","6":"5.0","7":"949896114"},{"1":"2102","2":"Steamboat Willie","3":"1928","4":"Animation|Children|Comedy|Musical","5":"4","6":"5.0","7":"949949396"},{"1":"2105","2":"Tron","3":"1982","4":"Action|Adventure|Sci-Fi","5":"4","6":"4.0","7":"949896114"},{"1":"2109","2":"Jerk, The","3":"1979","4":"Comedy","5":"4","6":"5.0","7":"949919399"},{"1":"2110","2":"Dead Men Don't Wear Plaid","3":"1982","4":"Comedy|Crime|Thriller","5":"4","6":"3.0","7":"949896309"},{"1":"2114","2":"Outsiders, The","3":"1983","4":"Drama","5":"4","6":"5.0","7":"949918927"},{"1":"2115","2":"Indiana Jones and the Temple of Doom","3":"1984","4":"Action|Adventure|Fantasy","5":"4","6":"5.0","7":"949918994"},{"1":"2124","2":"Addams Family, The","3":"1991","4":"Children|Comedy|Fantasy","5":"4","6":"4.0","7":"949919763"},{"1":"2140","2":"Dark Crystal, The","3":"1982","4":"Adventure|Fantasy","5":"4","6":"5.0","7":"949896070"},{"1":"2141","2":"American Tail, An","3":"1986","4":"Adventure|Animation|Children|Comedy","5":"4","6":"4.0","7":"949982239"},{"1":"2143","2":"Legend","3":"1985","4":"Adventure|Fantasy|Romance","5":"4","6":"4.0","7":"949896114"},{"1":"2144","2":"Sixteen Candles","3":"1984","4":"Comedy|Romance","5":"4","6":"5.0","7":"949811603"},{"1":"2161","2":"NeverEnding Story, The","3":"1984","4":"Adventure|Children|Fantasy","5":"4","6":"5.0","7":"949896070"},{"1":"2174","2":"Beetlejuice","3":"1988","4":"Comedy|Fantasy","5":"4","6":"5.0","7":"949896070"},{"1":"2193","2":"Willow","3":"1988","4":"Action|Adventure|Fantasy","5":"4","6":"3.0","7":"949896070"},{"1":"2194","2":"Untouchables, The","3":"1987","4":"Action|Crime|Drama","5":"4","6":"5.0","7":"949918764"},{"1":"2248","2":"Say Anything...","3":"1989","4":"Comedy|Drama|Romance","5":"4","6":"4.0","7":"949811559"},{"1":"2263","2":"Seventh Sign, The","3":"1988","4":"Drama|Fantasy|Thriller","5":"4","6":"3.0","7":"949896309"},{"1":"2268","2":"Few Good Men, A","3":"1992","4":"Crime|Drama|Thriller","5":"4","6":"5.0","7":"949895772"},{"1":"2289","2":"Player, The","3":"1992","4":"Comedy|Crime|Drama","5":"4","6":"5.0","7":"949919556"},{"1":"2348","2":"Sid and Nancy","3":"1986","4":"Drama","5":"4","6":"4.0","7":"949918927"},{"1":"2371","2":"Fletch","3":"1985","4":"Comedy|Crime|Mystery","5":"4","6":"4.0","7":"949811603"},{"1":"2403","2":"First Blood (Rambo: First Blood)","3":"1982","4":"Action|Adventure|Drama|Thriller","5":"4","6":"4.0","7":"949918994"},{"1":"2406","2":"Romancing the Stone","3":"1984","4":"Action|Adventure|Comedy|Romance","5":"4","6":"5.0","7":"949918994"},{"1":"2409","2":"Rocky II","3":"1979","4":"Action|Drama","5":"4","6":"4.0","7":"949810302"},{"1":"2454","2":"Fly, The","3":"1958","4":"Horror|Mystery|Sci-Fi","5":"4","6":"5.0","7":"949982274"},{"1":"2467","2":"Name of the Rose, The (Name der Rose, Der)","3":"1986","4":"Crime|Drama|Mystery|Thriller","5":"4","6":"5.0","7":"949918875"},{"1":"2551","2":"Dead Ringers","3":"1988","4":"Drama|Horror|Thriller","5":"4","6":"4.0","7":"949896275"},{"1":"2616","2":"Dick Tracy","3":"1990","4":"Action|Crime","5":"4","6":"1.0","7":"949895993"},{"1":"2628","2":"Star Wars: Episode I - The Phantom Menace","3":"1999","4":"Action|Adventure|Sci-Fi","5":"4","6":"5.0","7":"949810582"},{"1":"2640","2":"Superman","3":"1978","4":"Action|Adventure|Sci-Fi","5":"4","6":"5.0","7":"949810261"},{"1":"2659","2":"It Came from Hollywood","3":"1982","4":"Comedy|Documentary","5":"4","6":"3.0","7":"949918671"},{"1":"2683","2":"Austin Powers: The Spy Who Shagged Me","3":"1999","4":"Action|Adventure|Comedy","5":"4","6":"4.0","7":"949896497"},{"1":"2699","2":"Arachnophobia","3":"1990","4":"Comedy|Horror","5":"4","6":"4.0","7":"949896497"},{"1":"2716","2":"Ghostbusters (a.k.a. Ghost Busters)","3":"1984","4":"Action|Comedy|Sci-Fi","5":"4","6":"5.0","7":"949811523"},{"1":"2723","2":"Mystery Men","3":"1999","4":"Action|Comedy|Fantasy","5":"4","6":"5.0","7":"949810582"},{"1":"2734","2":"Mosquito Coast, The","3":"1986","4":"Adventure|Drama|Thriller","5":"4","6":"4.0","7":"949778714"},{"1":"2770","2":"Bowfinger","3":"1999","4":"Comedy","5":"4","6":"1.0","7":"949896521"},{"1":"2788","2":"Monty Python's And Now for Something Completely Different","3":"1971","4":"Comedy","5":"4","6":"5.0","7":"949919399"},{"1":"2791","2":"Airplane!","3":"1980","4":"Comedy","5":"4","6":"5.0","7":"949811490"},{"1":"2795","2":"National Lampoon's Vacation","3":"1983","4":"Comedy","5":"4","6":"4.0","7":"949811603"},{"1":"2797","2":"Big","3":"1988","4":"Comedy|Drama|Fantasy|Romance","5":"4","6":"5.0","7":"949896070"},{"1":"2804","2":"Christmas Story, A","3":"1983","4":"Children|Comedy","5":"4","6":"5.0","7":"949811738"},{"1":"2822","2":"Medicine Man","3":"1992","4":"Adventure|Romance","5":"4","6":"3.0","7":"949811230"},{"1":"2867","2":"Fright Night","3":"1985","4":"Comedy|Horror|Thriller","5":"4","6":"3.0","7":"949918875"},{"1":"2872","2":"Excalibur","3":"1981","4":"Adventure|Fantasy","5":"4","6":"5.0","7":"949896114"},{"1":"2877","2":"Tommy","3":"1975","4":"Musical","5":"4","6":"5.0","7":"949896377"},{"1":"2902","2":"Psycho II","3":"1983","4":"Horror|Mystery|Thriller","5":"4","6":"2.0","7":"949896309"},{"1":"2903","2":"Psycho III","3":"1986","4":"Horror|Thriller","5":"4","6":"1.0","7":"949896309"},{"1":"2916","2":"Total Recall","3":"1990","4":"Action|Adventure|Sci-Fi|Thriller","5":"4","6":"4.0","7":"949810534"},{"1":"2918","2":"Ferris Bueller's Day Off","3":"1986","4":"Comedy","5":"4","6":"5.0","7":"949811559"},{"1":"2968","2":"Time Bandits","3":"1981","4":"Adventure|Comedy|Fantasy|Sci-Fi","5":"4","6":"5.0","7":"949896070"},{"1":"2986","2":"RoboCop 2","3":"1990","4":"Action|Crime|Sci-Fi|Thriller","5":"4","6":"3.0","7":"949896015"},{"1":"2987","2":"Who Framed Roger Rabbit?","3":"1988","4":"Adventure|Animation|Children|Comedy|Crime|Fantasy|Mystery","5":"4","6":"5.0","7":"949918807"},{"1":"2991","2":"Live and Let Die","3":"1973","4":"Action|Adventure|Thriller","5":"4","6":"5.0","7":"949810261"},{"1":"3016","2":"Creepshow","3":"1982","4":"Horror","5":"4","6":"3.0","7":"949896543"},{"1":"3034","2":"Robin Hood","3":"1973","4":"Adventure|Animation|Children|Comedy|Musical","5":"4","6":"5.0","7":"949949444"},{"1":"3039","2":"Trading Places","3":"1983","4":"Comedy","5":"4","6":"4.0","7":"949811559"},{"1":"3040","2":"Meatballs","3":"1979","4":"Comedy","5":"4","6":"5.0","7":"949919419"},{"1":"3060","2":"Commitments, The","3":"1991","4":"Comedy|Drama|Musical","5":"4","6":"5.0","7":"949919656"},{"1":"3071","2":"Stand and Deliver","3":"1988","4":"Comedy|Drama","5":"4","6":"4.0","7":"949918715"},{"1":"3101","2":"Fatal Attraction","3":"1987","4":"Drama|Thriller","5":"4","6":"4.0","7":"949896275"},{"1":"3104","2":"Midnight Run","3":"1988","4":"Action|Comedy|Crime|Thriller","5":"4","6":"4.0","7":"949811603"},{"1":"3108","2":"Fisher King, The","3":"1991","4":"Comedy|Drama|Fantasy|Romance","5":"4","6":"5.0","7":"949919738"},{"1":"3169","2":"The Falcon and the Snowman","3":"1985","4":"Crime|Drama|Thriller","5":"4","6":"4.0","7":"949918904"},{"1":"3208","2":"Loaded Weapon 1 (National Lampoon's Loaded Weapon 1)","3":"1993","4":"Action|Comedy","5":"4","6":"2.0","7":"949778946"},{"1":"3210","2":"Fast Times at Ridgemont High","3":"1982","4":"Comedy|Drama|Romance","5":"4","6":"4.0","7":"949811523"},{"1":"3251","2":"Agnes of God","3":"1985","4":"Drama|Mystery","5":"4","6":"5.0","7":"949918970"},{"1":"3255","2":"League of Their Own, A","3":"1992","4":"Comedy|Drama","5":"4","6":"4.0","7":"949919738"},{"1":"3263","2":"White Men Can't Jump","3":"1992","4":"Comedy|Drama","5":"4","6":"3.0","7":"949919845"},{"1":"3265","2":"Hard-Boiled (Lat sau san taam)","3":"1992","4":"Action|Crime|Drama|Thriller","5":"4","6":"5.0","7":"949895732"},{"1":"4006","2":"Transformers: The Movie","3":"1986","4":"Adventure|Animation|Children|Sci-Fi","5":"4","6":"2.0","7":"949982238"},{"1":"3","2":"Grumpier Old Men","3":"1995","4":"Comedy|Romance","5":"5","6":"4.0","7":"1163374957"},{"1":"39","2":"Clueless","3":"1995","4":"Comedy|Romance","5":"5","6":"4.0","7":"1163374952"},{"1":"104","2":"Happy Gilmore","3":"1996","4":"Comedy","5":"5","6":"4.0","7":"1163374639"},{"1":"141","2":"Birdcage, The","3":"1996","4":"Comedy","5":"5","6":"4.0","7":"1163374242"},{"1":"150","2":"Apollo 13","3":"1995","4":"Adventure|Drama|IMAX","5":"5","6":"4.0","7":"1163374404"},{"1":"231","2":"Dumb & Dumber (Dumb and Dumber)","3":"1994","4":"Adventure|Comedy","5":"5","6":"3.5","7":"1163373762"},{"1":"277","2":"Miracle on 34th Street","3":"1994","4":"Drama","5":"5","6":"4.5","7":"1163373208"},{"1":"344","2":"Ace Ventura: Pet Detective","3":"1994","4":"Comedy","5":"5","6":"3.5","7":"1163373636"},{"1":"356","2":"Forrest Gump","3":"1994","4":"Comedy|Drama|Romance|War","5":"5","6":"4.0","7":"1163374152"},{"1":"364","2":"Lion King, The","3":"1994","4":"Adventure|Animation|Children|Drama|Musical|IMAX","5":"5","6":"4.0","7":"1163373752"},{"1":"367","2":"Mask, The","3":"1994","4":"Action|Comedy|Crime|Fantasy","5":"5","6":"4.0","7":"1163373755"},{"1":"377","2":"Speed","3":"1994","4":"Action|Romance|Thriller","5":"5","6":"4.0","7":"1163373610"},{"1":"440","2":"Dave","3":"1993","4":"Comedy|Romance","5":"5","6":"4.0","7":"1163374477"},{"1":"500","2":"Mrs. Doubtfire","3":"1993","4":"Comedy|Drama","5":"5","6":"4.5","7":"1163373718"},{"1":"586","2":"Home Alone","3":"1990","4":"Children|Comedy","5":"5","6":"4.0","7":"1163374392"},{"1":"588","2":"Aladdin","3":"1992","4":"Adventure|Animation|Children|Comedy|Musical","5":"5","6":"3.5","7":"1163373551"},{"1":"595","2":"Beauty and the Beast","3":"1991","4":"Animation|Children|Fantasy|Musical|Romance|IMAX","5":"5","6":"4.0","7":"1163374190"},{"1":"597","2":"Pretty Woman","3":"1990","4":"Comedy|Romance","5":"5","6":"5.0","7":"1163373711"},{"1":"788","2":"Nutty Professor, The","3":"1996","4":"Comedy|Fantasy|Romance|Sci-Fi","5":"5","6":"3.5","7":"1163374993"},{"1":"858","2":"Godfather, The","3":"1972","4":"Crime|Drama","5":"5","6":"2.5","7":"1163373651"},{"1":"903","2":"Vertigo","3":"1958","4":"Drama|Mystery|Romance|Thriller","5":"5","6":"3.5","7":"1163373135"},{"1":"919","2":"Wizard of Oz, The","3":"1939","4":"Adventure|Children|Fantasy|Musical","5":"5","6":"4.0","7":"1163374286"},{"1":"1022","2":"Cinderella","3":"1950","4":"Animation|Children|Fantasy|Musical|Romance","5":"5","6":"4.0","7":"1163373316"},{"1":"1035","2":"Sound of Music, The","3":"1965","4":"Musical|Romance","5":"5","6":"5.0","7":"1163373103"},{"1":"1193","2":"One Flew Over the Cuckoo's Nest","3":"1975","4":"Drama","5":"5","6":"3.0","7":"1163374173"},{"1":"1221","2":"Godfather: Part II, The","3":"1974","4":"Crime|Drama","5":"5","6":"2.5","7":"1163374239"},{"1":"1247","2":"Graduate, The","3":"1967","4":"Comedy|Drama|Romance","5":"5","6":"4.0","7":"1163374582"},{"1":"1307","2":"When Harry Met Sally...","3":"1989","4":"Comedy|Romance","5":"5","6":"4.0","7":"1163374495"},{"1":"1380","2":"Grease","3":"1978","4":"Comedy|Musical|Romance","5":"5","6":"5.0","7":"1163373044"},{"1":"1393","2":"Jerry Maguire","3":"1996","4":"Drama|Romance","5":"5","6":"3.5","7":"1163374165"},{"1":"1485","2":"Liar Liar","3":"1997","4":"Comedy","5":"5","6":"4.5","7":"1163374608"},{"1":"1544","2":"Lost World: Jurassic Park, The","3":"1997","4":"Action|Adventure|Sci-Fi|Thriller","5":"5","6":"3.5","7":"1163374562"},{"1":"1682","2":"Truman Show, The","3":"1998","4":"Comedy|Drama|Sci-Fi","5":"5","6":"4.0","7":"1163374214"},{"1":"1721","2":"Titanic","3":"1997","4":"Drama|Romance","5":"5","6":"4.0","7":"1163373744"},{"1":"1777","2":"Wedding Singer, The","3":"1998","4":"Comedy|Romance","5":"5","6":"4.0","7":"1163374601"},{"1":"1784","2":"As Good as It Gets","3":"1997","4":"Comedy|Drama|Romance","5":"5","6":"4.5","7":"1163374251"},{"1":"1923","2":"There's Something About Mary","3":"1998","4":"Comedy|Romance","5":"5","6":"4.5","7":"1163373726"},{"1":"1961","2":"Rain Man","3":"1988","4":"Drama","5":"5","6":"4.0","7":"1163373675"},{"1":"1968","2":"Breakfast Club, The","3":"1985","4":"Comedy|Drama","5":"5","6":"4.0","7":"1163374324"},{"1":"1997","2":"Exorcist, The","3":"1973","4":"Horror|Mystery","5":"5","6":"3.5","7":"1163374593"},{"1":"2023","2":"Godfather: Part III, The","3":"1990","4":"Crime|Drama|Mystery|Thriller","5":"5","6":"1.5","7":"1163373188"},{"1":"2081","2":"Little Mermaid, The","3":"1989","4":"Animation|Children|Comedy|Musical|Romance","5":"5","6":"5.0","7":"1163373109"},{"1":"2273","2":"Rush Hour","3":"1998","4":"Action|Comedy|Crime|Thriller","5":"5","6":"4.0","7":"1163373276"},{"1":"2294","2":"Antz","3":"1998","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"5","6":"4.0","7":"1163373251"},{"1":"2355","2":"Bug's Life, A","3":"1998","4":"Adventure|Animation|Children|Comedy","5":"5","6":"3.5","7":"1163374184"},{"1":"2424","2":"You've Got Mail","3":"1998","4":"Comedy|Romance","5":"5","6":"4.0","7":"1163373193"},{"1":"2502","2":"Office Space","3":"1999","4":"Comedy|Crime","5":"5","6":"3.5","7":"1163374290"},{"1":"2683","2":"Austin Powers: The Spy Who Shagged Me","3":"1999","4":"Action|Adventure|Comedy","5":"5","6":"4.0","7":"1163373679"},{"1":"2694","2":"Big Daddy","3":"1999","4":"Comedy","5":"5","6":"4.5","7":"1163373293"},{"1":"2706","2":"American Pie","3":"1999","4":"Comedy|Romance","5":"5","6":"4.0","7":"1163374246"},{"1":"2762","2":"Sixth Sense, The","3":"1999","4":"Drama|Horror|Mystery","5":"5","6":"3.5","7":"1163373743"},{"1":"2770","2":"Bowfinger","3":"1999","4":"Comedy","5":"5","6":"3.5","7":"1163374947"},{"1":"2918","2":"Ferris Bueller's Day Off","3":"1986","4":"Comedy","5":"5","6":"3.5","7":"1163374278"},{"1":"2997","2":"Being John Malkovich","3":"1999","4":"Comedy|Drama|Fantasy","5":"5","6":"3.5","7":"1163374127"},{"1":"3114","2":"Toy Story 2","3":"1999","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"5","6":"3.5","7":"1163374263"},{"1":"3176","2":"Talented Mr. Ripley, The","3":"1999","4":"Drama|Mystery|Thriller","5":"5","6":"3.5","7":"1163374408"},{"1":"3408","2":"Erin Brockovich","3":"2000","4":"Drama","5":"5","6":"4.0","7":"1163374137"},{"1":"3753","2":"Patriot, The","3":"2000","4":"Action|Drama|War","5":"5","6":"3.5","7":"1163373148"},{"1":"3897","2":"Almost Famous","3":"2000","4":"Drama","5":"5","6":"4.5","7":"1163374235"},{"1":"3948","2":"Meet the Parents","3":"2000","4":"Comedy","5":"5","6":"3.5","7":"1163374198"},{"1":"4014","2":"Chocolat","3":"2000","4":"Drama|Romance","5":"5","6":"4.0","7":"1163373273"},{"1":"4018","2":"What Women Want","3":"2000","4":"Comedy|Romance","5":"5","6":"4.0","7":"1163375025"},{"1":"4022","2":"Cast Away","3":"2000","4":"Drama","5":"5","6":"3.5","7":"1163373696"},{"1":"4025","2":"Miss Congeniality","3":"2000","4":"Comedy|Crime","5":"5","6":"4.5","7":"1163375145"},{"1":"4306","2":"Shrek","3":"2001","4":"Adventure|Animation|Children|Comedy|Fantasy|Romance","5":"5","6":"3.5","7":"1163374103"},{"1":"4308","2":"Moulin Rouge","3":"2001","4":"Drama|Musical|Romance","5":"5","6":"3.5","7":"1163374354"},{"1":"4447","2":"Legally Blonde","3":"2001","4":"Comedy|Romance","5":"5","6":"4.5","7":"1163375033"},{"1":"4718","2":"American Pie 2","3":"2001","4":"Comedy","5":"5","6":"3.5","7":"1163375131"},{"1":"4963","2":"Ocean's Eleven","3":"2001","4":"Crime|Thriller","5":"5","6":"3.0","7":"1163373548"},{"1":"4995","2":"Beautiful Mind, A","3":"2001","4":"Drama|Romance","5":"5","6":"4.5","7":"1163373616"},{"1":"5266","2":"Panic Room","3":"2002","4":"Thriller","5":"5","6":"3.5","7":"1163374995"},{"1":"5299","2":"My Big Fat Greek Wedding","3":"2002","4":"Comedy|Romance","5":"5","6":"4.5","7":"1163373299"},{"1":"5349","2":"Spider-Man","3":"2002","4":"Action|Adventure|Sci-Fi|Thriller","5":"5","6":"4.5","7":"1163373606"},{"1":"5464","2":"Road to Perdition","3":"2002","4":"Crime|Drama","5":"5","6":"4.0","7":"1163374443"},{"1":"5669","2":"Bowling for Columbine","3":"2002","4":"Documentary","5":"5","6":"3.5","7":"1163374206"},{"1":"5679","2":"Ring, The","3":"2002","4":"Horror|Mystery|Thriller","5":"5","6":"4.5","7":"1163374389"},{"1":"5816","2":"Harry Potter and the Chamber of Secrets","3":"2002","4":"Adventure|Fantasy","5":"5","6":"3.0","7":"1163373688"},{"1":"5995","2":"Pianist, The","3":"2002","4":"Drama|War","5":"5","6":"4.0","7":"1163374533"},{"1":"6218","2":"Bend It Like Beckham","3":"2002","4":"Comedy|Drama|Romance","5":"5","6":"3.5","7":"1163374491"},{"1":"6373","2":"Bruce Almighty","3":"2003","4":"Comedy|Drama|Fantasy|Romance","5":"5","6":"4.0","7":"1163374572"},{"1":"6377","2":"Finding Nemo","3":"2003","4":"Adventure|Animation|Children|Comedy","5":"5","6":"4.0","7":"1163373626"},{"1":"6502","2":"28 Days Later","3":"2002","4":"Action|Horror|Sci-Fi","5":"5","6":"4.0","7":"1163374455"},{"1":"6711","2":"Lost in Translation","3":"2003","4":"Comedy|Drama|Romance","5":"5","6":"2.5","7":"1163373238"},{"1":"6942","2":"Love Actually","3":"2003","4":"Comedy|Drama|Romance","5":"5","6":"4.0","7":"1163374511"},{"1":"8376","2":"Napoleon Dynamite","3":"2004","4":"Comedy","5":"5","6":"4.0","7":"1163374742"},{"1":"8464","2":"Super Size Me","3":"2004","4":"Comedy|Documentary|Drama","5":"5","6":"4.0","7":"1163374381"},{"1":"8622","2":"Fahrenheit 9/11","3":"2004","4":"Documentary","5":"5","6":"3.5","7":"1163374118"},{"1":"8636","2":"Spider-Man 2","3":"2004","4":"Action|Adventure|Sci-Fi|IMAX","5":"5","6":"4.5","7":"1163373593"},{"1":"8644","2":"I, Robot","3":"2004","4":"Action|Adventure|Sci-Fi|Thriller","5":"5","6":"4.0","7":"1163374426"},{"1":"30707","2":"Million Dollar Baby","3":"2004","4":"Drama","5":"5","6":"4.5","7":"1163374340"},{"1":"30749","2":"Hotel Rwanda","3":"2004","4":"Drama|War","5":"5","6":"4.5","7":"1163374702"},{"1":"30793","2":"Charlie and the Chocolate Factory","3":"2005","4":"Adventure|Children|Comedy|Fantasy|IMAX","5":"5","6":"3.5","7":"1163374177"},{"1":"33166","2":"Crash","3":"2004","4":"Crime|Drama","5":"5","6":"5.0","7":"1163374211"},{"1":"33679","2":"Mr. & Mrs. Smith","3":"2005","4":"Action|Adventure|Comedy|Romance","5":"5","6":"4.0","7":"1163374517"},{"1":"34162","2":"Wedding Crashers","3":"2005","4":"Comedy|Romance","5":"5","6":"4.5","7":"1163374227"},{"1":"35836","2":"40-Year-Old Virgin, The","3":"2005","4":"Comedy|Romance","5":"5","6":"4.0","7":"1163374275"},{"1":"40819","2":"Walk the Line","3":"2005","4":"Drama|Musical|Romance","5":"5","6":"4.5","7":"1163374283"},{"1":"41566","2":"Chronicles of Narnia: The Lion, the Witch and the Wardrobe, The","3":"2005","4":"Adventure|Children|Fantasy","5":"5","6":"4.0","7":"1163374144"},{"1":"41569","2":"King Kong","3":"2005","4":"Action|Adventure|Drama|Fantasy|Thriller","5":"5","6":"4.0","7":"1163374167"},{"1":"48385","2":"Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan","3":"2006","4":"Comedy","5":"5","6":"4.5","7":"1163374357"},{"1":"111","2":"Taxi Driver","3":"1976","4":"Crime|Drama|Thriller","5":"6","6":"4.0","7":"1109258212"},{"1":"158","2":"Casper","3":"1995","4":"Adventure|Children","5":"6","6":"2.0","7":"1108134263"},{"1":"173","2":"Judge Dredd","3":"1995","4":"Action|Crime|Sci-Fi","5":"6","6":"2.0","7":"1109258228"},{"1":"293","2":"Léon: The Professional (a.k.a. The Professional) (Léon)","3":"1994","4":"Action|Crime|Drama|Thriller","5":"6","6":"5.0","7":"1108134539"},{"1":"596","2":"Pinocchio","3":"1940","4":"Animation|Children|Fantasy|Musical","5":"6","6":"4.0","7":"1108134269"},{"1":"903","2":"Vertigo","3":"1958","4":"Drama|Mystery|Romance|Thriller","5":"6","6":"4.0","7":"1108134299"},{"1":"1204","2":"Lawrence of Arabia","3":"1962","4":"Adventure|Drama|War","5":"6","6":"5.0","7":"1108134266"},{"1":"1250","2":"Bridge on the River Kwai, The","3":"1957","4":"Adventure|Drama|War","5":"6","6":"4.5","7":"1108134284"},{"1":"1259","2":"Stand by Me","3":"1986","4":"Adventure|Drama","5":"6","6":"4.5","7":"1109258196"},{"1":"1276","2":"Cool Hand Luke","3":"1967","4":"Drama","5":"6","6":"4.5","7":"1108134309"},{"1":"1285","2":"Heathers","3":"1989","4":"Comedy","5":"6","6":"4.5","7":"1108134339"},{"1":"1358","2":"Sling Blade","3":"1996","4":"Drama","5":"6","6":"2.0","7":"1109258181"},{"1":"1639","2":"Chasing Amy","3":"1997","4":"Comedy|Drama|Romance","5":"6","6":"2.0","7":"1109258179"},{"1":"1687","2":"Jackal, The","3":"1997","4":"Action|Thriller","5":"6","6":"2.0","7":"1109258281"},{"1":"1747","2":"Wag the Dog","3":"1997","4":"Comedy","5":"6","6":"2.0","7":"1109258194"},{"1":"1876","2":"Deep Impact","3":"1998","4":"Drama|Sci-Fi|Thriller","5":"6","6":"0.5","7":"1108134334"},{"1":"1909","2":"X-Files: Fight the Future, The","3":"1998","4":"Action|Crime|Mystery|Sci-Fi|Thriller","5":"6","6":"3.0","7":"1108134344"},{"1":"2001","2":"Lethal Weapon 2","3":"1989","4":"Action|Comedy|Crime|Drama","5":"6","6":"3.0","7":"1108134289"},{"1":"2019","2":"Seven Samurai (Shichinin no samurai)","3":"1954","4":"Action|Adventure|Drama","5":"6","6":"4.0","7":"1109258270"},{"1":"2072","2":"'burbs, The","3":"1989","4":"Comedy","5":"6","6":"4.0","7":"1109258285"},{"1":"2174","2":"Beetlejuice","3":"1988","4":"Comedy|Fantasy","5":"6","6":"4.0","7":"1109258175"},{"1":"2502","2":"Office Space","3":"1999","4":"Comedy|Crime","5":"6","6":"3.5","7":"1108134291"},{"1":"2528","2":"Logan's Run","3":"1976","4":"Action|Adventure|Sci-Fi","5":"6","6":"3.0","7":"1109258245"},{"1":"2529","2":"Planet of the Apes","3":"1968","4":"Action|Drama|Sci-Fi","5":"6","6":"4.0","7":"1108134293"},{"1":"2571","2":"Matrix, The","3":"1999","4":"Action|Sci-Fi|Thriller","5":"6","6":"1.0","7":"1109258202"},{"1":"2657","2":"Rocky Horror Picture Show, The","3":"1975","4":"Comedy|Horror|Musical|Sci-Fi","5":"6","6":"2.0","7":"1108134271"},{"1":"2692","2":"Run Lola Run (Lola rennt)","3":"1998","4":"Action|Crime","5":"6","6":"4.0","7":"1108134274"},{"1":"2723","2":"Mystery Men","3":"1999","4":"Action|Comedy|Fantasy","5":"6","6":"3.0","7":"1109258257"},{"1":"2761","2":"Iron Giant, The","3":"1999","4":"Adventure|Animation|Children|Drama|Sci-Fi","5":"6","6":"4.5","7":"1108134545"},{"1":"2890","2":"Three Kings","3":"1999","4":"Action|Adventure|Comedy|Drama|War","5":"6","6":"3.0","7":"1109258199"},{"1":"3052","2":"Dogma","3":"1999","4":"Adventure|Comedy|Fantasy","5":"6","6":"1.0","7":"1108134337"},{"1":"3114","2":"Toy Story 2","3":"1999","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"6","6":"4.0","7":"1109258183"},{"1":"3300","2":"Pitch Black","3":"2000","4":"Horror|Sci-Fi|Thriller","5":"6","6":"3.5","7":"1109258250"},{"1":"3751","2":"Chicken Run","3":"2000","4":"Animation|Children|Comedy","5":"6","6":"1.5","7":"1109258190"},{"1":"4641","2":"Ghost World","3":"2001","4":"Comedy|Drama","5":"6","6":"1.5","7":"1109258217"},{"1":"4975","2":"Vanilla Sky","3":"2001","4":"Mystery|Romance|Sci-Fi|Thriller","5":"6","6":"1.5","7":"1109258226"},{"1":"5952","2":"Lord of the Rings: The Two Towers, The","3":"2002","4":"Adventure|Fantasy","5":"6","6":"5.0","7":"1108134311"},{"1":"7090","2":"Hero (Ying xiong)","3":"2002","4":"Action|Adventure|Drama","5":"6","6":"3.0","7":"1108134534"},{"1":"7153","2":"Lord of the Rings: The Return of the King, The","3":"2003","4":"Action|Adventure|Drama|Fantasy","5":"6","6":"5.0","7":"1108134519"},{"1":"7361","2":"Eternal Sunshine of the Spotless Mind","3":"2004","4":"Drama|Romance|Sci-Fi","5":"6","6":"4.0","7":"1108134524"},{"1":"8368","2":"Harry Potter and the Prisoner of Azkaban","3":"2004","4":"Adventure|Fantasy|IMAX","5":"6","6":"3.5","7":"1108134526"},{"1":"8636","2":"Spider-Man 2","3":"2004","4":"Action|Adventure|Sci-Fi|IMAX","5":"6","6":"4.0","7":"1108134537"},{"1":"8784","2":"Garden State","3":"2004","4":"Comedy|Drama|Romance","5":"6","6":"3.0","7":"1108134531"},{"1":"8874","2":"Shaun of the Dead","3":"2004","4":"Comedy|Horror","5":"6","6":"4.5","7":"1108134521"},{"1":"1","2":"Toy Story","3":"1995","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"7","6":"3.0","7":"851866703"},{"1":"10","2":"GoldenEye","3":"1995","4":"Action|Adventure|Thriller","5":"7","6":"3.0","7":"851869035"},{"1":"21","2":"Get Shorty","3":"1995","4":"Comedy|Crime|Thriller","5":"7","6":"3.0","7":"851867289"},{"1":"31","2":"Dangerous Minds","3":"1995","4":"Drama","5":"7","6":"3.0","7":"851868750"},{"1":"34","2":"Babe","3":"1995","4":"Children|Drama","5":"7","6":"4.0","7":"851867861"},{"1":"40","2":"Cry, the Beloved Country","3":"1995","4":"Drama","5":"7","6":"4.0","7":"851866901"},{"1":"104","2":"Happy Gilmore","3":"1996","4":"Comedy","5":"7","6":"3.0","7":"851866744"},{"1":"110","2":"Braveheart","3":"1995","4":"Action|Drama|War","5":"7","6":"5.0","7":"851868188"},{"1":"112","2":"Rumble in the Bronx (Hont faan kui)","3":"1995","4":"Action|Adventure|Comedy|Crime","5":"7","6":"4.0","7":"851866720"},{"1":"141","2":"Birdcage, The","3":"1996","4":"Comedy","5":"7","6":"4.0","7":"851866704"},{"1":"151","2":"Rob Roy","3":"1995","4":"Action|Drama|Romance|War","5":"7","6":"4.0","7":"851868206"},{"1":"198","2":"Strange Days","3":"1995","4":"Action|Crime|Drama|Mystery|Sci-Fi|Thriller","5":"7","6":"2.0","7":"851868704"},{"1":"207","2":"Walk in the Clouds, A","3":"1995","4":"Drama|Romance","5":"7","6":"3.0","7":"851868705"},{"1":"260","2":"Star Wars: Episode IV - A New Hope","3":"1977","4":"Action|Adventure|Sci-Fi","5":"7","6":"5.0","7":"851869062"},{"1":"272","2":"Madness of King George, The","3":"1994","4":"Comedy|Drama","5":"7","6":"3.0","7":"851868188"},{"1":"316","2":"Stargate","3":"1994","4":"Action|Adventure|Sci-Fi","5":"7","6":"2.0","7":"851869161"},{"1":"318","2":"Shawshank Redemption, The","3":"1994","4":"Crime|Drama","5":"7","6":"5.0","7":"851868187"},{"1":"329","2":"Star Trek: Generations","3":"1994","4":"Adventure|Drama|Sci-Fi","5":"7","6":"3.0","7":"851868524"},{"1":"333","2":"Tommy Boy","3":"1995","4":"Comedy","5":"7","6":"3.0","7":"851867345"},{"1":"345","2":"Adventures of Priscilla, Queen of the Desert, The","3":"1994","4":"Comedy|Drama","5":"7","6":"3.0","7":"851867621"},{"1":"355","2":"Flintstones, The","3":"1994","4":"Children|Comedy|Fantasy","5":"7","6":"3.0","7":"851867941"},{"1":"356","2":"Forrest Gump","3":"1994","4":"Comedy|Drama|Romance|War","5":"7","6":"3.0","7":"851868188"},{"1":"357","2":"Four Weddings and a Funeral","3":"1994","4":"Comedy|Romance","5":"7","6":"3.0","7":"851867436"},{"1":"364","2":"Lion King, The","3":"1994","4":"Adventure|Animation|Children|Drama|Musical|IMAX","5":"7","6":"3.0","7":"851868020"},{"1":"367","2":"Mask, The","3":"1994","4":"Action|Comedy|Crime|Fantasy","5":"7","6":"3.0","7":"851867790"},{"1":"377","2":"Speed","3":"1994","4":"Action|Romance|Thriller","5":"7","6":"3.0","7":"851869291"},{"1":"380","2":"True Lies","3":"1994","4":"Action|Adventure|Comedy|Romance|Thriller","5":"7","6":"4.0","7":"851869291"},{"1":"480","2":"Jurassic Park","3":"1993","4":"Action|Adventure|Sci-Fi|Thriller","5":"7","6":"4.0","7":"851869161"},{"1":"500","2":"Mrs. Doubtfire","3":"1993","4":"Comedy|Drama","5":"7","6":"3.0","7":"851867669"},{"1":"534","2":"Shadowlands","3":"1993","4":"Drama|Romance","5":"7","6":"4.0","7":"851868246"},{"1":"539","2":"Sleepless in Seattle","3":"1993","4":"Comedy|Drama|Romance","5":"7","6":"3.0","7":"851867688"},{"1":"541","2":"Blade Runner","3":"1982","4":"Action|Sci-Fi|Thriller","5":"7","6":"4.0","7":"851869035"},{"1":"551","2":"Nightmare Before Christmas, The","3":"1993","4":"Animation|Children|Fantasy|Musical","5":"7","6":"4.0","7":"851868019"},{"1":"588","2":"Aladdin","3":"1992","4":"Adventure|Animation|Children|Comedy|Musical","5":"7","6":"4.0","7":"851868044"},{"1":"589","2":"Terminator 2: Judgment Day","3":"1991","4":"Action|Sci-Fi","5":"7","6":"3.0","7":"851869140"},{"1":"590","2":"Dances with Wolves","3":"1990","4":"Adventure|Drama|Western","5":"7","6":"4.0","7":"851868186"},{"1":"592","2":"Batman","3":"1989","4":"Action|Crime|Thriller","5":"7","6":"3.0","7":"851868863"},{"1":"594","2":"Snow White and the Seven Dwarfs","3":"1937","4":"Animation|Children|Drama|Fantasy|Musical","5":"7","6":"4.0","7":"851868020"},{"1":"595","2":"Beauty and the Beast","3":"1991","4":"Animation|Children|Fantasy|Musical|Romance|IMAX","5":"7","6":"3.0","7":"851868044"},{"1":"610","2":"Heavy Metal","3":"1981","4":"Action|Adventure|Animation|Horror|Sci-Fi","5":"7","6":"3.0","7":"851868074"},{"1":"671","2":"Mystery Science Theater 3000: The Movie","3":"1996","4":"Comedy|Sci-Fi","5":"7","6":"4.0","7":"851866806"},{"1":"708","2":"Truth About Cats & Dogs, The","3":"1996","4":"Comedy|Romance","5":"7","6":"3.0","7":"851866744"},{"1":"720","2":"Wallace & Gromit: The Best of Aardman Animation","3":"1996","4":"Adventure|Animation|Comedy","5":"7","6":"5.0","7":"851868019"},{"1":"724","2":"Craft, The","3":"1996","4":"Drama|Fantasy|Horror|Thriller","5":"7","6":"2.0","7":"851866784"},{"1":"736","2":"Twister","3":"1996","4":"Action|Adventure|Romance|Thriller","5":"7","6":"1.0","7":"851866704"},{"1":"737","2":"Barb Wire","3":"1996","4":"Action|Sci-Fi","5":"7","6":"1.0","7":"851866763"},{"1":"745","2":"Wallace & Gromit: A Close Shave","3":"1995","4":"Animation|Children|Comedy","5":"7","6":"5.0","7":"851868020"},{"1":"780","2":"Independence Day (a.k.a. ID4)","3":"1996","4":"Action|Adventure|Sci-Fi|Thriller","5":"7","6":"3.0","7":"851866703"},{"1":"786","2":"Eraser","3":"1996","4":"Action|Drama|Thriller","5":"7","6":"2.0","7":"851866744"},{"1":"924","2":"2001: A Space Odyssey","3":"1968","4":"Adventure|Drama|Sci-Fi","5":"7","6":"4.0","7":"851868289"},{"1":"1036","2":"Die Hard","3":"1988","4":"Action|Crime|Thriller","5":"7","6":"3.0","7":"851869102"},{"1":"1073","2":"Willy Wonka & the Chocolate Factory","3":"1971","4":"Children|Comedy|Fantasy|Musical","5":"7","6":"3.0","7":"851866744"},{"1":"1079","2":"Fish Called Wanda, A","3":"1988","4":"Comedy|Crime","5":"7","6":"4.0","7":"851867320"},{"1":"1080","2":"Monty Python's Life of Brian","3":"1979","4":"Comedy","5":"7","6":"4.0","7":"851867289"},{"1":"1097","2":"E.T. the Extra-Terrestrial","3":"1982","4":"Children|Drama|Sci-Fi","5":"7","6":"3.0","7":"851868308"},{"1":"1125","2":"Return of the Pink Panther, The","3":"1975","4":"Comedy|Crime","5":"7","6":"3.0","7":"851867553"},{"1":"1129","2":"Escape from New York","3":"1981","4":"Action|Adventure|Sci-Fi|Thriller","5":"7","6":"3.0","7":"851869103"},{"1":"1136","2":"Monty Python and the Holy Grail","3":"1975","4":"Adventure|Comedy|Fantasy","5":"7","6":"4.0","7":"851867289"},{"1":"1148","2":"Wallace & Gromit: The Wrong Trousers","3":"1993","4":"Animation|Children|Comedy|Crime","5":"7","6":"5.0","7":"851868019"},{"1":"1196","2":"Star Wars: Episode V - The Empire Strikes Back","3":"1980","4":"Action|Adventure|Sci-Fi","5":"7","6":"5.0","7":"851869034"},{"1":"1197","2":"Princess Bride, The","3":"1987","4":"Action|Adventure|Comedy|Fantasy|Romance","5":"7","6":"3.0","7":"851867289"},{"1":"1198","2":"Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark)","3":"1981","4":"Action|Adventure","5":"7","6":"5.0","7":"851869035"},{"1":"1210","2":"Star Wars: Episode VI - Return of the Jedi","3":"1983","4":"Action|Adventure|Sci-Fi","5":"7","6":"5.0","7":"851869034"},{"1":"1220","2":"Blues Brothers, The","3":"1980","4":"Action|Comedy|Musical","5":"7","6":"4.0","7":"851867379"},{"1":"1223","2":"Grand Day Out with Wallace and Gromit, A","3":"1989","4":"Adventure|Animation|Children|Comedy|Sci-Fi","5":"7","6":"5.0","7":"851868044"},{"1":"1225","2":"Amadeus","3":"1984","4":"Drama","5":"7","6":"5.0","7":"851868246"},{"1":"1231","2":"Right Stuff, The","3":"1983","4":"Drama","5":"7","6":"4.0","7":"851868321"},{"1":"1240","2":"Terminator, The","3":"1984","4":"Action|Sci-Fi|Thriller","5":"7","6":"4.0","7":"851869062"},{"1":"1242","2":"Glory","3":"1989","4":"Drama|War","5":"7","6":"5.0","7":"851868289"},{"1":"1270","2":"Back to the Future","3":"1985","4":"Adventure|Comedy|Sci-Fi","5":"7","6":"3.0","7":"851867436"},{"1":"1275","2":"Highlander","3":"1986","4":"Action|Adventure|Fantasy","5":"7","6":"4.0","7":"851869080"},{"1":"1278","2":"Young Frankenstein","3":"1974","4":"Comedy|Fantasy","5":"7","6":"3.0","7":"851867436"},{"1":"1287","2":"Ben-Hur","3":"1959","4":"Action|Adventure|Drama","5":"7","6":"4.0","7":"851868332"},{"1":"1288","2":"This Is Spinal Tap","3":"1984","4":"Comedy","5":"7","6":"4.0","7":"851867401"},{"1":"1291","2":"Indiana Jones and the Last Crusade","3":"1989","4":"Action|Adventure","5":"7","6":"3.0","7":"851867320"},{"1":"1298","2":"Pink Floyd: The Wall","3":"1982","4":"Drama|Musical","5":"7","6":"3.0","7":"851868471"},{"1":"1302","2":"Field of Dreams","3":"1989","4":"Children|Drama|Fantasy","5":"7","6":"4.0","7":"851868471"},{"1":"1307","2":"When Harry Met Sally...","3":"1989","4":"Comedy|Romance","5":"7","6":"3.0","7":"851867401"},{"1":"1353","2":"Mirror Has Two Faces, The","3":"1996","4":"Comedy|Drama|Romance","5":"7","6":"3.0","7":"851866935"},{"1":"1371","2":"Star Trek: The Motion Picture","3":"1979","4":"Adventure|Sci-Fi","5":"7","6":"3.0","7":"851869160"},{"1":"1372","2":"Star Trek VI: The Undiscovered Country","3":"1991","4":"Action|Mystery|Sci-Fi","5":"7","6":"3.0","7":"851869102"},{"1":"1373","2":"Star Trek V: The Final Frontier","3":"1989","4":"Action|Sci-Fi","5":"7","6":"2.0","7":"851869230"},{"1":"1374","2":"Star Trek II: The Wrath of Khan","3":"1982","4":"Action|Adventure|Sci-Fi|Thriller","5":"7","6":"4.0","7":"851869035"},{"1":"1375","2":"Star Trek III: The Search for Spock","3":"1984","4":"Action|Adventure|Sci-Fi","5":"7","6":"3.0","7":"851869140"},{"1":"1376","2":"Star Trek IV: The Voyage Home","3":"1986","4":"Adventure|Comedy|Sci-Fi","5":"7","6":"3.0","7":"851869062"},{"1":"1394","2":"Raising Arizona","3":"1987","4":"Comedy","5":"7","6":"3.0","7":"851867688"},{"1":"1405","2":"Beavis and Butt-Head Do America","3":"1996","4":"Adventure|Animation|Comedy|Crime","5":"7","6":"5.0","7":"851866978"},{"1":"1408","2":"Last of the Mohicans, The","3":"1992","4":"Action|Romance|War|Western","5":"7","6":"1.0","7":"851869140"},{"1":"32","2":"Twelve Monkeys (a.k.a. 12 Monkeys)","3":"1995","4":"Mystery|Sci-Fi|Thriller","5":"8","6":"5.0","7":"1154465405"},{"1":"45","2":"To Die For","3":"1995","4":"Comedy|Drama|Thriller","5":"8","6":"2.5","7":"1154389572"},{"1":"47","2":"Seven (a.k.a. Se7en)","3":"1995","4":"Mystery|Thriller","5":"8","6":"5.0","7":"1154464836"},{"1":"50","2":"Usual Suspects, The","3":"1995","4":"Crime|Mystery|Thriller","5":"8","6":"5.0","7":"1154400173"},{"1":"110","2":"Braveheart","3":"1995","4":"Action|Drama|War","5":"8","6":"4.0","7":"1154473268"},{"1":"260","2":"Star Wars: Episode IV - A New Hope","3":"1977","4":"Action|Adventure|Sci-Fi","5":"8","6":"3.5","7":"1154464829"},{"1":"282","2":"Nell","3":"1994","4":"Drama","5":"8","6":"2.0","7":"1154389420"},{"1":"296","2":"Pulp Fiction","3":"1994","4":"Comedy|Crime|Drama|Thriller","5":"8","6":"4.0","7":"1154465380"},{"1":"318","2":"Shawshank Redemption, The","3":"1994","4":"Crime|Drama","5":"8","6":"5.0","7":"1154464714"},{"1":"356","2":"Forrest Gump","3":"1994","4":"Comedy|Drama|Romance|War","5":"8","6":"4.0","7":"1154400390"},{"1":"457","2":"Fugitive, The","3":"1993","4":"Thriller","5":"8","6":"4.5","7":"1154400475"},{"1":"520","2":"Robin Hood: Men in Tights","3":"1993","4":"Comedy","5":"8","6":"3.5","7":"1154389356"},{"1":"524","2":"Rudy","3":"1993","4":"Drama","5":"8","6":"2.0","7":"1154400357"},{"1":"527","2":"Schindler's List","3":"1993","4":"Drama|War","5":"8","6":"5.0","7":"1154400170"},{"1":"543","2":"So I Married an Axe Murderer","3":"1993","4":"Comedy|Romance|Thriller","5":"8","6":"5.0","7":"1154389386"},{"1":"589","2":"Terminator 2: Judgment Day","3":"1991","4":"Action|Sci-Fi","5":"8","6":"4.0","7":"1154464853"},{"1":"593","2":"Silence of the Lambs, The","3":"1991","4":"Crime|Horror|Thriller","5":"8","6":"4.5","7":"1154464833"},{"1":"628","2":"Primal Fear","3":"1996","4":"Crime|Drama|Mystery|Thriller","5":"8","6":"4.0","7":"1154400284"},{"1":"805","2":"Time to Kill, A","3":"1996","4":"Drama|Thriller","5":"8","6":"3.5","7":"1154389432"},{"1":"858","2":"Godfather, The","3":"1972","4":"Crime|Drama","5":"8","6":"5.0","7":"1154400181"},{"1":"1196","2":"Star Wars: Episode V - The Empire Strikes Back","3":"1980","4":"Action|Adventure|Sci-Fi","5":"8","6":"3.5","7":"1154464841"},{"1":"1197","2":"Princess Bride, The","3":"1987","4":"Action|Adventure|Comedy|Fantasy|Romance","5":"8","6":"4.0","7":"1154464887"},{"1":"1198","2":"Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark)","3":"1981","4":"Action|Adventure","5":"8","6":"4.0","7":"1154464730"},{"1":"1210","2":"Star Wars: Episode VI - Return of the Jedi","3":"1983","4":"Action|Adventure|Sci-Fi","5":"8","6":"4.0","7":"1154464769"},{"1":"1219","2":"Psycho","3":"1960","4":"Crime|Horror","5":"8","6":"4.0","7":"1154465573"},{"1":"1225","2":"Amadeus","3":"1984","4":"Drama","5":"8","6":"4.0","7":"1154465394"},{"1":"1258","2":"Shining, The","3":"1980","4":"Horror","5":"8","6":"4.0","7":"1154465555"},{"1":"1259","2":"Stand by Me","3":"1986","4":"Adventure|Drama","5":"8","6":"4.0","7":"1154464811"},{"1":"1265","2":"Groundhog Day","3":"1993","4":"Comedy|Fantasy|Romance","5":"8","6":"3.0","7":"1154465230"},{"1":"1270","2":"Back to the Future","3":"1985","4":"Adventure|Comedy|Sci-Fi","5":"8","6":"4.0","7":"1154464747"},{"1":"1291","2":"Indiana Jones and the Last Crusade","3":"1989","4":"Action|Adventure","5":"8","6":"4.0","7":"1154464720"},{"1":"1302","2":"Field of Dreams","3":"1989","4":"Children|Drama|Fantasy","5":"8","6":"3.5","7":"1154400465"},{"1":"1358","2":"Sling Blade","3":"1996","4":"Drama","5":"8","6":"0.5","7":"1154474527"},{"1":"1387","2":"Jaws","3":"1975","4":"Action|Horror","5":"8","6":"4.0","7":"1154465285"},{"1":"1393","2":"Jerry Maguire","3":"1996","4":"Drama|Romance","5":"8","6":"3.0","7":"1154400324"},{"1":"1500","2":"Grosse Pointe Blank","3":"1997","4":"Comedy|Crime|Romance","5":"8","6":"4.0","7":"1154465235"},{"1":"1552","2":"Con Air","3":"1997","4":"Action|Adventure|Thriller","5":"8","6":"3.0","7":"1154389408"},{"1":"1617","2":"L.A. Confidential","3":"1997","4":"Crime|Film-Noir|Mystery|Thriller","5":"8","6":"3.5","7":"1154464990"},{"1":"1625","2":"Game, The","3":"1997","4":"Drama|Mystery|Thriller","5":"8","6":"5.0","7":"1154389385"},{"1":"1674","2":"Witness","3":"1985","4":"Drama|Romance|Thriller","5":"8","6":"4.0","7":"1154389491"},{"1":"1704","2":"Good Will Hunting","3":"1997","4":"Drama|Romance","5":"8","6":"4.0","7":"1154473252"},{"1":"1754","2":"Fallen","3":"1998","4":"Crime|Drama|Fantasy|Thriller","5":"8","6":"3.0","7":"1154400308"},{"1":"1777","2":"Wedding Singer, The","3":"1998","4":"Comedy|Romance","5":"8","6":"5.0","7":"1154400458"},{"1":"1876","2":"Deep Impact","3":"1998","4":"Drama|Sci-Fi|Thriller","5":"8","6":"3.5","7":"1154389528"},{"1":"1961","2":"Rain Man","3":"1988","4":"Drama","5":"8","6":"5.0","7":"1154473322"},{"1":"2028","2":"Saving Private Ryan","3":"1998","4":"Action|Drama|War","5":"8","6":"4.0","7":"1154473259"},{"1":"2100","2":"Splash","3":"1984","4":"Comedy|Fantasy|Romance","5":"8","6":"3.0","7":"1154389460"},{"1":"2139","2":"Secret of NIMH, The","3":"1982","4":"Adventure|Animation|Children|Drama","5":"8","6":"3.0","7":"1154465222"},{"1":"2194","2":"Untouchables, The","3":"1987","4":"Action|Crime|Drama","5":"8","6":"4.5","7":"1154464735"},{"1":"2302","2":"My Cousin Vinny","3":"1992","4":"Comedy","5":"8","6":"4.5","7":"1154400414"},{"1":"2324","2":"Life Is Beautiful (La Vita è bella)","3":"1997","4":"Comedy|Drama|Romance|War","5":"8","6":"4.0","7":"1154464850"},{"1":"2329","2":"American History X","3":"1998","4":"Crime|Drama","5":"8","6":"5.0","7":"1154473217"},{"1":"2353","2":"Enemy of the State","3":"1998","4":"Action|Thriller","5":"8","6":"3.5","7":"1154389340"},{"1":"2423","2":"Christmas Vacation (National Lampoon's Christmas Vacation)","3":"1989","4":"Comedy","5":"8","6":"3.5","7":"1154400351"},{"1":"2502","2":"Office Space","3":"1999","4":"Comedy|Crime","5":"8","6":"5.0","7":"1154464889"},{"1":"2571","2":"Matrix, The","3":"1999","4":"Action|Sci-Fi|Thriller","5":"8","6":"5.0","7":"1154464738"},{"1":"2716","2":"Ghostbusters (a.k.a. Ghost Busters)","3":"1984","4":"Action|Comedy|Sci-Fi","5":"8","6":"3.5","7":"1154464944"},{"1":"2762","2":"Sixth Sense, The","3":"1999","4":"Drama|Horror|Mystery","5":"8","6":"4.5","7":"1154464717"},{"1":"2770","2":"Bowfinger","3":"1999","4":"Comedy","5":"8","6":"2.5","7":"1154389541"},{"1":"2791","2":"Airplane!","3":"1980","4":"Comedy","5":"8","6":"4.5","7":"1154465558"},{"1":"2797","2":"Big","3":"1988","4":"Comedy|Drama|Fantasy|Romance","5":"8","6":"3.5","7":"1154400340"},{"1":"2804","2":"Christmas Story, A","3":"1983","4":"Children|Comedy","5":"8","6":"3.5","7":"1154473419"},{"1":"2841","2":"Stir of Echoes","3":"1999","4":"Horror|Mystery|Thriller","5":"8","6":"3.0","7":"1154400360"},{"1":"2858","2":"American Beauty","3":"1999","4":"Drama|Romance","5":"8","6":"4.5","7":"1154464950"},{"1":"2918","2":"Ferris Bueller's Day Off","3":"1986","4":"Comedy","5":"8","6":"5.0","7":"1154473364"},{"1":"2959","2":"Fight Club","3":"1999","4":"Action|Crime|Drama|Thriller","5":"8","6":"4.0","7":"1154464901"},{"1":"3147","2":"Green Mile, The","3":"1999","4":"Crime|Drama","5":"8","6":"4.5","7":"1154400266"},{"1":"3578","2":"Gladiator","3":"2000","4":"Action|Adventure|Drama","5":"8","6":"5.0","7":"1154400441"},{"1":"3916","2":"Remember the Titans","3":"2000","4":"Drama","5":"8","6":"3.5","7":"1154400198"},{"1":"3948","2":"Meet the Parents","3":"2000","4":"Comedy","5":"8","6":"4.0","7":"1154389482"},{"1":"3996","2":"Crouching Tiger, Hidden Dragon (Wo hu cang long)","3":"2000","4":"Action|Drama|Romance","5":"8","6":"4.0","7":"1154465526"},{"1":"4011","2":"Snatch","3":"2000","4":"Comedy|Crime|Thriller","5":"8","6":"4.5","7":"1154464873"},{"1":"4019","2":"Finding Forrester","3":"2000","4":"Drama","5":"8","6":"3.5","7":"1154400245"},{"1":"4034","2":"Traffic","3":"2000","4":"Crime|Drama|Thriller","5":"8","6":"4.5","7":"1154465279"},{"1":"4226","2":"Memento","3":"2000","4":"Mystery|Thriller","5":"8","6":"5.0","7":"1154464905"},{"1":"4262","2":"Scarface","3":"1983","4":"Action|Crime|Drama","5":"8","6":"4.0","7":"1154464915"},{"1":"4448","2":"Score, The","3":"2001","4":"Action|Drama","5":"8","6":"2.5","7":"1154400334"},{"1":"4886","2":"Monsters, Inc.","3":"2001","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"8","6":"3.5","7":"1154464808"},{"1":"4896","2":"Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone)","3":"2001","4":"Adventure|Children|Fantasy","5":"8","6":"3.5","7":"1154400398"},{"1":"4963","2":"Ocean's Eleven","3":"2001","4":"Crime|Thriller","5":"8","6":"4.5","7":"1154400383"},{"1":"4973","2":"Amelie (Fabuleux destin d'Amélie Poulain, Le)","3":"2001","4":"Comedy|Romance","5":"8","6":"4.0","7":"1154465373"},{"1":"4993","2":"Lord of the Rings: The Fellowship of the Ring, The","3":"2001","4":"Adventure|Fantasy","5":"8","6":"3.5","7":"1154464819"},{"1":"4995","2":"Beautiful Mind, A","3":"2001","4":"Drama|Romance","5":"8","6":"3.5","7":"1154389449"},{"1":"5064","2":"The Count of Monte Cristo","3":"2002","4":"Action|Adventure|Drama|Thriller","5":"8","6":"5.0","7":"1154400236"},{"1":"5378","2":"Star Wars: Episode II - Attack of the Clones","3":"2002","4":"Action|Adventure|Sci-Fi|IMAX","5":"8","6":"3.5","7":"1154389552"},{"1":"5445","2":"Minority Report","3":"2002","4":"Action|Crime|Mystery|Sci-Fi|Thriller","5":"8","6":"4.5","7":"1154400449"},{"1":"5464","2":"Road to Perdition","3":"2002","4":"Crime|Drama","5":"8","6":"4.0","7":"1154464756"},{"1":"5630","2":"Red Dragon","3":"2002","4":"Crime|Mystery|Thriller","5":"8","6":"4.0","7":"1154400407"},{"1":"5650","2":"Strange Brew","3":"1983","4":"Comedy","5":"8","6":"4.0","7":"1154465399"},{"1":"5669","2":"Bowling for Columbine","3":"2002","4":"Documentary","5":"8","6":"3.0","7":"1154465385"},{"1":"5952","2":"Lord of the Rings: The Two Towers, The","3":"2002","4":"Adventure|Fantasy","5":"8","6":"4.0","7":"1154464762"},{"1":"5989","2":"Catch Me If You Can","3":"2002","4":"Crime|Drama","5":"8","6":"3.5","7":"1154400444"},{"1":"6377","2":"Finding Nemo","3":"2003","4":"Adventure|Animation|Children|Comedy","5":"8","6":"4.0","7":"1154389474"},{"1":"6378","2":"Italian Job, The","3":"2003","4":"Action|Crime","5":"8","6":"3.0","7":"1154400191"},{"1":"6870","2":"Mystic River","3":"2003","4":"Crime|Drama|Mystery","5":"8","6":"4.0","7":"1154464895"},{"1":"6874","2":"Kill Bill: Vol. 1","3":"2003","4":"Action|Crime|Thriller","5":"8","6":"5.0","7":"1154465296"},{"1":"6879","2":"Runaway Jury","3":"2003","4":"Drama|Thriller","5":"8","6":"3.0","7":"1154400253"},{"1":"7143","2":"Last Samurai, The","3":"2003","4":"Action|Adventure|Drama|War","5":"8","6":"3.5","7":"1154400294"},{"1":"7153","2":"Lord of the Rings: The Return of the King, The","3":"2003","4":"Action|Adventure|Drama|Fantasy","5":"8","6":"4.0","7":"1154464753"},{"1":"7361","2":"Eternal Sunshine of the Spotless Mind","3":"2004","4":"Drama|Romance|Sci-Fi","5":"8","6":"4.0","7":"1154465367"},{"1":"7438","2":"Kill Bill: Vol. 2","3":"2004","4":"Action|Drama|Thriller","5":"8","6":"4.0","7":"1154464994"},{"1":"8533","2":"Notebook, The","3":"2004","4":"Drama|Romance","5":"8","6":"2.5","7":"1154400471"},{"1":"8784","2":"Garden State","3":"2004","4":"Comedy|Drama|Romance","5":"8","6":"4.5","7":"1154464868"},{"1":"8873","2":"Motorcycle Diaries, The (Diarios de motocicleta)","3":"2004","4":"Adventure|Drama","5":"8","6":"4.0","7":"1154465499"},{"1":"8874","2":"Shaun of the Dead","3":"2004","4":"Comedy|Horror","5":"8","6":"3.0","7":"1154465548"},{"1":"32587","2":"Sin City","3":"2005","4":"Action|Crime|Film-Noir|Mystery|Thriller","5":"8","6":"3.5","7":"1154465243"},{"1":"33166","2":"Crash","3":"2004","4":"Crime|Drama","5":"8","6":"4.5","7":"1154473310"},{"1":"33493","2":"Star Wars: Episode III - Revenge of the Sith","3":"2005","4":"Action|Adventure|Sci-Fi","5":"8","6":"4.5","7":"1154400263"},{"1":"33794","2":"Batman Begins","3":"2005","4":"Action|Crime|IMAX","5":"8","6":"4.5","7":"1154464726"},{"1":"34162","2":"Wedding Crashers","3":"2005","4":"Comedy|Romance","5":"8","6":"3.5","7":"1154400423"},{"1":"40583","2":"Syriana","3":"2005","4":"Drama|Thriller","5":"8","6":"3.5","7":"1154473088"},{"1":"40819","2":"Walk the Line","3":"2005","4":"Drama|Musical|Romance","5":"8","6":"3.5","7":"1154473264"},{"1":"42007","2":"Rumor Has It...","3":"2005","4":"Comedy|Drama|Romance","5":"8","6":"2.0","7":"1154473042"},{"1":"43556","2":"Annapolis","3":"2006","4":"Drama","5":"8","6":"3.5","7":"1154473022"},{"1":"43871","2":"Firewall","3":"2006","4":"Crime|Drama|Thriller","5":"8","6":"3.0","7":"1154473014"},{"1":"44004","2":"Failure to Launch","3":"2006","4":"Comedy|Romance","5":"8","6":"3.0","7":"1154473017"},{"1":"1","2":"Toy Story","3":"1995","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"9","6":"4.0","7":"938629179"},{"1":"17","2":"Sense and Sensibility","3":"1995","4":"Drama|Romance","5":"9","6":"4.0","7":"938628337"},{"1":"26","2":"Othello","3":"1995","4":"Drama","5":"9","6":"3.0","7":"938628655"},{"1":"36","2":"Dead Man Walking","3":"1995","4":"Crime|Drama","5":"9","6":"5.0","7":"938629110"},{"1":"47","2":"Seven (a.k.a. Se7en)","3":"1995","4":"Mystery|Thriller","5":"9","6":"3.0","7":"938628897"},{"1":"318","2":"Shawshank Redemption, The","3":"1994","4":"Crime|Drama","5":"9","6":"4.0","7":"938628966"},{"1":"497","2":"Much Ado About Nothing","3":"1993","4":"Comedy|Romance","5":"9","6":"4.0","7":"938628777"},{"1":"515","2":"Remains of the Day, The","3":"1993","4":"Drama|Romance","5":"9","6":"4.0","7":"938628577"},{"1":"527","2":"Schindler's List","3":"1993","4":"Drama|War","5":"9","6":"5.0","7":"938628843"},{"1":"534","2":"Shadowlands","3":"1993","4":"Drama|Romance","5":"9","6":"5.0","7":"938628337"},{"1":"593","2":"Silence of the Lambs, The","3":"1991","4":"Crime|Horror|Thriller","5":"9","6":"4.0","7":"938628843"},{"1":"608","2":"Fargo","3":"1996","4":"Comedy|Crime|Drama|Thriller","5":"9","6":"5.0","7":"938628843"},{"1":"733","2":"Rock, The","3":"1996","4":"Action|Adventure|Thriller","5":"9","6":"2.0","7":"938628337"},{"1":"1059","2":"William Shakespeare's Romeo + Juliet","3":"1996","4":"Drama|Romance","5":"9","6":"5.0","7":"938629250"},{"1":"1177","2":"Enchanted April","3":"1992","4":"Drama|Romance","5":"9","6":"3.0","7":"938629470"},{"1":"1357","2":"Shine","3":"1996","4":"Drama|Romance","5":"9","6":"4.0","7":"938628655"},{"1":"1358","2":"Sling Blade","3":"1996","4":"Drama","5":"9","6":"4.0","7":"938628450"},{"1":"1411","2":"Hamlet","3":"1996","4":"Crime|Drama|Romance","5":"9","6":"3.0","7":"938628655"},{"1":"1541","2":"Addicted to Love","3":"1997","4":"Comedy|Romance","5":"9","6":"2.0","7":"938628777"},{"1":"1584","2":"Contact","3":"1997","4":"Drama|Sci-Fi","5":"9","6":"4.0","7":"938629341"},{"1":"1680","2":"Sliding Doors","3":"1998","4":"Drama|Romance","5":"9","6":"4.0","7":"938629054"},{"1":"1682","2":"Truman Show, The","3":"1998","4":"Comedy|Drama|Sci-Fi","5":"9","6":"5.0","7":"938628690"},{"1":"1704","2":"Good Will Hunting","3":"1997","4":"Drama|Romance","5":"9","6":"4.0","7":"938628966"},{"1":"1721","2":"Titanic","3":"1997","4":"Drama|Romance","5":"9","6":"3.0","7":"938629470"},{"1":"1784","2":"As Good as It Gets","3":"1997","4":"Comedy|Drama|Romance","5":"9","6":"5.0","7":"938628966"},{"1":"2028","2":"Saving Private Ryan","3":"1998","4":"Action|Drama|War","5":"9","6":"4.0","7":"938629341"},{"1":"2125","2":"Ever After: A Cinderella Story","3":"1998","4":"Comedy|Drama|Romance","5":"9","6":"4.0","7":"938629522"},{"1":"2140","2":"Dark Crystal, The","3":"1982","4":"Adventure|Fantasy","5":"9","6":"4.0","7":"938629747"},{"1":"2249","2":"My Blue Heaven","3":"1990","4":"Comedy","5":"9","6":"4.0","7":"938629053"},{"1":"2268","2":"Few Good Men, A","3":"1992","4":"Crime|Drama|Thriller","5":"9","6":"3.0","7":"938629053"},{"1":"2273","2":"Rush Hour","3":"1998","4":"Action|Comedy|Crime|Thriller","5":"9","6":"3.0","7":"938628777"},{"1":"2278","2":"Ronin","3":"1998","4":"Action|Crime|Thriller","5":"9","6":"4.0","7":"938628897"},{"1":"2291","2":"Edward Scissorhands","3":"1990","4":"Drama|Fantasy|Romance","5":"9","6":"4.0","7":"938629341"},{"1":"2294","2":"Antz","3":"1998","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"9","6":"2.0","7":"938629250"},{"1":"2302","2":"My Cousin Vinny","3":"1992","4":"Comedy","5":"9","6":"4.0","7":"938628843"},{"1":"2391","2":"Simple Plan, A","3":"1998","4":"Crime|Drama|Thriller","5":"9","6":"4.0","7":"939122916"},{"1":"2396","2":"Shakespeare in Love","3":"1998","4":"Comedy|Drama|Romance","5":"9","6":"4.0","7":"938628577"},{"1":"2427","2":"Thin Red Line, The","3":"1998","4":"Action|Drama|War","5":"9","6":"2.0","7":"938629681"},{"1":"2490","2":"Payback","3":"1999","4":"Action|Thriller","5":"9","6":"3.0","7":"938629681"},{"1":"2501","2":"October Sky","3":"1999","4":"Drama","5":"9","6":"4.0","7":"938628966"},{"1":"2539","2":"Analyze This","3":"1999","4":"Comedy","5":"9","6":"2.0","7":"938629341"},{"1":"2571","2":"Matrix, The","3":"1999","4":"Action|Sci-Fi|Thriller","5":"9","6":"5.0","7":"938628450"},{"1":"2628","2":"Star Wars: Episode I - The Phantom Menace","3":"1999","4":"Action|Adventure|Sci-Fi","5":"9","6":"3.0","7":"938628843"},{"1":"2762","2":"Sixth Sense, The","3":"1999","4":"Drama|Horror|Mystery","5":"9","6":"4.0","7":"938627748"},{"1":"2857","2":"Yellow Submarine","3":"1968","4":"Adventure|Animation|Comedy|Fantasy|Musical","5":"9","6":"4.0","7":"938629681"},{"1":"50","2":"Usual Suspects, The","3":"1995","4":"Crime|Mystery|Thriller","5":"10","6":"5.0","7":"942766420"},{"1":"152","2":"Addiction, The","3":"1995","4":"Drama|Horror","5":"10","6":"4.0","7":"942766793"},{"1":"318","2":"Shawshank Redemption, The","3":"1994","4":"Crime|Drama","5":"10","6":"4.0","7":"942766515"},{"1":"344","2":"Ace Ventura: Pet Detective","3":"1994","4":"Comedy","5":"10","6":"3.0","7":"942766603"},{"1":"345","2":"Adventures of Priscilla, Queen of the Desert, The","3":"1994","4":"Comedy|Drama","5":"10","6":"4.0","7":"942766603"},{"1":"592","2":"Batman","3":"1989","4":"Action|Crime|Thriller","5":"10","6":"3.0","7":"942767328"},{"1":"735","2":"Cemetery Man (Dellamorte Dellamore)","3":"1994","4":"Horror","5":"10","6":"4.0","7":"942766974"},{"1":"1036","2":"Die Hard","3":"1988","4":"Action|Crime|Thriller","5":"10","6":"3.0","7":"942767258"},{"1":"1089","2":"Reservoir Dogs","3":"1992","4":"Crime|Mystery|Thriller","5":"10","6":"3.0","7":"942766420"},{"1":"1101","2":"Top Gun","3":"1986","4":"Action|Romance","5":"10","6":"2.0","7":"942767328"},{"1":"1127","2":"Abyss, The","3":"1989","4":"Action|Adventure|Sci-Fi|Thriller","5":"10","6":"4.0","7":"942767328"},{"1":"1196","2":"Star Wars: Episode V - The Empire Strikes Back","3":"1980","4":"Action|Adventure|Sci-Fi","5":"10","6":"4.0","7":"942767258"},{"1":"1197","2":"Princess Bride, The","3":"1987","4":"Action|Adventure|Comedy|Fantasy|Romance","5":"10","6":"4.0","7":"942767258"},{"1":"1198","2":"Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark)","3":"1981","4":"Action|Adventure","5":"10","6":"4.0","7":"942767258"},{"1":"1200","2":"Aliens","3":"1986","4":"Action|Adventure|Horror|Sci-Fi","5":"10","6":"4.0","7":"942767258"},{"1":"1210","2":"Star Wars: Episode VI - Return of the Jedi","3":"1983","4":"Action|Adventure|Sci-Fi","5":"10","6":"4.0","7":"942767258"},{"1":"1220","2":"Blues Brothers, The","3":"1980","4":"Action|Comedy|Musical","5":"10","6":"3.0","7":"942767328"},{"1":"1240","2":"Terminator, The","3":"1984","4":"Action|Sci-Fi|Thriller","5":"10","6":"4.0","7":"942767258"},{"1":"1291","2":"Indiana Jones and the Last Crusade","3":"1989","4":"Action|Adventure","5":"10","6":"4.0","7":"942767258"},{"1":"1358","2":"Sling Blade","3":"1996","4":"Drama","5":"10","6":"5.0","7":"942766420"},{"1":"1423","2":"Hearts and Minds","3":"1996","4":"Drama","5":"10","6":"4.0","7":"942766420"},{"1":"1459","2":"Absolute Power","3":"1997","4":"Mystery|Thriller","5":"10","6":"3.0","7":"942766725"},{"1":"1499","2":"Anaconda","3":"1997","4":"Action|Adventure|Thriller","5":"10","6":"3.0","7":"942766845"},{"1":"1611","2":"My Own Private Idaho","3":"1991","4":"Drama|Romance","5":"10","6":"5.0","7":"942767029"},{"1":"1690","2":"Alien: Resurrection","3":"1997","4":"Action|Horror|Sci-Fi","5":"10","6":"3.0","7":"942766679"},{"1":"1704","2":"Good Will Hunting","3":"1997","4":"Drama|Romance","5":"10","6":"4.0","7":"942766472"},{"1":"1719","2":"Sweet Hereafter, The","3":"1997","4":"Drama","5":"10","6":"5.0","7":"942766472"},{"1":"1887","2":"Almost Heroes","3":"1998","4":"Adventure|Comedy|Western","5":"10","6":"2.0","7":"942766845"},{"1":"1923","2":"There's Something About Mary","3":"1998","4":"Comedy|Romance","5":"10","6":"5.0","7":"942766515"},{"1":"2108","2":"L.A. Story","3":"1991","4":"Comedy|Romance","5":"10","6":"3.0","7":"942766472"},{"1":"2344","2":"Runaway Train","3":"1985","4":"Action|Adventure|Drama|Thriller","5":"10","6":"5.0","7":"942766991"},{"1":"2406","2":"Romancing the Stone","3":"1984","4":"Action|Adventure|Comedy|Romance","5":"10","6":"4.0","7":"942767328"},{"1":"2410","2":"Rocky III","3":"1982","4":"Action|Drama","5":"10","6":"2.0","7":"942767328"},{"1":"2539","2":"Analyze This","3":"1999","4":"Comedy","5":"10","6":"4.0","7":"942766845"},{"1":"2571","2":"Matrix, The","3":"1999","4":"Action|Sci-Fi|Thriller","5":"10","6":"5.0","7":"942766515"},{"1":"2826","2":"13th Warrior, The","3":"1999","4":"Action|Adventure|Fantasy","5":"10","6":"5.0","7":"942766109"},{"1":"2827","2":"Astronaut's Wife, The","3":"1999","4":"Horror|Sci-Fi|Thriller","5":"10","6":"3.0","7":"942766251"},{"1":"2840","2":"Stigmata","3":"1999","4":"Drama|Thriller","5":"10","6":"3.0","7":"942766213"},{"1":"2841","2":"Stir of Echoes","3":"1999","4":"Horror|Mystery|Thriller","5":"10","6":"4.0","7":"942766029"},{"1":"2881","2":"Double Jeopardy","3":"1999","4":"Action|Crime|Drama|Thriller","5":"10","6":"3.0","7":"942766164"},{"1":"2890","2":"Three Kings","3":"1999","4":"Action|Adventure|Comedy|Drama|War","5":"10","6":"4.0","7":"942765978"},{"1":"2907","2":"Superstar","3":"1999","4":"Comedy","5":"10","6":"2.0","7":"942766213"},{"1":"2926","2":"Hairspray","3":"1988","4":"Comedy|Drama","5":"10","6":"5.0","7":"942767121"},{"1":"2995","2":"House on Haunted Hill","3":"1999","4":"Horror|Thriller","5":"10","6":"2.0","7":"942766251"},{"1":"3005","2":"Bone Collector, The","3":"1999","4":"Thriller","5":"10","6":"3.0","7":"942766059"},{"1":"3019","2":"Drugstore Cowboy","3":"1989","4":"Crime|Drama","5":"10","6":"4.0","7":"942767571"},{"1":"50","2":"Usual Suspects, The","3":"1995","4":"Crime|Mystery|Thriller","5":"11","6":"5.0","7":"1391658537"},{"1":"70","2":"From Dusk Till Dawn","3":"1996","4":"Action|Comedy|Horror|Thriller","5":"11","6":"1.0","7":"1391656827"},{"1":"126","2":"NeverEnding Story III, The","3":"1994","4":"Adventure|Children|Fantasy","5":"11","6":"4.0","7":"1391657561"},{"1":"169","2":"Free Willy 2: The Adventure Home","3":"1995","4":"Adventure|Children|Drama","5":"11","6":"3.0","7":"1391657297"},{"1":"296","2":"Pulp Fiction","3":"1994","4":"Comedy|Crime|Drama|Thriller","5":"11","6":"5.0","7":"1391658423"},{"1":"778","2":"Trainspotting","3":"1996","4":"Comedy|Crime|Drama","5":"11","6":"4.5","7":"1391658505"},{"1":"785","2":"Kingpin","3":"1996","4":"Comedy","5":"11","6":"3.5","7":"1391656845"},{"1":"923","2":"Citizen Kane","3":"1941","4":"Drama|Mystery","5":"11","6":"5.0","7":"1391658556"},{"1":"1027","2":"Robin Hood: Prince of Thieves","3":"1991","4":"Adventure|Drama","5":"11","6":"4.5","7":"1391658634"},{"1":"1201","2":"Good, the Bad and the Ugly, The (Buono, il brutto, il cattivo, Il)","3":"1966","4":"Action|Adventure|Western","5":"11","6":"5.0","7":"1391658440"},{"1":"1408","2":"Last of the Mohicans, The","3":"1992","4":"Action|Romance|War|Western","5":"11","6":"5.0","7":"1391658667"},{"1":"1918","2":"Lethal Weapon 4","3":"1998","4":"Action|Comedy|Crime|Thriller","5":"11","6":"3.0","7":"1391657062"},{"1":"2042","2":"D2: The Mighty Ducks","3":"1994","4":"Children|Comedy","5":"11","6":"3.5","7":"1391657376"},{"1":"2596","2":"SLC Punk!","3":"1998","4":"Comedy|Drama","5":"11","6":"4.5","7":"1391657543"},{"1":"2762","2":"Sixth Sense, The","3":"1999","4":"Drama|Horror|Mystery","5":"11","6":"3.0","7":"1391658583"},{"1":"3424","2":"Do the Right Thing","3":"1989","4":"Drama","5":"11","6":"3.0","7":"1391657112"},{"1":"5669","2":"Bowling for Columbine","3":"2002","4":"Documentary","5":"11","6":"3.0","7":"1391658601"},{"1":"6598","2":"Step Into Liquid","3":"2002","4":"Documentary","5":"11","6":"5.0","7":"1391657861"},{"1":"26614","2":"Bourne Identity, The","3":"1988","4":"Action|Adventure|Drama|Mystery|Thriller","5":"11","6":"5.0","7":"1391658574"},{"1":"48516","2":"Departed, The","3":"2006","4":"Crime|Drama|Thriller","5":"11","6":"5.0","7":"1391658450"},{"1":"51084","2":"Music and Lyrics","3":"2007","4":"Comedy|Romance","5":"11","6":"4.0","7":"1391657605"},{"1":"58295","2":"Bank Job, The","3":"2008","4":"Action|Crime|Thriller","5":"11","6":"4.5","7":"1391657459"},{"1":"71211","2":"Informant!, The","3":"2009","4":"Comedy|Crime|Drama|Thriller","5":"11","6":"3.5","7":"1391657720"},{"1":"77455","2":"Exit Through the Gift Shop","3":"2010","4":"Comedy|Documentary","5":"11","6":"4.5","7":"1391658141"},{"1":"79132","2":"Inception","3":"2010","4":"Action|Crime|Drama|Mystery|Sci-Fi|Thriller|IMAX","5":"11","6":"4.0","7":"1391658115"},{"1":"80489","2":"Town, The","3":"2010","4":"Crime|Drama|Thriller","5":"11","6":"4.5","7":"1391658399"},{"1":"80906","2":"Inside Job","3":"2010","4":"Documentary","5":"11","6":"3.0","7":"1391658137"},{"1":"81158","2":"Restrepo","3":"2010","4":"Documentary|War","5":"11","6":"4.0","7":"1391658210"},{"1":"81562","2":"127 Hours","3":"2010","4":"Adventure|Drama|Thriller","5":"11","6":"3.5","7":"1391658218"},{"1":"88129","2":"Drive","3":"2011","4":"Crime|Drama|Film-Noir|Thriller","5":"11","6":"4.0","7":"1391658385"},{"1":"91500","2":"The Hunger Games","3":"2012","4":"Action|Adventure|Drama|Sci-Fi|Thriller","5":"11","6":"4.5","7":"1391658311"},{"1":"91529","2":"Dark Knight Rises, The","3":"2012","4":"Action|Adventure|Crime|IMAX","5":"11","6":"4.5","7":"1391658125"},{"1":"91548","2":"Life in a Day","3":"2011","4":"Documentary|Drama","5":"11","6":"4.0","7":"1391658270"},{"1":"96079","2":"Skyfall","3":"2012","4":"Action|Adventure|Thriller|IMAX","5":"11","6":"4.0","7":"1391658186"},{"1":"96861","2":"Taken 2","3":"2012","4":"Action|Crime|Drama|Thriller","5":"11","6":"4.0","7":"1391657924"},{"1":"97938","2":"Life of Pi","3":"2012","4":"Adventure|Drama|IMAX","5":"11","6":"4.0","7":"1391657610"},{"1":"104841","2":"Gravity","3":"2013","4":"Action|Sci-Fi|IMAX","5":"11","6":"5.0","7":"1391658157"},{"1":"106487","2":"The Hunger Games: Catching Fire","3":"2013","4":"Action|Adventure|Sci-Fi|IMAX","5":"11","6":"5.0","7":"1391657798"},{"1":"253","2":"Interview with the Vampire: The Vampire Chronicles","3":"1994","4":"Drama|Horror","5":"12","6":"3.0","7":"968045587"},{"1":"529","2":"Searching for Bobby Fischer","3":"1993","4":"Drama","5":"12","6":"1.0","7":"968045407"},{"1":"538","2":"Six Degrees of Separation","3":"1993","4":"Drama","5":"12","6":"3.0","7":"968045438"},{"1":"608","2":"Fargo","3":"1996","4":"Comedy|Crime|Drama|Thriller","5":"12","6":"2.0","7":"968045353"},{"1":"673","2":"Space Jam","3":"1996","4":"Adventure|Animation|Children|Comedy|Fantasy|Sci-Fi","5":"12","6":"1.0","7":"968045732"},{"1":"736","2":"Twister","3":"1996","4":"Action|Adventure|Romance|Thriller","5":"12","6":"4.0","7":"968045680"},{"1":"737","2":"Barb Wire","3":"1996","4":"Action|Sci-Fi","5":"12","6":"3.0","7":"968045764"},{"1":"1028","2":"Mary Poppins","3":"1964","4":"Children|Comedy|Fantasy|Musical","5":"12","6":"1.0","7":"968045500"},{"1":"1032","2":"Alice in Wonderland","3":"1951","4":"Adventure|Animation|Children|Fantasy|Musical","5":"12","6":"2.0","7":"968045561"},{"1":"1077","2":"Sleeper","3":"1973","4":"Comedy|Sci-Fi","5":"12","6":"3.0","7":"968045500"},{"1":"1197","2":"Princess Bride, The","3":"1987","4":"Action|Adventure|Comedy|Fantasy|Romance","5":"12","6":"1.0","7":"955407153"},{"1":"1215","2":"Army of Darkness","3":"1993","4":"Action|Adventure|Comedy|Fantasy|Horror","5":"12","6":"5.0","7":"968045353"},{"1":"1220","2":"Blues Brothers, The","3":"1980","4":"Action|Comedy|Musical","5":"12","6":"5.0","7":"955407153"},{"1":"1230","2":"Annie Hall","3":"1977","4":"Comedy|Romance","5":"12","6":"2.0","7":"968045354"},{"1":"1235","2":"Harold and Maude","3":"1971","4":"Comedy|Drama|Romance","5":"12","6":"5.0","7":"968045354"},{"1":"1295","2":"Unbearable Lightness of Being, The","3":"1988","4":"Drama","5":"12","6":"1.0","7":"955407153"},{"1":"1374","2":"Star Trek II: The Wrath of Khan","3":"1982","4":"Action|Adventure|Sci-Fi|Thriller","5":"12","6":"1.0","7":"968045475"},{"1":"1387","2":"Jaws","3":"1975","4":"Action|Horror","5":"12","6":"4.0","7":"968045475"},{"1":"1639","2":"Chasing Amy","3":"1997","4":"Comedy|Drama|Romance","5":"12","6":"2.0","7":"968045438"},{"1":"1732","2":"Big Lebowski, The","3":"1998","4":"Comedy|Crime","5":"12","6":"3.0","7":"968045529"},{"1":"2259","2":"Blame It on Rio","3":"1984","4":"Comedy|Romance","5":"12","6":"2.0","7":"955407153"},{"1":"2355","2":"Bug's Life, A","3":"1998","4":"Adventure|Animation|Children|Comedy","5":"12","6":"2.0","7":"968045475"},{"1":"2460","2":"Texas Chainsaw Massacre 2, The","3":"1986","4":"Horror","5":"12","6":"4.0","7":"968045680"},{"1":"2529","2":"Planet of the Apes","3":"1968","4":"Action|Drama|Sci-Fi","5":"12","6":"1.0","7":"968045475"},{"1":"2668","2":"Swamp Thing","3":"1982","4":"Horror|Sci-Fi","5":"12","6":"2.0","7":"968045764"},{"1":"2959","2":"Fight Club","3":"1999","4":"Action|Crime|Drama|Thriller","5":"12","6":"4.0","7":"968045407"},{"1":"3146","2":"Deuce Bigalow: Male Gigolo","3":"1999","4":"Comedy","5":"12","6":"2.0","7":"968045732"},{"1":"3148","2":"Cider House Rules, The","3":"1999","4":"Drama","5":"12","6":"1.0","7":"968045379"},{"1":"3176","2":"Talented Mr. Ripley, The","3":"1999","4":"Drama|Mystery|Thriller","5":"12","6":"3.0","7":"968045649"},{"1":"3179","2":"Angela's Ashes","3":"1999","4":"Drama","5":"12","6":"2.0","7":"968045354"},{"1":"3298","2":"Boiler Room","3":"2000","4":"Crime|Drama|Thriller","5":"12","6":"4.0","7":"968045529"},{"1":"3324","2":"Drowning Mona","3":"2000","4":"Comedy","5":"12","6":"1.0","7":"968045732"},{"1":"3408","2":"Erin Brockovich","3":"2000","4":"Drama","5":"12","6":"4.0","7":"968045379"},{"1":"3474","2":"Retroactive","3":"1997","4":"Sci-Fi|Thriller","5":"12","6":"4.0","7":"955407153"},{"1":"3770","2":"Dreamscape","3":"1984","4":"Horror|Sci-Fi|Thriller","5":"12","6":"3.0","7":"968045619"},{"1":"3773","2":"House Party","3":"1990","4":"Comedy","5":"12","6":"2.0","7":"968045680"},{"1":"3780","2":"Rocketship X-M","3":"1950","4":"Sci-Fi","5":"12","6":"2.0","7":"968045619"},{"1":"3791","2":"Footloose","3":"1984","4":"Drama","5":"12","6":"1.0","7":"968045232"},{"1":"3793","2":"X-Men","3":"2000","4":"Action|Adventure|Sci-Fi","5":"12","6":"5.0","7":"968045232"},{"1":"3794","2":"Chuck & Buck","3":"2000","4":"Comedy|Drama","5":"12","6":"3.0","7":"968045232"},{"1":"3798","2":"What Lies Beneath","3":"2000","4":"Drama|Horror|Mystery","5":"12","6":"4.0","7":"968045232"},{"1":"3799","2":"Pokémon the Movie 2000","3":"2000","4":"Animation|Children","5":"12","6":"1.0","7":"968045198"},{"1":"3801","2":"Anatomy of a Murder","3":"1959","4":"Drama|Mystery","5":"12","6":"3.0","7":"968045198"},{"1":"3809","2":"What About Bob?","3":"1991","4":"Comedy","5":"12","6":"3.0","7":"968045172"},{"1":"3825","2":"Coyote Ugly","3":"2000","4":"Comedy|Drama|Romance","5":"12","6":"5.0","7":"968045142"},{"1":"3827","2":"Space Cowboys","3":"2000","4":"Action|Adventure|Comedy|Sci-Fi","5":"12","6":"2.0","7":"968045142"},{"1":"3829","2":"Mad About Mambo","3":"2000","4":"Comedy|Romance","5":"12","6":"2.0","7":"968045106"},{"1":"3831","2":"Saving Grace","3":"2000","4":"Comedy","5":"12","6":"4.0","7":"968045106"},{"1":"3841","2":"Air America","3":"1990","4":"Action|Comedy","5":"12","6":"2.0","7":"968045074"},{"1":"3844","2":"Steel Magnolias","3":"1989","4":"Drama","5":"12","6":"1.0","7":"968045074"},{"1":"3861","2":"Replacements, The","3":"2000","4":"Comedy","5":"12","6":"3.0","7":"968045015"},{"1":"3863","2":"Cell, The","3":"2000","4":"Drama|Horror|Thriller","5":"12","6":"5.0","7":"968045015"},{"1":"3864","2":"Godzilla 2000 (Gojira ni-sen mireniamu)","3":"1999","4":"Action|Adventure|Sci-Fi","5":"12","6":"3.0","7":"968045015"},{"1":"3865","2":"Original Kings of Comedy, The","3":"2000","4":"Comedy|Documentary","5":"12","6":"5.0","7":"968045015"},{"1":"3869","2":"Naked Gun 2 1/2: The Smell of Fear, The","3":"1991","4":"Comedy","5":"12","6":"2.0","7":"968044981"},{"1":"3871","2":"Shane","3":"1953","4":"Drama|Western","5":"12","6":"2.0","7":"968044981"},{"1":"3873","2":"Cat Ballou","3":"1965","4":"Comedy|Western","5":"12","6":"3.0","7":"968044981"},{"1":"3879","2":"Art of War, The","3":"2000","4":"Action|Thriller","5":"12","6":"5.0","7":"968044949"},{"1":"3885","2":"Love & Sex","3":"2000","4":"Comedy|Drama|Romance","5":"12","6":"3.0","7":"968044949"},{"1":"3886","2":"Steal This Movie!","3":"2000","4":"Drama","5":"12","6":"2.0","7":"968044949"},{"1":"6184","2":"Man Who Fell to Earth, The","3":"1976","4":"Drama|Sci-Fi","5":"12","6":"4.0","7":"968045173"},{"1":"1","2":"Toy Story","3":"1995","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"13","6":"5.0","7":"1331380058"},{"1":"47","2":"Seven (a.k.a. Se7en)","3":"1995","4":"Mystery|Thriller","5":"13","6":"2.5","7":"1331380914"},{"1":"110","2":"Braveheart","3":"1995","4":"Action|Drama|War","5":"13","6":"4.0","7":"1331380038"},{"1":"277","2":"Miracle on 34th Street","3":"1994","4":"Drama","5":"13","6":"4.0","7":"1331379348"},{"1":"296","2":"Pulp Fiction","3":"1994","4":"Comedy|Crime|Drama|Thriller","5":"13","6":"3.5","7":"1331380895"},{"1":"318","2":"Shawshank Redemption, The","3":"1994","4":"Crime|Drama","5":"13","6":"4.5","7":"1331380029"},{"1":"356","2":"Forrest Gump","3":"1994","4":"Comedy|Drama|Romance|War","5":"13","6":"5.0","7":"1331380018"},{"1":"362","2":"Jungle Book, The","3":"1994","4":"Adventure|Children|Romance","5":"13","6":"4.5","7":"1331379479"},{"1":"480","2":"Jurassic Park","3":"1993","4":"Action|Adventure|Sci-Fi|Thriller","5":"13","6":"3.0","7":"1331380025"},{"1":"524","2":"Rudy","3":"1993","4":"Drama","5":"13","6":"3.5","7":"1331379485"},{"1":"527","2":"Schindler's List","3":"1993","4":"Drama|War","5":"13","6":"4.0","7":"1331380901"},{"1":"531","2":"Secret Garden, The","3":"1993","4":"Children|Drama","5":"13","6":"4.0","7":"1331379483"},{"1":"587","2":"Ghost","3":"1990","4":"Comedy|Drama|Fantasy|Romance|Thriller","5":"13","6":"3.0","7":"1331380851"},{"1":"590","2":"Dances with Wolves","3":"1990","4":"Adventure|Drama|Western","5":"13","6":"4.0","7":"1331380062"},{"1":"914","2":"My Fair Lady","3":"1964","4":"Comedy|Drama|Musical|Romance","5":"13","6":"4.0","7":"1331379366"},{"1":"919","2":"Wizard of Oz, The","3":"1939","4":"Adventure|Children|Fantasy|Musical","5":"13","6":"3.5","7":"1331380871"},{"1":"1027","2":"Robin Hood: Prince of Thieves","3":"1991","4":"Adventure|Drama","5":"13","6":"3.0","7":"1331379520"},{"1":"1259","2":"Stand by Me","3":"1986","4":"Adventure|Drama","5":"13","6":"4.0","7":"1331380814"},{"1":"1265","2":"Groundhog Day","3":"1993","4":"Comedy|Fantasy|Romance","5":"13","6":"2.5","7":"1331380845"},{"1":"1918","2":"Lethal Weapon 4","3":"1998","4":"Action|Comedy|Crime|Thriller","5":"13","6":"3.0","7":"1331379386"},{"1":"1961","2":"Rain Man","3":"1988","4":"Drama","5":"13","6":"4.0","7":"1331380745"},{"1":"2355","2":"Bug's Life, A","3":"1998","4":"Adventure|Animation|Children|Comedy","5":"13","6":"4.0","7":"1331380741"},{"1":"2571","2":"Matrix, The","3":"1999","4":"Action|Sci-Fi|Thriller","5":"13","6":"3.0","7":"1331380888"},{"1":"2572","2":"10 Things I Hate About You","3":"1999","4":"Comedy|Romance","5":"13","6":"3.5","7":"1331379777"},{"1":"2761","2":"Iron Giant, The","3":"1999","4":"Adventure|Animation|Children|Drama|Sci-Fi","5":"13","6":"4.0","7":"1331379470"},{"1":"2762","2":"Sixth Sense, The","3":"1999","4":"Drama|Horror|Mystery","5":"13","6":"3.0","7":"1331380765"},{"1":"2804","2":"Christmas Story, A","3":"1983","4":"Children|Comedy","5":"13","6":"3.0","7":"1331380731"},{"1":"2908","2":"Boys Don't Cry","3":"1999","4":"Drama","5":"13","6":"3.0","7":"1331379494"},{"1":"2918","2":"Ferris Bueller's Day Off","3":"1986","4":"Comedy","5":"13","6":"3.0","7":"1331380761"},{"1":"3114","2":"Toy Story 2","3":"1999","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"13","6":"3.0","7":"1331380738"},{"1":"3147","2":"Green Mile, The","3":"1999","4":"Crime|Drama","5":"13","6":"4.0","7":"1331380752"},{"1":"3255","2":"League of Their Own, A","3":"1992","4":"Comedy|Drama","5":"13","6":"3.5","7":"1331379342"},{"1":"3396","2":"Muppet Movie, The","3":"1979","4":"Adventure|Children|Comedy|Musical","5":"13","6":"3.5","7":"1331379599"},{"1":"3624","2":"Shanghai Noon","3":"2000","4":"Action|Adventure|Comedy|Western","5":"13","6":"3.0","7":"1331379574"},{"1":"4306","2":"Shrek","3":"2001","4":"Adventure|Animation|Children|Comedy|Fantasy|Romance","5":"13","6":"4.0","7":"1331380721"},{"1":"4310","2":"Pearl Harbor","3":"2001","4":"Action|Drama|Romance|War","5":"13","6":"4.5","7":"1331379506"},{"1":"4321","2":"City Slickers","3":"1991","4":"Comedy|Western","5":"13","6":"2.5","7":"1331379589"},{"1":"4718","2":"American Pie 2","3":"2001","4":"Comedy","5":"13","6":"3.5","7":"1331379530"},{"1":"4878","2":"Donnie Darko","3":"2001","4":"Drama|Mystery|Sci-Fi|Thriller","5":"13","6":"4.5","7":"1331380145"},{"1":"4886","2":"Monsters, Inc.","3":"2001","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"13","6":"3.5","7":"1331380834"},{"1":"4993","2":"Lord of the Rings: The Fellowship of the Ring, The","3":"2001","4":"Adventure|Fantasy","5":"13","6":"4.5","7":"1331380883"},{"1":"5989","2":"Catch Me If You Can","3":"2002","4":"Crime|Drama","5":"13","6":"4.0","7":"1331380785"},{"1":"6377","2":"Finding Nemo","3":"2003","4":"Adventure|Animation|Children|Comedy","5":"13","6":"4.5","7":"1331380734"},{"1":"7361","2":"Eternal Sunshine of the Spotless Mind","3":"2004","4":"Drama|Romance|Sci-Fi","5":"13","6":"4.0","7":"1331380159"},{"1":"7502","2":"Band of Brothers","3":"2001","4":"Action|Drama|War","5":"13","6":"4.5","7":"1331380306"},{"1":"54286","2":"Bourne Ultimatum, The","3":"2007","4":"Action|Crime|Thriller","5":"13","6":"3.5","7":"1331380782"},{"1":"58559","2":"Dark Knight, The","3":"2008","4":"Action|Crime|Drama|IMAX","5":"13","6":"4.5","7":"1331380873"},{"1":"64614","2":"Gran Torino","3":"2008","4":"Crime|Drama","5":"13","6":"4.5","7":"1331380830"},{"1":"69757","2":"(500) Days of Summer","3":"2009","4":"Comedy|Drama|Romance","5":"13","6":"4.0","7":"1331379748"},{"1":"78499","2":"Toy Story 3","3":"2010","4":"Adventure|Animation|Children|Comedy|Fantasy|IMAX","5":"13","6":"4.0","7":"1331380725"},{"1":"81834","2":"Harry Potter and the Deathly Hallows: Part 1","3":"2010","4":"Action|Adventure|Fantasy|IMAX","5":"13","6":"4.5","7":"1331380387"},{"1":"88125","2":"Harry Potter and the Deathly Hallows: Part 2","3":"2011","4":"Action|Adventure|Drama|Fantasy|Mystery|IMAX","5":"13","6":"4.5","7":"1331380390"},{"1":"93363","2":"John Carter","3":"2012","4":"Action|Adventure|Sci-Fi|IMAX","5":"13","6":"3.0","7":"1331379985"},{"1":"594","2":"Snow White and the Seven Dwarfs","3":"1937","4":"Animation|Children|Drama|Fantasy|Musical","5":"14","6":"1.0","7":"976243997"},{"1":"1196","2":"Star Wars: Episode V - The Empire Strikes Back","3":"1980","4":"Action|Adventure|Sci-Fi","5":"14","6":"4.0","7":"976243997"},{"1":"1721","2":"Titanic","3":"1997","4":"Drama|Romance","5":"14","6":"3.0","7":"976243912"},{"1":"2038","2":"Cat from Outer Space, The","3":"1978","4":"Children|Comedy|Sci-Fi","5":"14","6":"3.0","7":"976244179"},{"1":"2355","2":"Bug's Life, A","3":"1998","4":"Adventure|Animation|Children|Comedy","5":"14","6":"2.0","7":"976244179"},{"1":"2394","2":"Prince of Egypt, The","3":"1998","4":"Animation|Musical","5":"14","6":"3.0","7":"976244471"},{"1":"2628","2":"Star Wars: Episode I - The Phantom Menace","3":"1999","4":"Action|Adventure|Sci-Fi","5":"14","6":"3.0","7":"976243933"},{"1":"2683","2":"Austin Powers: The Spy Who Shagged Me","3":"1999","4":"Action|Adventure|Comedy","5":"14","6":"2.0","7":"976244131"},{"1":"2716","2":"Ghostbusters (a.k.a. Ghost Busters)","3":"1984","4":"Action|Comedy|Sci-Fi","5":"14","6":"3.0","7":"976244313"},{"1":"2720","2":"Inspector Gadget","3":"1999","4":"Action|Adventure|Children|Comedy","5":"14","6":"2.0","7":"976244367"},{"1":"2724","2":"Runaway Bride","3":"1999","4":"Comedy|Romance","5":"14","6":"3.0","7":"976244524"},{"1":"2861","2":"For Love of the Game","3":"1999","4":"Comedy|Drama","5":"14","6":"2.0","7":"976244287"},{"1":"3114","2":"Toy Story 2","3":"1999","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"14","6":"4.0","7":"976244591"},{"1":"3157","2":"Stuart Little","3":"1999","4":"Children|Comedy|Fantasy","5":"14","6":"2.0","7":"976244564"},{"1":"3175","2":"Galaxy Quest","3":"1999","4":"Adventure|Comedy|Sci-Fi","5":"14","6":"5.0","7":"976244313"},{"1":"3354","2":"Mission to Mars","3":"2000","4":"Sci-Fi","5":"14","6":"3.0","7":"976243997"},{"1":"3623","2":"Mission: Impossible II","3":"2000","4":"Action|Adventure|Thriller","5":"14","6":"3.0","7":"976244423"},{"1":"3751","2":"Chicken Run","3":"2000","4":"Animation|Children|Comedy","5":"14","6":"4.0","7":"976244205"},{"1":"3986","2":"6th Day, The","3":"2000","4":"Action|Sci-Fi|Thriller","5":"14","6":"3.0","7":"976244107"},{"1":"3988","2":"How the Grinch Stole Christmas (a.k.a. The Grinch)","3":"2000","4":"Children|Comedy|Fantasy","5":"14","6":"4.0","7":"976244343"},{"1":"1","2":"Toy Story","3":"1995","4":"Adventure|Animation|Children|Comedy|Fantasy","5":"15","6":"2.0","7":"997938310"},{"1":"2","2":"Jumanji","3":"1995","4":"Adventure|Children|Fantasy","5":"15","6":"2.0","7":"1134521380"},{"1":"5","2":"Father of the Bride Part II","3":"1995","4":"Comedy","5":"15","6":"4.5","7":"1093070098"},{"1":"6","2":"Heat","3":"1995","4":"Action|Crime|Thriller","5":"15","6":"4.0","7":"1040205753"},{"1":"10","2":"GoldenEye","3":"1995","4":"Action|Adventure|Thriller","5":"15","6":"3.0","7":"1093028290"},{"1":"11","2":"American President, The","3":"1995","4":"Comedy|Drama|Romance","5":"15","6":"2.5","7":"1093028381"},{"1":"14","2":"Nixon","3":"1995","4":"Drama","5":"15","6":"2.5","7":"1166586286"},{"1":"16","2":"Casino","3":"1995","4":"Crime|Drama","5":"15","6":"3.5","7":"1093070150"},{"1":"17","2":"Sense and Sensibility","3":"1995","4":"Drama|Romance","5":"15","6":"3.0","7":"997939404"},{"1":"19","2":"Ace Ventura: When Nature Calls","3":"1995","4":"Comedy","5":"15","6":"1.0","7":"1093028409"},{"1":"21","2":"Get Shorty","3":"1995","4":"Comedy|Crime|Thriller","5":"15","6":"4.5","7":"1058249528"},{"1":"22","2":"Copycat","3":"1995","4":"Crime|Drama|Horror|Mystery|Thriller","5":"15","6":"2.5","7":"1093070415"},{"1":"25","2":"Leaving Las Vegas","3":"1995","4":"Drama|Romance","5":"15","6":"3.0","7":"1033345172"},{"1":"32","2":"Twelve Monkeys (a.k.a. 12 Monkeys)","3":"1995","4":"Mystery|Sci-Fi|Thriller","5":"15","6":"4.0","7":"997938466"},{"1":"34","2":"Babe","3":"1995","4":"Children|Drama","5":"15","6":"3.0","7":"997938310"},{"1":"36","2":"Dead Man Walking","3":"1995","4":"Crime|Drama","5":"15","6":"1.0","7":"1033345287"},{"1":"39","2":"Clueless","3":"1995","4":"Comedy|Romance","5":"15","6":"2.5","7":"1093028334"},{"1":"44","2":"Mortal Kombat","3":"1995","4":"Action|Adventure|Fantasy","5":"15","6":"3.0","7":"1122576665"},{"1":"47","2":"Seven (a.k.a. Se7en)","3":"1995","4":"Mystery|Thriller","5":"15","6":"5.0","7":"1054449816"},{"1":"50","2":"Usual Suspects, The","3":"1995","4":"Crime|Mystery|Thriller","5":"15","6":"5.0","7":"997938500"},{"1":"52","2":"Mighty Aphrodite","3":"1995","4":"Comedy|Drama|Romance","5":"15","6":"2.5","7":"1093070271"},{"1":"62","2":"Mr. Holland's Opus","3":"1995","4":"Drama","5":"15","6":"2.0","7":"1093028336"},{"1":"70","2":"From Dusk Till Dawn","3":"1996","4":"Action|Comedy|Horror|Thriller","5":"15","6":"0.5","7":"1093070467"},{"1":"82","2":"Antonia's Line (Antonia)","3":"1995","4":"Comedy|Drama","5":"15","6":"5.0","7":"1044220302"},{"1":"94","2":"Beautiful Girls","3":"1996","4":"Comedy|Drama|Romance","5":"15","6":"3.0","7":"1166587063"},{"1":"95","2":"Broken Arrow","3":"1996","4":"Action|Adventure|Thriller","5":"15","6":"1.5","7":"1093028331"},{"1":"101","2":"Bottle Rocket","3":"1996","4":"Adventure|Comedy|Crime|Romance","5":"15","6":"4.0","7":"1134522072"},{"1":"104","2":"Happy Gilmore","3":"1996","4":"Comedy","5":"15","6":"1.0","7":"1093070113"},{"1":"107","2":"Muppet Treasure Island","3":"1996","4":"Adventure|Children|Comedy|Musical","5":"15","6":"2.0","7":"1166586594"},{"1":"110","2":"Braveheart","3":"1995","4":"Action|Drama|War","5":"15","6":"3.0","7":"1040205792"},{"1":"111","2":"Taxi Driver","3":"1976","4":"Crime|Drama|Thriller","5":"15","6":"5.0","7":"997938500"},{"1":"112","2":"Rumble in the Bronx (Hont faan kui)","3":"1995","4":"Action|Adventure|Comedy|Crime","5":"15","6":"2.5","7":"1093070156"},{"1":"123","2":"Chungking Express (Chung Hing sam lam)","3":"1994","4":"Drama|Mystery|Romance","5":"15","6":"4.0","7":"997938358"},{"1":"125","2":"Flirting With Disaster","3":"1996","4":"Comedy","5":"15","6":"3.5","7":"1245362506"},{"1":"145","2":"Bad Boys","3":"1995","4":"Action|Comedy|Crime|Drama|Thriller","5":"15","6":"3.5","7":"1134521543"},{"1":"149","2":"Amateur","3":"1994","4":"Crime|Drama|Thriller","5":"15","6":"5.0","7":"1075142933"},{"1":"150","2":"Apollo 13","3":"1995","4":"Adventure|Drama|IMAX","5":"15","6":"3.0","7":"997939380"},{"1":"153","2":"Batman Forever","3":"1995","4":"Action|Adventure|Comedy|Crime","5":"15","6":"1.0","7":"1128274517"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Each row represents a rating given by one user to one movie.</p>
<p>We can see the number of unique users that provided ratings and how many unique movies were rated:</p>
<pre class="r"><code>movielens %&gt;% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["n_users"],"name":[1],"type":["int"],"align":["right"]},{"label":["n_movies"],"name":[2],"type":["int"],"align":["right"]}],"data":[{"1":"671","2":"9066"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>If we multiply those two numbers, we get a number larger than 5 million, yet our data table has about 100,000 rows. This implies that not every user rated every movie. So we can think of these data as a very large matrix, with users on the rows and movies on the columns, with many empty cells. The <code>gather</code> function permits us to convert it to this format, but if we try it for the entire matrix, it will crash R. Let’s show the matrix for seven users and four movies.</p>
<p>You can think of the task of a recommendation system as filling in the <code>NA</code>s in the table above. To see how <em>sparse</em> the matrix is, here is the matrix for a random sample of 100 movies and 100 users with yellow indicating a user/movie combination for which we have a rating.</p>
<p><img src="datsci_08_files/figure-html/sparsity-of-movie-recs-1.png" width="40%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>This machine learning challenge is more complicated than what we have studied up to now because each outcome <span class="math inline">\(Y\)</span> has a different set of predictors. To see this, note that if we are predicting the rating for movie <span class="math inline">\(i\)</span> by user <span class="math inline">\(u\)</span>, in principle, all other ratings related to movie <span class="math inline">\(i\)</span> and by user <span class="math inline">\(u\)</span> may be used as predictors, but different users rate different movies and a different number of movies. Furthermore, we may be able to use information from other movies that we have determined are similar to movie <span class="math inline">\(i\)</span> or from users determined to be similar to user <span class="math inline">\(u\)</span>. In essence, the entire matrix can be used as predictors for each cell.</p>
<p>Let’s look at some of the general properties of the data to better understand the challenges.</p>
<p>The first thing we notice is that some movies get rated more than others. Below is the distribution. This should not surprise us given that there are blockbuster movies watched by millions and artsy, independent movies watched by just a few. Our second observation is that some users are more active than others at rating movies:</p>
</div>
<div id="section-recommendation-systems-as-a-machine-learning-challenge" class="section level3">
<h3>Recommendation systems as a machine learning challenge</h3>
<p>To see how this is a type of machine learning, notice that we need to build an algorithm with data we have collected that will then be applied outside our control, as users look for movie recommendations. So let’s create a test set to assess the accuracy of the models we implement.</p>
<pre class="r"><code>library(caret)
set.seed(755)
test_index &lt;- createDataPartition(y = movielens$rating, times = 1, p = 0.2, 
                                  list = FALSE)
train_set &lt;- movielens[-test_index,]
test_set &lt;- movielens[test_index,]</code></pre>
<p>To make sure we don’t include users and movies in the test set that do not appear in the training set, we remove these entries using the <code>semi_join</code> function:</p>
<pre class="r"><code>test_set &lt;- test_set %&gt;% 
  semi_join(train_set, by = &quot;movieId&quot;) %&gt;%
  semi_join(train_set, by = &quot;userId&quot;)</code></pre>
</div>
<div id="section-loss-function" class="section level3">
<h3>Loss function</h3>
<p>The Netflix challenge used the typical error loss: they decided on a winner based on the residual mean squared error (RMSE) on a test set. We define <span class="math inline">\(y_{u,i}\)</span> as the rating for movie <span class="math inline">\(i\)</span> by user <span class="math inline">\(u\)</span> and denote our prediction with <span class="math inline">\(\hat{y}_{u,i}\)</span>. The RMSE is then defined as:</p>
<p><span class="math display">\[
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
\]</span> with <span class="math inline">\(N\)</span> being the number of user/movie combinations and the sum occurring over all these combinations.</p>
<p>Remember that we can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating. If this number is larger than 1, it means our typical error is larger than one star, which is not good.</p>
<p>Let’s write a function that computes the RMSE for vectors of ratings and their corresponding predictors:</p>
<pre class="r"><code>RMSE &lt;- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p><strong>Recommendation systems</strong> are more complicated machine learning challenges because each outcome has a different set of predictors. For example, different users rate a different number of movies and rate different movies.</p></li>
<li><p>To compare different models or to see how well we’re doing compared to a baseline, we will use <strong>root mean squared error (RMSE) as our loss function</strong>. We can interpret RMSE similar to standard deviation.</p></li>
<li><p>If <span class="math inline">\(N\)</span> is the number of user-movie combinations, <span class="math inline">\(y_{u,i}\)</span> is the rating for movie <span class="math inline">\(i\)</span> by user <span class="math inline">\(u\)</span>, and <span class="math inline">\(\hat{y}_{u,i}\)</span> is our prediction, then <strong>RMSE is defined as follows</strong>:</p></li>
</ul>
<p><span class="math display">\[
\sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
\]</span></p>
<p><strong>Netflix Challenge links:</strong></p>
<p>For more information about the “Netflix Challenge,” you can check out these sites:</p>
<ul>
<li><p><a href="https://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/">Link</a></p></li>
<li><p><a href="http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/">Link</a></p></li>
<li><p><a href="https://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf">Link</a></p></li>
</ul>
</div>
</div>
<div id="section-building-the-recommendation-system" class="section level3">
<h3>Building the Recommendation System</h3>
<div class="infobox">
<p><strong>Textbook link</strong> <strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#a-first-model">textbook section - 33.7.4 A first model</a>, <a href="https://rafalab.github.io/dsbook/large-datasets.html#modeling-movie-effects">textbook section - 33.7.5 Modeling movie effects</a>, and <a href="https://rafalab.github.io/dsbook/large-datasets.html#user-effects">textbook section - 33.7.6 User effects</a>.</p>
</div>
<p>Let’s start by building the simplest possible recommendation system: we predict the same rating for all movies regardless of user. What number should this prediction be? We can use a model based approach to answer this. A model that assumes the same rating for all movies and users with all the differences explained by random variation would look like this:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + \varepsilon_{u,i}
\]</span></p>
<p>with <span class="math inline">\(\varepsilon_{i,u}\)</span> independent errors sampled from the same distribution centered at 0 and <span class="math inline">\(\mu\)</span> the “true” rating for all movies. We know that the estimate that minimizes the RMSE is the least squares estimate of <span class="math inline">\(\mu\)</span> and, in this case, is the average of all ratings:</p>
<pre class="r"><code>mu_hat &lt;- mean(train_set$rating)
mu_hat</code></pre>
<pre><code>## [1] 3.543305</code></pre>
<p>If we predict all unknown ratings with <span class="math inline">\(\hat{\mu}\)</span> we obtain the following RMSE:</p>
<pre class="r"><code>naive_rmse &lt;- RMSE(test_set$rating, mu_hat)
naive_rmse</code></pre>
<pre><code>## [1] 1.052805</code></pre>
<p>Keep in mind that if you plug in any other number, you get a higher RMSE. For example:</p>
<pre class="r"><code>predictions &lt;- rep(3, nrow(test_set))
RMSE(test_set$rating, predictions)</code></pre>
<pre><code>## [1] 1.191732</code></pre>
<p>From looking at the distribution of ratings, we can visualize that this is the standard deviation of that distribution. We get a RMSE of about 1. To win the grand prize of $1,000,000, a participating team had to get an RMSE of about 0.857. So we can definitely do better!</p>
<p>As we go along, we will be comparing different approaches. Let’s start by creating a results table with this naive approach:</p>
<pre class="r"><code>rmse_results &lt;- tibble(method = &quot;Just the average&quot;, RMSE = naive_rmse)</code></pre>
</div>
<div id="section-modeling-movie-effects" class="section level3">
<h3>Modeling movie effects</h3>
<p>We know from experience that some movies are just generally rated higher than others. This intuition, that different movies are rated differently, is confirmed by data. We can augment our previous model by adding the term <span class="math inline">\(b_i\)</span> to represent average ranking for movie <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
\]</span></p>
<p>Statistics textbooks refer to the <span class="math inline">\(b\)</span>s as effects. However, in the Netflix challenge papers, they refer to them as “bias”, thus the <span class="math inline">\(b\)</span> notation.</p>
<p>We can again use least squares to estimate the <span class="math inline">\(b_i\)</span> in the following way:</p>
<pre class="r"><code>fit &lt;- lm(rating ~ as.factor(movieId), data = movielens)</code></pre>
<p>Because there are thousands of <span class="math inline">\(b_i\)</span> as each movie gets one, the <code>lm()</code> function will be very slow here. We therefore don’t recommend running the code above. But in this particular situation, we know that the least squares estimate <span class="math inline">\(\hat{b}_i\)</span> is just the average of <span class="math inline">\(Y_{u,i} - \hat{\mu}\)</span> for each movie <span class="math inline">\(i\)</span>. So we can compute them this way (we will drop the <code>hat</code> notation in the code to represent estimates going forward):</p>
<pre class="r"><code>mu &lt;- mean(train_set$rating) 
movie_avgs &lt;- train_set %&gt;% 
  group_by(movieId) %&gt;% 
  summarize(b_i = mean(rating - mu))</code></pre>
<p>We can see that these estimates vary substantially:</p>
<pre class="r"><code>qplot(b_i, data = movie_avgs, bins = 10, color = I(&quot;black&quot;))</code></pre>
<p><img src="datsci_08_files/figure-html/movie-effects-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Remember <span class="math inline">\(\hat{\mu}=3.5\)</span> so a <span class="math inline">\(b_i = 1.5\)</span> implies a perfect five star rating.</p>
<p>Let’s see how much our prediction improves once we use <span class="math inline">\(\hat{y}_{u,i} = \hat{\mu} + \hat{b}_i\)</span>:</p>
<pre class="r"><code>predicted_ratings &lt;- mu + test_set %&gt;% 
  left_join(movie_avgs, by=&#39;movieId&#39;) %&gt;%
  pull(b_i)
RMSE(predicted_ratings, test_set$rating)</code></pre>
<pre><code>## [1] 0.9885931</code></pre>
<p>We already see an improvement. But can we make it better?</p>
</div>
<div id="section-user-effects" class="section level3">
<h3>User effects</h3>
<p>Let’s compute the average rating for user <span class="math inline">\(u\)</span> for those that have rated over 100 movies:</p>
<pre class="r"><code>train_set %&gt;% 
  group_by(userId) %&gt;% 
  summarize(b_u = mean(rating)) %&gt;% 
  filter(n()&gt;=100) %&gt;%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = &quot;black&quot;)</code></pre>
<p><img src="datsci_08_files/figure-html/user-effect-hist-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Notice that there is substantial variability across users as well: some users are very cranky and others love every movie. This implies that a further improvement to our model may be:</p>
<p><span class="math display">\[ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
\]</span></p>
<p>where <span class="math inline">\(b_u\)</span> is a user-specific effect. Now if a cranky user (negative <span class="math inline">\(b_u\)</span>) rates a great movie (positive <span class="math inline">\(b_i\)</span>), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5.</p>
<p>To fit this model, we could again use <code>lm</code> like this:</p>
<pre class="r"><code>lm(rating ~ as.factor(movieId) + as.factor(userId))</code></pre>
<p>but, for the reasons described earlier, we won’t. Instead, we will compute an approximation by computing <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{b}_i\)</span> and estimating <span class="math inline">\(\hat{b}_u\)</span> as the average of <span class="math inline">\(y_{u,i} - \hat{\mu} - \hat{b}_i\)</span>:</p>
<pre class="r"><code>user_avgs &lt;- train_set %&gt;% 
  left_join(movie_avgs, by=&#39;movieId&#39;) %&gt;%
  group_by(userId) %&gt;%
  summarize(b_u = mean(rating - mu - b_i))</code></pre>
<p>We can now construct predictors and see how much the RMSE improves:</p>
<pre class="r"><code>predicted_ratings &lt;- test_set %&gt;% 
  left_join(movie_avgs, by=&#39;movieId&#39;) %&gt;%
  left_join(user_avgs, by=&#39;userId&#39;) %&gt;%
  mutate(pred = mu + b_i + b_u) %&gt;%
  pull(pred)
RMSE(predicted_ratings, test_set$rating)</code></pre>
<pre><code>## [1] 0.9049899</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>We start with a model that <strong>assumes the same rating for all movies and all users</strong>, with all the differences explained by random variation: If <span class="math inline">\(\mu\)</span> represents the true rating for all movies and users and <span class="math inline">\(\varepsilon\)</span> represents independent errors sampled from the same distribution centered at zero, then:</li>
</ul>
<p><span class="math display">\[ 
Y_{u,i} = \mu +  \varepsilon_{u,i}
\]</span></p>
<ul>
<li><p>In this case, the <strong>least squares estimate of <span class="math inline">\(\mu\)</span></strong> — the estimate that minimizes the root mean squared error — is the average rating of all movies across all users.</p></li>
<li><p>We can improve our model by adding a term, <span class="math inline">\(b_i\)</span>, that represents the <strong>average rating for movie <span class="math inline">\(i\)</span></strong>:</p></li>
</ul>
<p><span class="math display">\[ 
Y_{u,i} = \mu + b_i +  \varepsilon_{u,i}
\]</span></p>
<p><span class="math inline">\(b_i\)</span> is the average of <span class="math inline">\(Y_{u,i}\)</span> minus the overall mean for each movie <span class="math inline">\(i\)</span>.</p>
<ul>
<li>We can further improve our model by adding <span class="math inline">\(b_u\)</span>, the <strong>user-specific effect</strong>:</li>
</ul>
<p><span class="math display">\[ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
\]</span></p>
<ul>
<li>Note that because there are thousands of <span class="math inline">\(b\)</span>’s, the <code>lm()</code> function will be very slow or cause R to crash, so <strong>we don’t recommend using linear regression to calculate these effects</strong>.</li>
</ul>
</div>
</div>
<div id="section-comprehension-check-recommendation-systems" class="section level3">
<h3>6.2 Comprehension Check: Recommendation Systems</h3>
<p>Insert assessment here</p>
</div>
<div id="section-regularization" class="section level3">
<h3>Regularization</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#regularization">textbook section - 33.9 Regularization</a>.</p>
</div>
<p>###’ <strong>Motivation</strong></p>
<p>Despite the large movie to movie variation, our improvement in RMSE was only about 5%. Let’s explore where we made mistakes in our first model, using only movie effects <span class="math inline">\(b_i\)</span>. Here are the 10 largest mistakes:</p>
<pre class="r"><code>test_set %&gt;% 
  left_join(movie_avgs, by=&#39;movieId&#39;) %&gt;%
  mutate(residual = rating - (mu + b_i)) %&gt;%
  arrange(desc(abs(residual))) %&gt;%  
  slice(1:10) %&gt;% 
  pull(title)</code></pre>
<pre><code>##  [1] &quot;Kingdom, The (Riget)&quot;            &quot;Heaven Knows, Mr. Allison&quot;      
##  [3] &quot;American Pimp&quot;                   &quot;Chinatown&quot;                      
##  [5] &quot;American Beauty&quot;                 &quot;Apocalypse Now&quot;                 
##  [7] &quot;Taxi Driver&quot;                     &quot;Wallace &amp; Gromit: A Close Shave&quot;
##  [9] &quot;Down in the Delta&quot;               &quot;Stalag 17&quot;</code></pre>
<p>These all seem like obscure movies. Many of them have large predictions. Let’s look at the top 10 worst and best movies based on <span class="math inline">\(\hat{b}_i\)</span>. First, let’s create a database that connects <code>movieId</code> to movie title:</p>
<pre class="r"><code>movie_titles &lt;- movielens %&gt;% 
  select(movieId, title) %&gt;%
  distinct()</code></pre>
<p>Here are the 10 best movies according to our estimate:</p>
<pre class="r"><code>movie_avgs %&gt;% left_join(movie_titles, by=&quot;movieId&quot;) %&gt;%
  arrange(desc(b_i)) %&gt;% 
  slice(1:10)  %&gt;% 
  pull(title)</code></pre>
<pre><code>##  [1] &quot;When Night Is Falling&quot;                                  
##  [2] &quot;Lamerica&quot;                                               
##  [3] &quot;Mute Witness&quot;                                           
##  [4] &quot;Picture Bride (Bijo photo)&quot;                             
##  [5] &quot;Red Firecracker, Green Firecracker (Pao Da Shuang Deng)&quot;
##  [6] &quot;Paris, France&quot;                                          
##  [7] &quot;Faces&quot;                                                  
##  [8] &quot;Maya Lin: A Strong Clear Vision&quot;                        
##  [9] &quot;Heavy&quot;                                                  
## [10] &quot;Gate of Heavenly Peace, The&quot;</code></pre>
<p>And here are the 10 worst:</p>
<pre class="r"><code>movie_avgs %&gt;% left_join(movie_titles, by=&quot;movieId&quot;) %&gt;%
  arrange(b_i) %&gt;% 
  slice(1:10)  %&gt;% 
  pull(title)</code></pre>
<pre><code>##  [1] &quot;Children of the Corn IV: The Gathering&quot;           
##  [2] &quot;Barney&#39;s Great Adventure&quot;                         
##  [3] &quot;Merry War, A&quot;                                     
##  [4] &quot;Whiteboyz&quot;                                        
##  [5] &quot;Catfish in Black Bean Sauce&quot;                      
##  [6] &quot;Killer Shrews, The&quot;                               
##  [7] &quot;Horrors of Spider Island (Ein Toter Hing im Netz)&quot;
##  [8] &quot;Monkeybone&quot;                                       
##  [9] &quot;Arthur 2: On the Rocks&quot;                           
## [10] &quot;Red Heat&quot;</code></pre>
<p>They all seem to be quite obscure. Let’s look at how often they are rated.</p>
<pre class="r"><code>train_set %&gt;% count(movieId) %&gt;% 
  left_join(movie_avgs, by=&quot;movieId&quot;) %&gt;%
  left_join(movie_titles, by=&quot;movieId&quot;) %&gt;%
  arrange(desc(b_i)) %&gt;% 
  slice(1:10) %&gt;% 
  pull(n)
 
train_set %&gt;% count(movieId) %&gt;% 
  left_join(movie_avgs) %&gt;%
  left_join(movie_titles, by=&quot;movieId&quot;) %&gt;%
  arrange(b_i) %&gt;% 
  slice(1:10) %&gt;% 
  pull(n)</code></pre>
<p>The supposed “best” and “worst” movies were rated by very few users, in most cases just 1. These movies were mostly obscure ones. This is because with just a few users, we have more uncertainty. Therefore, larger estimates of <span class="math inline">\(b_i\)</span>, negative or positive, are more likely.</p>
<p>These are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure.</p>
<p>In previous sections, we computed standard error and constructed confidence intervals to account for different levels of uncertainty. However, when making predictions, we need one number, one prediction, not an interval. For this, we introduce the concept of regularization.</p>
<p>Regularization permits us to penalize large estimates that are formed using small sample sizes. It has commonalities with the Bayesian approach that shrunk predictions described in the textbook <a href="https://rafalab.github.io/dsbook/models.html#bayesian-statistics">(Section - 16.4 Bayesian statistics)</a>.</p>
</div>
<div id="section-penalized-least-squares" class="section level3">
<h3>Penalized least squares</h3>
<p>The general idea behind regularization is to constrain the total variability of the effect sizes. Why does this help? Consider a case in which we have movie <span class="math inline">\(i=1\)</span> with 100 user ratings and 4 movies <span class="math inline">\(i=2,3,4,5\)</span> with just one user rating. We intend to fit the model</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
\]</span></p>
<p>Suppose we know the average rating is, say, <span class="math inline">\(\mu = 3\)</span>. If we use least squares, the estimate for the first movie effect <span class="math inline">\(b_1\)</span> is the average of the 100 user ratings, <span class="math inline">\(1/100 \sum_{i=1}^{100} (Y_{i,1} - \mu)\)</span>, which we expect to be a quite precise. However, the estimate for movies 2, 3, 4, and 5 will simply be the observed deviation from the average rating <span class="math inline">\(\hat{b}_i = Y_{u,i} - \hat{\mu}\)</span> which is an estimate based on just one number so it won’t be precise at all. Note these estimates make the error <span class="math inline">\(Y_{u,i} - \mu + \hat{b}_i\)</span> equal to 0 for <span class="math inline">\(i=2,3,4,5\)</span>, but this is a case of over-training. In fact, ignoring the one user and guessing that movies 2,3,4, and 5 are just average movies (<span class="math inline">\(b_i = 0\)</span>) might provide a better prediction. The general idea of penalized regression is to control the total variability of the movie effects: <span class="math inline">\(\sum_{i=1}^5 b_i^2\)</span>. Specifically, instead of minimizing the least squares equation, we minimize an equation that adds a penalty:</p>
<p><span class="math display">\[\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i\right)^2 + \lambda \sum_{i} b_i^2\]</span> The first term is just least squares and the second is a penalty that gets larger when many <span class="math inline">\(b_i\)</span> are large. Using calculus we can actually show that the values of <span class="math inline">\(b_i\)</span> that minimize this equation are:</p>
<p><span class="math display">\[
\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
\]</span></p>
<p>where <span class="math inline">\(n_i\)</span> is the number of ratings made for movie <span class="math inline">\(i\)</span>. This approach will have our desired effect: when our sample size <span class="math inline">\(n_i\)</span> is very large, a case which will give us a stable estimate, then the penalty <span class="math inline">\(\lambda\)</span> is effectively ignored since <span class="math inline">\(n_i+\lambda \approx n_i\)</span>. However, when the <span class="math inline">\(n_i\)</span> is small, then the estimate <span class="math inline">\(\hat{b}_i(\lambda)\)</span> is shrunken towards 0. The larger <span class="math inline">\(\lambda\)</span>, the more we shrink.</p>
<p>Let’s compute these regularized estimates of <span class="math inline">\(b_i\)</span> using <span class="math inline">\(\lambda=3\)</span>. Later, we will see why we picked 3.</p>
<pre class="r"><code>lambda &lt;- 3
mu &lt;- mean(train_set$rating)
movie_reg_avgs &lt;- train_set %&gt;% 
  group_by(movieId) %&gt;% 
  summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n()) </code></pre>
<p>To see how the estimates shrink, let’s make a plot of the regularized estimates versus the least squares estimates.</p>
<pre class="r"><code>tibble(original = movie_avgs$b_i, 
       regularlized = movie_reg_avgs$b_i, 
       n = movie_reg_avgs$n_i) %&gt;%
  ggplot(aes(original, regularlized, size=sqrt(n))) + 
  geom_point(shape=1, alpha=0.5)</code></pre>
<p><img src="datsci_08_files/figure-html/regularization-shrinkage-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Now, let’s look at the top 10 best movies based on the penalized estimates <span class="math inline">\(\hat{b}_i(\lambda)\)</span>:</p>
<pre class="r"><code>train_set %&gt;%
  count(movieId) %&gt;% 
  left_join(movie_reg_avgs, by = &quot;movieId&quot;) %&gt;%
  left_join(movie_titles, by = &quot;movieId&quot;) %&gt;%
  arrange(desc(b_i)) %&gt;% 
  slice(1:10) %&gt;% 
  pull(title)</code></pre>
<p>These make much more sense! These movies are watched more and have more ratings. Here are the top 10 worst movies:</p>
<pre class="r"><code>train_set %&gt;%
  count(movieId) %&gt;% 
  left_join(movie_reg_avgs, by = &quot;movieId&quot;) %&gt;%
  left_join(movie_titles, by=&quot;movieId&quot;) %&gt;%
  arrange(b_i) %&gt;% 
  select(title, b_i, n) %&gt;% 
  slice(1:10) %&gt;% 
  pull(title)</code></pre>
<p>Do we improve our results?</p>
<pre class="r"><code>predicted_ratings &lt;- test_set %&gt;% 
  left_join(movie_reg_avgs, by = &quot;movieId&quot;) %&gt;%
  mutate(pred = mu + b_i) %&gt;%
  pull(pred)
RMSE(predicted_ratings, test_set$rating)</code></pre>
<p>The penalized estimates provide a large improvement over the least squares estimates.</p>
</div>
<div id="section-choosing-the-penalty-terms" class="section level3">
<h3>Choosing the penalty terms</h3>
<p>Note that <span class="math inline">\(\lambda\)</span> is a tuning parameter. We can use cross-validation to choose it.</p>
<pre class="r"><code>lambdas &lt;- seq(0, 10, 0.25)

mu &lt;- mean(train_set$rating)
just_the_sum &lt;- train_set %&gt;% 
  group_by(movieId) %&gt;% 
  summarize(s = sum(rating - mu), n_i = n())

rmses &lt;- sapply(lambdas, function(l){
  predicted_ratings &lt;- test_set %&gt;% 
    left_join(just_the_sum, by=&#39;movieId&#39;) %&gt;% 
    mutate(b_i = s/(n_i+l)) %&gt;%
    mutate(pred = mu + b_i) %&gt;%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)  </code></pre>
<p><img src="datsci_08_files/figure-html/best-penalty-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<pre class="r"><code>lambdas[which.min(rmses)]</code></pre>
<pre><code>## [1] 3</code></pre>
<p>However, while we show this as an illustration, in practice we should be using full cross-validation just on the train set, without using the test set until the final assessment. The test set should never be used for tuning.</p>
<p>We can use regularization for the estimate user effects as well. We are minimizing:</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + 
\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
\]</span></p>
<p>The estimates that minimize this can be found similarly to what we did above. Here we use cross-validation to pick a <span class="math inline">\(\lambda\)</span>:</p>
<pre class="r"><code>lambdas &lt;- seq(0, 10, 0.25)

rmses &lt;- sapply(lambdas, function(l){

  mu &lt;- mean(train_set$rating)
  
  b_i &lt;- train_set %&gt;% 
    group_by(movieId) %&gt;%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u &lt;- train_set %&gt;% 
    left_join(b_i, by=&quot;movieId&quot;) %&gt;%
    group_by(userId) %&gt;%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))

  predicted_ratings &lt;- 
    test_set %&gt;% 
    left_join(b_i, by = &quot;movieId&quot;) %&gt;%
    left_join(b_u, by = &quot;userId&quot;) %&gt;%
    mutate(pred = mu + b_i + b_u) %&gt;%
    pull(pred)
  
    return(RMSE(predicted_ratings, test_set$rating))
})

qplot(lambdas, rmses)  </code></pre>
<p><img src="datsci_08_files/figure-html/best-lambdas-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>For the full model, the optimal <span class="math inline">\(\lambda\)</span> is:</p>
<pre class="r"><code>lambda &lt;- lambdas[which.min(rmses)]
lambda</code></pre>
<pre><code>## [1] 3.25</code></pre>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
method
</th>
<th style="text-align:right;">
RMSE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Just the average
</td>
<td style="text-align:right;">
1.0528047
</td>
</tr>
<tr>
<td style="text-align:left;">
Movie Effect Model
</td>
<td style="text-align:right;">
0.9885931
</td>
</tr>
<tr>
<td style="text-align:left;">
Movie + User Effects Model
</td>
<td style="text-align:right;">
0.9049899
</td>
</tr>
<tr>
<td style="text-align:left;">
Regularized Movie + User Effect Model
</td>
<td style="text-align:right;">
0.8814297
</td>
</tr>
</tbody>
</table>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>To improve our results, we will use <strong>regularization</strong>. Regularization constrains the total variability of the effect sizes by penalizing large estimates that come from small sample sizes.</p></li>
<li><p>To estimate the <span class="math inline">\(b\)</span>’s, we will now <strong>minimize this equation</strong>, which contains a penalty term:</p></li>
</ul>
<p><span class="math display">\[\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i\right)^2 + \lambda \sum_{i} b_i^2\]</span></p>
<ul>
<li>The first term is the mean squared error and the second is a penalty term that gets larger when many <span class="math inline">\(b\)</span>’s are large.</li>
</ul>
<p>The values of <span class="math inline">\(b\)</span> that minimize this equation are given by:</p>
<p><span class="math display">\[
\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
\]</span></p>
<p>where <span class="math inline">\(n_i\)</span> is a number of ratings <span class="math inline">\(b\)</span> for movie <span class="math inline">\(i\)</span>.</p>
<ul>
<li><p>The <strong>larger <span class="math inline">\(\lambda\)</span> is, the more we shrink</strong>. <span class="math inline">\(\lambda\)</span> is a tuning parameter, so we can use cross-validation to choose it. We should be using full cross-validation on just the training set, without using the test set until the final assessment.</p></li>
<li><p>We can also use regularization to estimate the <strong>user effect</strong>. We will now minimize this equation:</p></li>
</ul>
<p><span class="math display">\[
\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + 
\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
\]</span></p>
</div>
</div>
<div id="section-comprehension-check-regularization" class="section level3">
<h3>6.3 Comprehension Check: Regularization</h3>
<p>Insert assessment here</p>
</div>
<div id="section-matrix-factorization" class="section level3">
<h3>Matrix Factorization</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#matrix-factorization">textbook section - 33.11 Matrix factorization</a>.</p>
</div>
<p>Matrix factorization is a widely used concept in machine learning. It is very much related to factor analysis, singular value decomposition (SVD), and principal component analysis (PCA). Here we describe the concept in the context of movie recommendation systems.</p>
<p>We have described how the model:</p>
<p><span class="math display">\[ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
\]</span></p>
<p>accounts for movie to movie differences through the <span class="math inline">\(b_i\)</span> and user to user differences through the <span class="math inline">\(b_u\)</span>. But this model leaves out an important source of variation related to the fact that groups of movies have similar rating patterns and groups of users have similar rating patterns as well. We will discover these patterns by studying the residuals:</p>
<p><span class="math display">\[
r_{u,i} = y_{u,i} - \hat{b}_i - \hat{b}_u
\]</span></p>
<p>To see this, we will convert the data into a matrix so that each user gets a row, each movie gets a column, and <span class="math inline">\(y_{u,i}\)</span> is the entry in row <span class="math inline">\(u\)</span> and column <span class="math inline">\(i\)</span>. For illustrative purposes, we will only consider a small subset of movies with many ratings and users that have rated many movies. We also keep Scent of a Woman (<code>movieId == 3252</code>) because we use it for a specific example:</p>
<pre class="r"><code>train_small &lt;- movielens %&gt;% 
  group_by(movieId) %&gt;%
  filter(n() &gt;= 50 | movieId == 3252) %&gt;% ungroup() %&gt;% 
  group_by(userId) %&gt;%
  filter(n() &gt;= 50) %&gt;% ungroup()

y &lt;- train_small %&gt;% 
  select(userId, movieId, rating) %&gt;%
  spread(movieId, rating) %&gt;%
  as.matrix()</code></pre>
<p>We add row names and column names:</p>
<pre class="r"><code>rownames(y)&lt;- y[,1]
y &lt;- y[,-1]

movie_titles &lt;- movielens %&gt;% 
  select(movieId, title) %&gt;%
  distinct()

colnames(y) &lt;- with(movie_titles, title[match(colnames(y), movieId)])</code></pre>
<p>and convert them to residuals by removing the column and row effects:</p>
<pre class="r"><code>y &lt;- sweep(y, 2, colMeans(y, na.rm=TRUE))
y &lt;- sweep(y, 1, rowMeans(y, na.rm=TRUE))</code></pre>
<p>If the model above explains all the signals, and the <span class="math inline">\(\varepsilon\)</span> are just noise, then the residuals for different movies should be independent from each other. But they are not. Here are some examples:</p>
<pre class="r"><code>m_1 &lt;- &quot;Godfather, The&quot;
m_2 &lt;- &quot;Godfather: Part II, The&quot;
p1 &lt;- qplot(y[ ,m_1], y[,m_2], xlab = m_1, ylab = m_2)

m_1 &lt;- &quot;Godfather, The&quot;
m_3 &lt;- &quot;Goodfellas&quot;
p2 &lt;- qplot(y[ ,m_1], y[,m_3], xlab = m_1, ylab = m_3)

m_4 &lt;- &quot;You&#39;ve Got Mail&quot; 
m_5 &lt;- &quot;Sleepless in Seattle&quot; 
p3 &lt;- qplot(y[ ,m_4], y[,m_5], xlab = m_4, ylab = m_5)

gridExtra::grid.arrange(p1, p2 ,p3, ncol = 3)</code></pre>
<p><img src="datsci_08_files/figure-html/movie-cor-1.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>This plot says that users that liked The Godfather more than what the model expects them to, based on the movie and user effects, also liked The Godfather II more than expected. A similar relationship is seen when comparing The Godfather and Goodfellas. Although not as strong, there is still correlation. We see correlations between You’ve Got Mail and Sleepless in Seattle as well</p>
<p>By looking at the correlation between movies, we can see a pattern (we rename the columns to save print space):</p>
<pre class="r"><code>x &lt;- y[, c(m_1, m_2, m_3, m_4, m_5)]
short_names &lt;- c(&quot;Godfather&quot;, &quot;Godfather2&quot;, &quot;Goodfellas&quot;,
                 &quot;You&#39;ve Got&quot;, &quot;Sleepless&quot;)
colnames(x) &lt;- short_names
cor(x, use=&quot;pairwise.complete&quot;)</code></pre>
<pre><code>##             Godfather Godfather2 Goodfellas You&#39;ve Got  Sleepless
## Godfather   1.0000000  0.8285097  0.4441251 -0.4396801 -0.3780517
## Godfather2  0.8285097  1.0000000  0.5212559 -0.3306926 -0.3578050
## Goodfellas  0.4441251  0.5212559  1.0000000 -0.4810451 -0.4020508
## You&#39;ve Got -0.4396801 -0.3306926 -0.4810451  1.0000000  0.5332399
## Sleepless  -0.3780517 -0.3578050 -0.4020508  0.5332399  1.0000000</code></pre>
<p>There seems to be people that like romantic comedies more than expected, while others that like gangster movies more than expected.</p>
<p>These results tell us that there is structure in the data. But how can we model this?</p>
</div>
<div id="section-factors-analysis" class="section level3">
<h3>Factors analysis</h3>
<p>Here is an illustration, using a simulation, of how we can use some structure to predict the <span class="math inline">\(r_{u,i}\)</span>. Suppose our residuals <code>r</code> look like this:</p>
<pre class="r"><code>round(r, 1)</code></pre>
<pre><code>##    Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless
## 1        2.0        2.3        2.2       -1.8      -1.9
## 2        2.0        1.7        2.0       -1.9      -1.7
## 3        1.9        2.4        2.1       -2.3      -2.0
## 4       -0.3        0.3        0.3       -0.4      -0.3
## 5       -0.3       -0.4        0.3        0.2       0.3
## 6       -0.1        0.1        0.2       -0.3       0.2
## 7       -0.1        0.0       -0.2       -0.2       0.3
## 8        0.2        0.2        0.1        0.0       0.4
## 9       -1.7       -2.1       -1.8        2.0       2.4
## 10      -2.3       -1.8       -1.7        1.8       1.7
## 11      -1.7       -2.0       -2.1        1.9       2.3
## 12      -1.8       -1.7       -2.1        2.3       2.0</code></pre>
<p>There seems to be a pattern here. In fact, we can see very strong correlation patterns:</p>
<pre class="r"><code>cor(r) </code></pre>
<pre><code>##             Godfather Godfather2 Goodfellas You&#39;ve Got  Sleepless
## Godfather   1.0000000  0.9803263  0.9777825 -0.9740312 -0.9657734
## Godfather2  0.9803263  1.0000000  0.9833056 -0.9866056 -0.9917416
## Goodfellas  0.9777825  0.9833056  1.0000000 -0.9864329 -0.9885380
## You&#39;ve Got -0.9740312 -0.9866056 -0.9864329  1.0000000  0.9855309
## Sleepless  -0.9657734 -0.9917416 -0.9885380  0.9855309  1.0000000</code></pre>
<p>We can create vectors <code>q</code> and <code>p</code>, that can explain much of the structure we see. The <code>q</code> would look like this:</p>
<pre class="r"><code>t(q) </code></pre>
<pre><code>##      Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless
## [1,]         1          1          1         -1        -1</code></pre>
<p>and it narrows down movies to two groups: gangster (coded with 1) and romance (coded with -1). We can also reduce the users to three groups:</p>
<pre class="r"><code>t(p)</code></pre>
<pre><code>##      1 2 3 4 5 6 7 8  9 10 11 12
## [1,] 2 2 2 0 0 0 0 0 -2 -2 -2 -2</code></pre>
<p>those that like gangster movies and dislike romance movies (coded as 2), those that like romance movies and dislike gangster movies (coded as -2), and those that don’t care (coded as 0). The main point here is that we can almost reconstruct <span class="math inline">\(r\)</span>, which has 60 values, with a couple of vectors totaling 17 values. If <span class="math inline">\(r\)</span> contains the residuals for users <span class="math inline">\(u=1,\dots,12\)</span> for movies <span class="math inline">\(i=1,\dots,5\)</span> we can write the following mathematical formula for our residuals <span class="math inline">\(r_{u,i}\)</span>.</p>
<p><span class="math display">\[
r_{u,i} \approx p_u q_i 
\]</span></p>
<p>This implies that we can explain more variability by modifying our previous model for movie recommendations to:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + b_u + p_u q_i + \varepsilon_{u,i}
\]</span></p>
<p>However, we motivated the need for the <span class="math inline">\(p_u q_i\)</span> term with a simple simulation. The structure found in data is usually more complex. For example, in this first simulation we assumed there were was just one factor <span class="math inline">\(p_u\)</span> that determined which of the two genres movie <span class="math inline">\(u\)</span> belongs to. But the structure in our movie data seems to be much more complicated than gangster movie versus romance. We may have many other factors. Here we present a slightly more complex simulation. We now add a sixth movie.</p>
<pre class="r"><code>round(r, 1)</code></pre>
<pre><code>##    Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless Scent
## 1        0.5        0.6        1.6       -0.5      -0.5  -1.6
## 2        1.5        1.4        0.5       -1.5      -1.4  -0.4
## 3        1.5        1.6        0.5       -1.6      -1.5  -0.5
## 4       -0.1        0.1        0.1       -0.1      -0.1   0.1
## 5       -0.1       -0.1        0.1        0.0       0.1  -0.1
## 6        0.5        0.5       -0.4       -0.6      -0.5   0.5
## 7        0.5        0.5       -0.5       -0.6      -0.4   0.4
## 8        0.5        0.6       -0.5       -0.5      -0.4   0.4
## 9       -0.9       -1.0       -0.9        1.0       1.1   0.9
## 10      -1.6       -1.4       -0.4        1.5       1.4   0.5
## 11      -1.4       -1.5       -0.5        1.5       1.6   0.6
## 12      -1.4       -1.4       -0.5        1.6       1.5   0.6</code></pre>
<p>By exploring the correlation structure of this new dataset</p>
<pre class="r"><code>colnames(r)[4:6] &lt;- c(&quot;YGM&quot;, &quot;SS&quot;, &quot;SW&quot;)
cor(r)</code></pre>
<pre><code>##             Godfather Godfather2 Goodfellas        YGM         SS         SW
## Godfather   1.0000000  0.9973680  0.5619427 -0.9968137 -0.9955408 -0.5712246
## Godfather2  0.9973680  1.0000000  0.5770328 -0.9983198 -0.9989206 -0.5833762
## Goodfellas  0.5619427  0.5770328  1.0000000 -0.5519540 -0.5831845 -0.9940376
## YGM        -0.9968137 -0.9983198 -0.5519540  1.0000000  0.9983286  0.5581798
## SS         -0.9955408 -0.9989206 -0.5831845  0.9983286  1.0000000  0.5875136
## SW         -0.5712246 -0.5833762 -0.9940376  0.5581798  0.5875136  1.0000000</code></pre>
<p>We note that we perhaps need a second factor to account for the fact that some users like Al Pacino, while others dislike him or don’t care. Notice that the overall structure of the correlation obtained from the simulated data is not that far off the real correlation:</p>
<pre class="r"><code>six_movies &lt;- c(m_1, m_2, m_3, m_4, m_5, m_6)
x &lt;- y[, six_movies]
colnames(x) &lt;- colnames(r)
cor(x, use=&quot;pairwise.complete&quot;)</code></pre>
<pre><code>##              Godfather Godfather2 Goodfellas        YGM         SS          SW
## Godfather   1.00000000  0.8285097  0.4441251 -0.4396801 -0.3780517  0.05890399
## Godfather2  0.82850973  1.0000000  0.5212559 -0.3306926 -0.3578050  0.11858818
## Goodfellas  0.44412508  0.5212559  1.0000000 -0.4810451 -0.4020508 -0.12299560
## YGM        -0.43968010 -0.3306926 -0.4810451  1.0000000  0.5332399 -0.16988665
## SS         -0.37805172 -0.3578050 -0.4020508  0.5332399  1.0000000 -0.18217771
## SW          0.05890399  0.1185882 -0.1229956 -0.1698867 -0.1821777  1.00000000</code></pre>
<p>To explain this more complicated structure, we need two factors. For example something like this:</p>
<pre class="r"><code>t(q) </code></pre>
<pre><code>##      Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless Scent
## [1,]         1          1          1         -1        -1    -1
## [2,]         1          1         -1         -1        -1     1</code></pre>
<p>With the first factor (the first row) used to code the gangster versus romance groups and a second factor (the second row) to explain the Al Pacino versus no Al Pacino groups. We will also need two sets of coefficients to explain the variability introduced by the <span class="math inline">\(3\times 3\)</span> types of groups:</p>
<pre class="r"><code>t(p)</code></pre>
<pre><code>##         1   2   3 4 5   6   7   8  9   10   11   12
## [1,]  1.0 1.0 1.0 0 0 0.0 0.0 0.0 -1 -1.0 -1.0 -1.0
## [2,] -0.5 0.5 0.5 0 0 0.5 0.5 0.5  0 -0.5 -0.5 -0.5</code></pre>
<p>The model with two factors has 36 parameters that can be used to explain much of the variability in the 72 ratings:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + b_u + p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \varepsilon_{u,i}
\]</span></p>
<p>Note that in an actual data application, we need to fit this model to data. To explain the complex correlation we observe in real data, we usually permit the entries of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> to be continuous values, rather than discrete ones as we used in the simulation. For example, rather than dividing movies into gangster or romance, we define a continuum. Also note that this is not a linear model and to fit it we need to use an algorithm other than the one used by <code>lm</code> to find the parameters that minimize the least squares. The winning algorithms for the Netflix challenge fit a model similar to the above and used regularization to penalize for large values of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>, rather than using least squares. Implementing this approach is beyond the scope of this course</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>Our earlier models fail to account for an important source of variation related to the fact that groups of movies and groups of users have similar rating patterns. We can observe these patterns by studying the residuals and <strong>converting our data into a matrix where each user gets a row and each movie gets a column</strong>:</li>
</ul>
<p><span class="math display">\[
r_{u,i} = y_{u,i} - \hat{b}_i - \hat{b}_u
\]</span></p>
<p>where <span class="math inline">\(y_{u,i}\)</span> is the entry in row <span class="math inline">\(u\)</span> and column <span class="math inline">\(i\)</span>.</p>
<ul>
<li>We can <strong>factorize the matrix of residuals <span class="math inline">\(r\)</span></strong> into a vector <span class="math inline">\(p\)</span> and vector $q,r_{u,i} p_u q_i $, allowing us to explain more of the variance using a model like this:</li>
</ul>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + b_u + p_u q_i +  \varepsilon_{i,j}
\]</span></p>
<ul>
<li>Because our example is more complicated, we can use <strong>two factors to explain the structure and two sets of coefficients to describe users</strong>:</li>
</ul>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + b_u + p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \varepsilon_{u,j}
\]</span></p>
<ul>
<li>To estimate factors using our data instead of constructing them ourselves, we can use <strong>principal component analysis (PCA) or singular value decomposition (SVD)</strong>.</li>
</ul>
</div>
</div>
<div id="section-svd-and-pca" class="section level3">
<h3>SVD and PCA</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/large-datasets.html#connection-to-svd-and-pca">textbook section - 33.11.2 Connection to SVD and PCA</a>.</p>
</div>
<p>The decomposition:</p>
<p><span class="math display">\[
r_{u,i} \approx p_{u,1} q_{1,i} + p_{u,2} q_{2,i}
\]</span></p>
<p>is very much related to SVD and PCA. SVD and PCA are complicated concepts, but one way to understand them is that SVD is an algorithm that finds the vectors <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> that permit us to rewrite the matrix <span class="math inline">\(\mbox{r}\)</span> with <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns as:</p>
<p><span class="math display">\[
r_{u,i} = p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \dots + p_{u,m} q_{m,i} 
\]</span></p>
<p>with the variability of each term decreasing and with the <span class="math inline">\(p\)</span>s uncorrelated. The algorithm also computes this variability so that we can know how much of the matrices, total variability is explained as we add new terms. This may permit us to see that, with just a few terms, we can explain most of the variability.</p>
<p>Let’s see an example with the movie data. To compute the decomposition, we will make the residuals with NAs equal to 0:</p>
<pre class="r"><code>y[is.na(y)] &lt;- 0
pca &lt;- prcomp(y)</code></pre>
<p>The <span class="math inline">\(q\)</span> vectors are called the principal components and they are stored in this matrix:</p>
<pre class="r"><code>dim(pca$rotation)</code></pre>
<pre><code>## [1] 454 292</code></pre>
<p>While the <span class="math inline">\(p\)</span>, or the user effects, are here:</p>
<pre class="r"><code>dim(pca$x)</code></pre>
<pre><code>## [1] 292 292</code></pre>
<p>We can see the variability of each of the vectors:</p>
<pre class="r"><code>qplot(1:nrow(x), pca$sdev, xlab = &quot;PC&quot;)</code></pre>
<p><img src="datsci_08_files/figure-html/pca-sds-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<!--
and see that just the first few already explain a large percent:


```r
var_explained <- cumsum(pca$sdev^2 / sum(pca$sdev^2))
qplot(1:nrow(x), var_explained, xlab = "PC")
```

<img src="datsci_08_files/figure-html/var-expained-pca-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" />
-->
<p>We also notice that the first two principal components are related to the structure in opinions about movies:</p>
<p><img src="datsci_08_files/figure-html/movies-pca-1.png" width="500px" style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;" /></p>
<p>Just by looking at the top 10 in each direction, we see a meaningful pattern. The first PC shows the difference between critically acclaimed movies on one side:</p>
<pre><code>##  [1] &quot;Pulp Fiction&quot;              &quot;Seven (a.k.a. Se7en)&quot;     
##  [3] &quot;Fargo&quot;                     &quot;2001: A Space Odyssey&quot;    
##  [5] &quot;Silence of the Lambs, The&quot; &quot;Clockwork Orange, A&quot;      
##  [7] &quot;Taxi Driver&quot;               &quot;Being John Malkovich&quot;     
##  [9] &quot;Royal Tenenbaums, The&quot;     &quot;Shining, The&quot;</code></pre>
<p>and Hollywood blockbusters on the other:</p>
<pre><code>##  [1] &quot;Independence Day (a.k.a. ID4)&quot;  &quot;Shrek&quot;                         
##  [3] &quot;Spider-Man&quot;                     &quot;Titanic&quot;                       
##  [5] &quot;Twister&quot;                        &quot;Armageddon&quot;                    
##  [7] &quot;Harry Potter and the Sorcer...&quot; &quot;Forrest Gump&quot;                  
##  [9] &quot;Lord of the Rings: The Retu...&quot; &quot;Enemy of the State&quot;</code></pre>
<p>While the second PC seems to go from artsy, independent films:</p>
<pre><code>##  [1] &quot;Shawshank Redemption, The&quot;      &quot;Truman Show, The&quot;              
##  [3] &quot;Little Miss Sunshine&quot;           &quot;Slumdog Millionaire&quot;           
##  [5] &quot;Amelie (Fabuleux destin d&#39;A...&quot; &quot;Kill Bill: Vol. 1&quot;             
##  [7] &quot;American Beauty&quot;                &quot;City of God (Cidade de Deus)&quot;  
##  [9] &quot;Mars Attacks!&quot;                  &quot;Beautiful Mind, A&quot;</code></pre>
<p>to nerd favorites:</p>
<pre><code>##  [1] &quot;Lord of the Rings: The Two ...&quot; &quot;Lord of the Rings: The Fell...&quot;
##  [3] &quot;Lord of the Rings: The Retu...&quot; &quot;Matrix, The&quot;                   
##  [5] &quot;Star Wars: Episode IV - A N...&quot; &quot;Star Wars: Episode VI - Ret...&quot;
##  [7] &quot;Star Wars: Episode V - The ...&quot; &quot;Spider-Man 2&quot;                  
##  [9] &quot;Dark Knight, The&quot;               &quot;Speed&quot;</code></pre>
<p>Fitting a model that incorporates these estimates is complicated. For those interested in implementing an approach that incorporates these ideas, we recommend trying the <strong>recommenderlab</strong> package. The details are beyond the scope of this course.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>You can think of <strong>singular value decomposition (SVD)</strong> as an algorithm that finds the vectors <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> that permit us to write the matrix of residuals <span class="math inline">\(r\)</span> with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(n\)</span> columns in the following way:</li>
</ul>
<p><span class="math display">\[
r_{u,i} = p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \dots + p_{u,m} q_{m,i} 
\]</span></p>
<p>with the variability of these terms decreasing and the <span class="math inline">\(p\)</span>’s uncorrelated to each other.</p>
<ul>
<li><p><strong>SVD also computes the variabilities</strong> so that we can know how much of the matrix’s total variability is explained as we add new terms.</p></li>
<li><p>The <strong>vectors <span class="math inline">\(q\)</span> are called the principal components</strong> and the <strong>vectors <span class="math inline">\(p\)</span> are the user effects</strong>. By using principal components analysis (PCA), matrix factorization can capture structure in the data determined by user opinions about movies.</p></li>
</ul>
</div>
</div>
<div id="section-comprehension-check-matrix-factorization" class="section level3">
<h3>6.3 Comprehension Check: Matrix Factorization</h3>
<p>Insert assessment here</p>
</div>
<div id="section-comprehension-check-dimension-reduction" class="section level3">
<h3>6.3 Comprehension Check: Dimension Reduction</h3>
<p>Insert assessment here</p>
</div>
<div id="section-comprehension-check-clustering" class="section level3">
<h3>6.3 Comprehension Check: Clustering</h3>
<p>Insert assessment here</p>
</div>
<div id="section-breast-cancer-project-part-1" class="section level3">
<h3>7.1 Breast Cancer Project Part 1</h3>
<p>Insert assessment here</p>
</div>
<div id="section-breast-cancer-project-part-2" class="section level3">
<h3>7.1 Breast Cancer Project Part 2</h3>
<p>Insert assessment here</p>
</div>
<div id="section-breast-cancer-project-part-3" class="section level3">
<h3>7.1 Breast Cancer Project Part 3</h3>
<p>Insert assessment here</p>
</div>
<div id="section-breast-cancer-project-part-4" class="section level3">
<h3>7.1 Breast Cancer Project Part 4</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-acknowledgement" class="section level2">
<h2>Acknowledgement</h2>
<p>I am extremely grateful to <a href="http://rafalab.github.io/pages/about.html">Prof Rafael Irizarry</a> for his support and encouragement to create this interactive tutorial which is based on his freely available textbook <a href="https://rafalab.github.io/dsbook/">Introduction to Data Science</a>. The textbook has been developed as the basis for the associated edX Course Series <em>HarvardX Professional Certificate in Data Science</em> and this tutorial follows the structure of this online course. I’m further very grateful to <a href="https://profiles.sussex.ac.uk/p9846-andy-field">Andy Field</a> for his generous permission to use his <code>discovr</code> package as a basis for the development of this tutorial. Thanks to his amazing <code>discovr</code> package I also indirectly benefited from the work of <a href="https://www.allisonhorst.com/">Allison Horst</a> and her very informative blog post on <a href="https://education.rstudio.com/blog/2020/05/learnr-for-remote/">styling learnr tutorials with CSS</a> as well as her CSS template file which I adapted here.</p>

<script type="application/shiny-prerendered" data-context="server-start">
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

library(learnr) #necessary to render tutorial correctly

library(forcats)
library(ggplot2)
library(htmltools)
library(kableExtra)
library(lubridate)
library(magrittr)
library(tibble)


source("./www/datsci_helpers.R")
img_path <- "./images/" #set image path
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::register_http_handlers(session, metadata = NULL)
</script>
 
<script type="application/shiny-prerendered" data-context="server">
session$onSessionEnded(function() {
        learnr:::session_stop_event(session)
      })
</script>
 <!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/lumen.min.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["navigation"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/navigation-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tabsets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["default.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["font-awesome"]},{"type":"character","attributes":{},"value":["5.1.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/fontawesome"]}]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["css/all.css","css/v4-shims.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootbox"]},{"type":"character","attributes":{},"value":["4.4.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/bootbox"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["bootbox.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["idb-keyvalue"]},{"type":"character","attributes":{},"value":["3.2.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/idb-keyval"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["idb-keyval-iife-compat.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125]}},"value":[{"type":"character","attributes":{},"value":["assertthat","backports","base","blob","broom","caret","cellranger","class","cli","codetools","colorspace","compiler","crayon","data.table","datasets","DBI","dbplyr","digest","dplyr","dslabs","e1071","ellipsis","evaluate","fansi","farver","fastmap","forcats","foreach","fs","gam","generics","gganimate","ggplot2","ggrepel","glue","gower","graphics","grDevices","grid","gridExtra","gtable","haven","highr","HistData","hms","htmltools","htmlwidgets","httpuv","httr","ipred","isoband","iterators","jsonlite","kableExtra","knitr","labeling","later","lattice","lava","learnr","lifecycle","lubridate","magick","magrittr","markdown","MASS","Matrix","matrixStats","methods","mgcv","mime","ModelMetrics","modelr","munsell","nlme","nnet","pillar","pkgconfig","plyr","prettyunits","pROC","prodlim","progress","promises","purrr","R6","rafalib","randomForest","RColorBrewer","Rcpp","readr","readxl","recipes","reprex","reshape2","rlang","rmarkdown","rpart","rprojroot","rstudioapi","rvest","scales","shiny","splines","stats","stats4","stringi","stringr","survival","tibble","tidyr","tidyselect","tidyverse","timeDate","tools","tweenr","utils","vctrs","viridisLite","webshot","withr","xfun","xml2","xtable","yaml"]},{"type":"character","attributes":{},"value":["0.2.1","1.1.8","4.0.2","1.2.1","0.7.0","6.0-86","1.1.0","7.3-17","2.0.2","0.2-16","1.4-1","4.0.2","1.3.4","1.13.0","4.0.2","1.1.0","1.4.4","0.6.25","1.0.0","0.7.3","1.7-3","0.3.1","0.14","0.4.1","2.0.3","1.0.1","0.5.0","1.5.0","1.4.2","1.20","0.0.2","1.0.6","3.3.2","0.8.2","1.4.1","0.2.2","4.0.2","4.0.2","4.0.2","2.3","0.3.0","2.3.1","0.8","0.8-6","0.5.3","0.5.0","1.5.1","1.5.4","1.4.2","0.9-9","0.2.2","1.0.12","1.7.0","1.1.0","1.29","0.3","1.1.0.1","0.20-41","1.6.7","0.10.1","0.2.0","1.7.9","2.4.0","1.5","1.1","7.3-52","1.2-18","0.56.0","4.0.2","1.8-31","0.9","1.2.2.2","0.1.8","0.5.0","3.1-148","7.3-14","1.4.6","2.0.3","1.8.6","1.1.1","1.16.2","2019.11.13","1.2.2","1.1.1","0.3.4","2.4.1","1.0.0","4.6-14","1.1-2","1.0.5","1.3.1","1.3.1","0.1.13","0.3.0","1.4.4","0.4.7","2.3","4.1-15","1.3-2","0.11","0.3.6","1.1.1","1.5.0","4.0.2","4.0.2","4.0.2","1.4.6","1.4.0","3.2-3","3.0.3","1.1.0","1.1.0","1.3.0","3043.102","4.0.2","1.0.1","4.0.2","0.3.2","0.3.0","0.5.2","2.2.0","0.16","1.3.2","1.8-4","2.2.1"]}]}]}
</script>
<!--/html_preserve-->
</div>
</div>

</div> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h2 class="title toc-ignore" style="display:none;">datsci_08: Machine Learning</h2>
<h4 class="author"><em>Rafael Irizarry &amp; Fatih Uenal</em></h4>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</div> <!-- bandContent page -->
</div> <!-- pageContent band -->




<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>

</html>
