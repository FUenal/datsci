<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Rafael Irizarry &amp; Fatih Uenal" />


<meta name="progressive" content="true" />
<meta name="allow-skip" content="false" />

<title>datsci_07: Linear Regression</title>


<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>

<!-- taken from https://github.com/rstudio/rmarkdown/blob/67b7f5fc779e4cfdfd0f021d3d7745b6b6e17149/inst/rmd/h/default.html#L296-L362 -->
<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("section-TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>
<!-- end tabsets -->


<link rel="stylesheet" href="css/datsci_style.css" type="text/css" />

</head>

<body>



<div class="pageContent band">
<div class="bandContent page">

<div class="topics">

<html lang="en">
<div id="section-datsci_07-linear-regression" class="section level1">
<h1>datsci_07: Linear Regression</h1>
<div id="section-welcome-to-data-science-linear-regression" class="section level2">
<h2>Welcome to Data Science: Linear Regression!</h2>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:100px;height:100px;" class = "img_left"></p>
<p><strong>Usage:</strong> This tutorial accompanies the textbook <a href="https://rafalab.github.io/dsbook/">Introduction to Data Science</a> by <a href="http://rafalab.github.io/pages/about.html">Prof Rafael Irizarry</a>. It contains material from the textbook which is offered under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a>.</p>
</div>
<div id="section-data-science-linear-regression" class="section level3">
<h3>Data Science: Linear Regression!</h3>
<p>We’re excited to have you join us in this course, which is designed to teach you about linear regression, one of the most common statistical modeling approaches used in data science.</p>
<p>This is the <strong>seventh</strong> in a series of courses in the Introduction to Data Science program, a series of courses that prepare you to do data analysis in <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>, from simple computations to machine learning. We assume that you have taken the preceding six courses in the series or are already familiar with the content covered in them.</p>
<p>Linear regression is commonly used to quantify the relationship between two or more variables. It is also used to adjust for confounding. In this course, we cover how to implement linear regression and adjust for confounding in practice using <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>.</p>
<p>In data science applications, it is very common to be interested in the relationship between two or more variables. The motivating case study we examine in this course relates to the data-driven approach used to construct baseball teams described in the book (and movie) Moneyball. We will try to determine which measured outcomes best predict baseball runs and to do this we’ll use linear regression.</p>
<p>We will also examine confounding, where extraneous variables affect the relationship between two or more other variables, leading to spurious associations. Linear regression is a powerful technique for removing confounders, but it is not a magical process, and it is essential to understand when it is appropriate to use. You will learn when to use it in this course.</p>
<p>The class notes for this course series can be found in Professor Irizarry’s freely available <a href="https://rafalab.github.io/dsbook/">Introduction to Data Science book</a>.</p>
</div>
<div id="section-in-this-course-you-will-learn" class="section level3">
<h3>In this course, you will learn:</h3>
<ul>
<li><p>How linear regression was originally developed by Galton</p></li>
<li><p>What confounding is and how to detect it</p></li>
<li><p>How to examine the relationships between variables by implementing linear regression in <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg></p></li>
</ul>
</div>
<div id="section-course-overview" class="section level3">
<h3>Course overview</h3>
<p>There are three major sections in this course: introduction to linear regression, linear models, and confounding.</p>
<p><strong>Introduction to Linear Regression</strong></p>
<p>In this section, you’ll learn the basics of linear regression through this course’s motivating example, the data-driven approach used to construct baseball teams. You’ll also learn about correlation, the correlation coefficient, stratification, and the variance explained.</p>
<p><strong>Linear Models</strong></p>
<p>In this section, you’ll learn about linear models. You’ll learn about least squares estimates, multivariate regression, and several useful features of <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>, such as tibbles, lm, do, and broom. You’ll learn how to apply regression to baseball to build a better offensive metric.</p>
<p><strong>Confounding</strong></p>
<p>In the final section of the course, you’ll learn about confounding and several reasons that correlation is not the same as causation, such as spurious correlation, outliers, reversing cause and effect, and confounders. You’ll also learn about Simpson’s Paradox.</p>
</div>
<div id="section-course-instructor" class="section level3">
<h3>Course Instructor</h3>
<div class="infobox">
<p><img src="images/photo2014.jpg" alt="Dr Fatih Uenal." style="width:150px;height:200px;" class = "img_left" ></p>
<p><strong>Fatih Uenal</strong> is currenlty a Visitng Postdoctoral Researcher at the University of Cambridge, Department of Psychology, where he conducts research on the psychology of anthropocentrism and social and ecological dominance. Prior to his current position, he has worked as a postdoc at <a href="https://scholar.harvard.edu/fatih-uenal/home">Harvard University</a>. Together with <a href="http://rafalab.github.io/pages/about.html">Prof Rafael Irizarry</a> he programmed this interactive tutorial based on the the textbook <a href="https://rafalab.github.io/dsbook/"><em>Introduction to Data Science</em></a>. This interactive tutorial is developed using the <code>learnr</code> package. It has a general social scientists audience in mind and is suited for undergrad and graduate levels of study.</p>
<p>Webpage: <a href="https://scholar.harvard.edu/fatih-uenal/home" class="uri">https://scholar.harvard.edu/fatih-uenal/home</a></p>
</div>
<hr />
</div>
<div id="section-essential-course-information" class="section level3">
<h3>Essential Course Information</h3>
<div id="section-course-objectives" class="section level4">
<h4><strong>Course Objectives</strong></h4>
<p>“Data science” is a catch-all term used to describe the practice of working with and analyzing messy data sources to draw meaningful conclusions using techniques developed by computer scientists and computational statisticians. The purpose of this course is to give students who are training as quantitative social scientists a broad introduction to this skillset via the statistical programming language, <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>. You will learn how to conduct many statistical analyses such as univariate statistics (e.g., correlation, regression) in <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> that you may have already done in SPSS, Excel, or another such program. Additionally, we will build on this foundation to explore new skillsets uncommon in the social sciences, such as natural language processing, and machine learning.</p>
<p>At the end of this course you will be able to:</p>
<ul>
<li><p>To answer research questions in Social Sciences (e.g., Psychology) with data</p></li>
<li><p>Understand the basics of research designs in Social Sciences, and how they relate to data-analysis strategies</p></li>
<li><p>Develop an intuitive, practical, and conceptual understanding of strategies for asking and answering questions with data</p></li>
<li><p>To use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>, a free and open-source statistics software program.</p></li>
<li><p>Develop a basic understanding of frequently used Data Science Techniques.</p></li>
<li><p>Practice your newly acquired skills with interesting, interactive, and fun projects.</p></li>
</ul>
<p><strong>NOTE</strong>: The schedule and procedures described in this syllabus are subject to change depending on specific needs and requirements. You will always be notified of changes on the homepage (see “last update”).</p>
</div>
<div id="section-course-structure" class="section level4">
<h4><strong>Course Structure</strong></h4>
<p>This is the first module in a series of a 8 week-intensive course. and I suggest that you devote approx 10 hours a week to learning <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>, or if you are teaching graduate students, I’d recommend adopting the schedule below, which is designed for an intense but doable semester-long course, one module per week. It is intended to take the average graduate student roughly 10 hours per week to complete all required tasks.However, some number of students will find programming to be more challenging and may take up to 15 hours per week. Some will breeze through the material in 5.</p>
</div>
<div id="section-grading" class="section level4">
<h4><strong>Grading</strong></h4>
<p>Each Monday, lessons will be assigned from datacamp.com. Some of these lessons will be complete DataCamp courses, and others will be specific modules of courses. This will all be managed by assigning content to your (free) DataCamp account. The amount of content assigned will vary between one and two courses of content. DataCamp considers a course to be roughly 4 hours of lessons, which includes practice time. Realistically, the time you need will depend upon how intuitive you find <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to be. For students already familiar with other programming languages and those with previous <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> experience, “8 hours” of courses is realistically closer to 2 hours; for complete novices that also find the material difficult, 8 hours is a realistic estimate. It is strongly recommended that you stretch out DataCamp lessons across the assignment period, for example, allocating 1 hour each day. You will gain the most by treating this as a foreign language immersion course by using R every day, including for your own research. Remember that you can always go to the <strong>Slack Group</strong> for help.</p>
</div>
<div id="section-passing-rate" class="section level4">
<h4><strong>Passing Rate</strong></h4>
<p>The passing rate is 70%.</p>
</div>
</div>
<div id="section-pre-course-survey" class="section level3">
<h3>Pre-Course Survey</h3>
<p>Insert Survey Link here</p>
<p><em>If you cannot see the survey above, click this link to access it in a new window.</em></p>
</div>
</div>
<div id="section-section-1-data-import" class="section level2">
<h2>Section 1: Data Import</h2>
<p>In the <strong>Introduction to Regression</strong> section, you will learn the basics of linear regression.</p>
<p>After completing this section, you will be able to:</p>
<ul>
<li><p>Understand how Galton developed <strong>linear regression</strong>.</p></li>
<li><p>Calculate and interpret the <strong>sample correlation</strong>.</p></li>
<li><p><strong>Stratify</strong> a dataset when appropriate.</p></li>
<li><p>Understand what a <strong>bivariate normal distribution</strong> is.</p></li>
<li><p>Explain what the term <strong>variance explained</strong> means.</p></li>
<li><p>Interpret the two <strong>regression lines</strong>.</p></li>
</ul>
<p>This section has three parts: <strong>Baseball as a Motivating Example, Correlation</strong>, and <strong>Stratification and Variance Explained</strong>. There are comprehension checks at the end of each part.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your own learning.</p>
<div id="section-introduction-and-motivation" class="section level3">
<h3>Introduction and Motivation</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#case-study-moneyball">textbook section - 18.1 Case study: Moneyball</a>.</p>
</div>
<p><em>Moneyball: The Art of Winning an Unfair Game</em> is a book by Michael Lewis about the Oakland Athletics (A’s) baseball team and its general manager, the person tasked with building the team, Billy Beane.</p>
<p>Traditionally, baseball teams use <em>scouts</em> to help them decide what players to hire. These scouts evaluate players by observing them perform. Scouts tend to favor athletic players with observable physical abilities. For this reason, scouts tend to agree on who the best players are and, as a result, these players tend to be in high demand. This in turn drives up their salaries.</p>
<p>From 1989 to 1991, the A’s had one of the highest payrolls in baseball. They were able to buy the best players and, during that time, they were one of the best teams. However, in 1995 the A’s team owner changed and the new management cut the budget drastically, leaving then general manager, Sandy Alderson, with one of the lowest payrolls in baseball. He could no longer afford the most sought-after players. Alderson began using a statistical approach to find inefficiencies in the market. Alderson was a mentor to Billy Beane, who succeeded him in 1998 and fully embraced data science, as opposed to scouts, as a method for finding low-cost players that data predicted would help the team win. Today, this strategy has been adapted by most baseball teams. As we will see, regression plays a large role in this approach.</p>
<p>As motivation for this section, we will pretend it is 2002 and try to build a baseball team with a limited budget, just like the A’s had to do. To appreciate what you are up against, note that in 2002 the Yankees’ payroll of $125,928,583 more than tripled the Oakland A’s $39,679,746:</p>
<p><img src="datsci_07_files/figure-html/mlb-2002-payroll-1.png" width="624" /></p>
</div>
<div id="section-sabermetics" class="section level3">
<h3>Sabermetics</h3>
<p>Statistics have been used in baseball since its beginnings. The dataset we will be using, included in the <strong>Lahman</strong> library, goes back to the 19th century. For example, a summary statistics we will describe soon, the <em>batting average</em>, has been used for decades to summarize a batter’s success. Other statistics](<a href="http://mlb.mlb.com/stats/league_leaders.jsp" class="uri">http://mlb.mlb.com/stats/league_leaders.jsp</a>) such as home runs (HR), runs batted in (RBI), and stolen bases (SB) are reported for each player in the [game summaries included in the sports section of newspapers, with players rewarded for high numbers. Although summary statistics such as these were widely used in baseball, data analysis per se was not. These statistics were arbitrarily decided on without much thought as to whether they actually predicted anything or were related to helping a team win.</p>
<p>This changed with <a href="https://en.wikipedia.org/wiki/Bill_James">Bill James</a>. In the late 1970s, this aspiring writer and baseball fan started publishing articles describing more in-depth analysis of baseball data. He named the approach of using data to predict what outcomes best predicted if a team would win <a href="https://en.wikipedia.org/wiki/Sabermetrics"><em>sabermetrics</em></a>. Until Billy Beane made sabermetrics the center of his baseball operation, Bill James’ work was mostly ignored by the baseball world. Currently, sabermetrics popularity is no longer limited to just baseball; other sports have started to use this approach as well.</p>
<p>In this section, to simplify the exercise, we will focus on scoring runs and ignore the two other important aspects of the game: pitching and fielding. We will see how regression analysis can help develop strategies to build a competitive baseball team with a constrained budget. The approach can be divided into two separate data analyses. In the first, we determine which recorded player-specific statistics predict runs. In the second, we examine if players were undervalued based on what our first analysis predicts.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>Bill James was the originator of the <strong>sabermetrics</strong>, the approach of using data to predict what outcomes best predicted if a team would win.</li>
</ul>
</div>
</div>
<div id="section-baseball-basics" class="section level3">
<h3>Baseball Basics</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#baseball-basics">textbook section - 18.1.2 Baseball basics</a>.</p>
</div>
<p>To see how regression will help us find undervalued players, we actually don’t need to understand all the details about the game of baseball, which has over 100 rules. Here, we distill the sport to the basic knowledge one needs to know how to effectively attack the data science problem.</p>
<p>The goal of a baseball game is to score more runs (points) than the other team. Each team has 9 batters that have an opportunity to hit a ball with a bat in a predetermined order. After the 9th batter has had their turn, the first batter bats again, then the second, and so on. Each time a batter has an opportunity to bat, we call it a plate appearance (PA). At each PA, the other team’s <em>pitcher</em> throws the ball and the batter tries to hit it. The PA ends with an binary outcome: the batter either makes an <em>out</em> (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases). Each team gets nine tries, referred to as <em>innings</em>, to score runs and each inning ends after three outs (three failures).</p>
<p>Here is a video showing a success: <a href="https://www.youtube.com/watch?v=HL-XjMCPfio">https://www.youtube.com/watch?v=HL-XjMCPfio</a>. And here is one showing a failure: <a href="https://www.youtube.com/watch?v=NeloljCx-1g">https://www.youtube.com/watch?v=NeloljCx-1g</a>. In these videos, we see how luck is involved in the process. When at bat, the batter wants to hit the ball hard. If the batter hits it hard enough, it is a HR, the best possible outcome as the batter gets at least one automatic run. But sometimes, due to chance, the batter hits the ball very hard and a defender catches it, resulting in an out. In contrast, sometimes the batter hits the ball softly, but it lands just in the right place. The fact that there is chance involved hints at why probability models will be involved.</p>
<p>Now there are several ways to succeed. Understanding this distinction will be important for our analysis. When the batter hits the ball, the batter wants to pass as many <em>bases</em> as possible. There are four bases with the fourth one called <em>home plate</em>. Home plate is where batters start by trying to hit, so the bases form a cycle.</p>
<p><img src="images/Baseball_Diamond1.png" width="50%" /> (<a href="https://en.wikipedia.org/wiki/User:Cburnett">Courtesy of Cburnett</a>. <a href="https://creativecommons.org/licenses/by-sa/3.0/deed.en">CC BY-SA 3.0 license</a>.) <!--Source: [Wikipedia Commons](https://commons.wikimedia.org/wiki/File:Baseball_diamond_simplified.svg))--></p>
<p>A batter who <em>goes around the bases</em> and arrives home, scores a run.</p>
<p>We are simplifying a bit, but there are five ways a batter can succeed, that is, not make an out:</p>
<ul>
<li>Bases on balls (BB) - the pitcher fails to throw the ball through a predefined area considered to be hittable (the strikezone), so the batter is permitted to go to first base.</li>
<li>Single - Batter hits the ball and gets to first base.</li>
<li>Double (2B) - Batter hits the ball and gets to second base.</li>
<li>Triple (3B) - Batter hits the ball and gets to third base.</li>
<li>Home Run (HR) - Batter hits the ball and goes all the way home and scores a run.</li>
</ul>
<p>Here is an example of a HR: <a href="https://www.youtube.com/watch?v=xYxSZJ9GZ-w">https://www.youtube.com/watch?v=xYxSZJ9GZ-w</a>. If a batter gets to a base, the batter still has a chance of getting home and scoring a run if the next batter hits successfully. While the batter is <em>on base</em>, the batter can also try to steal a base (SB). If a batter runs fast enough, the batter can try to go from one base to the next without the other team tagging the runner. [Here] is an example of a stolen base: <a href="https://www.youtube.com/watch?v=JSE5kfxkzfk">https://www.youtube.com/watch?v=JSE5kfxkzfk</a>.</p>
<p>All these events are kept track of during the season and are available to us through the <strong>Lahman</strong> package. Now we will start discussing how data analysis can help us decide how to use these statistics to evaluate players.</p>
</div>
<div id="section-no-awards-for-bb" class="section level3">
<h3>No awards for BB</h3>
<p>Historically, the <em>batting average</em> has been considered the most important offensive statistic. To define this average, we define a <em>hit</em> (H) and an <em>at bat</em> (AB). Singles, doubles, triples, and home runs are hits. The fifth way to be successful, BB, is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate. Today this success rate ranges from 20% to 38%. We refer to the batting average in thousands so, for example, if your success rate is 28%, we call it <em>batting 280</em>.</p>
<p><img src="images/JumboTron.png" width="360" /> (Picture courtesy of <a href="https://www.flickr.com/people/27003603@N00">Keith Allison</a>. <a href="https://creativecommons.org/licenses/by-sa/2.0">CC BY-SA 2.0 license</a>.)</p>
<p>One of Bill James’ first important insights is that the batting average ignores BB, but a BB is a success. He proposed we use the <em>on base percentage</em> (OBP) instead of batting average. He defined OBP as (H+BB)/(AB+BB) which is simply the proportion of plate appearances that don’t result in an out, a very intuitive measure. He noted that a player that gets many more BB than the average player might not be recognized if the batter does not excel in batting average. But is this player not helping produce runs? No award is given to the player with the most BB. However, bad habits are hard to break and baseball did not immediately adopt OBP as an important statistic. In contrast, total stolen bases were considered important and an <a href="http://www.baseball-almanac.com/awards/lou_brock_award.shtml">award</a> given to the player with the most. But players with high totals of SB also made more outs as they did not always succeed. Does a player with high SB total help produce runs? Can we use data science to determine if it’s better to pay for players with high BB or SB?</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The goal of a baseball game is to score more runs (points) than the other team.</p></li>
<li><p>Each team has 9 batters who have an opportunity to hit a ball with a bat in a predetermined order.</p></li>
<li><p>Each time a batter has an opportunity to bat, we call it a plate appearance (PA).</p></li>
<li><p>The PA ends with a binary outcome: the batter either makes an out (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases).</p></li>
<li><p>We are simplifying a bit, but there are five ways a batter can succeed (not make an out):</p>
<ul>
<li><p>Bases on balls (BB): the pitcher fails to throw the ball through a predefined area considered to be hittable (the strike zone), so the batter is permitted to go to first base.</p></li>
<li><p>Single: the batter hits the ball and gets to first base.</p></li>
<li><p>Double (2B): the batter hits the ball and gets to second base.</p></li>
<li><p>Triple (3B): the batter hits the ball and gets to third base.</p></li>
<li><p>Home Run (HR): the batter hits the ball and goes all the way home and scores a run.</p></li>
</ul></li>
<li><p>Historically, the batting average has been considered the most important offensive statistic. To define this average, we define a hit (H) and an at bat (AB). Singles, doubles, triples and home runs are hits. The fifth way to be successful, a walk (BB), is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate.</p></li>
</ul>
</div>
</div>
<div id="section-bases-on-balls-or-stolen-bases" class="section level3">
<h3>Bases on Balls or Stolen Bases?</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#base-on-balls-or-stolen-bases">textbook section - 18.1.4 Base on balls or stolen bases?</a>.</p>
</div>
<p>One of the challenges in this analysis is that it is not obvious how to determine if a player produces runs because so much depends on his teammates. We do keep track of the number of runs scored by a player. However, remember that if a player X bats right before someone who hits many HRs, batter X will score many runs. But these runs don’t necessarily happen if we hire player X but not his HR hitting teammate. However, we can examine team-level statistics. How do teams with many SB compare to teams with few? How about BB? We have data! Let’s examine some.</p>
<p>Let’s start with an obvious one: HRs. Do teams that hit more home runs score more runs? We examine data from 1961 to 2001. The visualization of choice when exploring the relationship between two variables, such as HRs and wins, is a scatterplot:</p>
<pre class="r"><code>library(Lahman)

Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;%
  mutate(HR_per_game = HR / G, R_per_game = R / G) %&gt;%
  ggplot(aes(HR_per_game, R_per_game)) + 
  geom_point(alpha = 0.5)</code></pre>
<p><img src="datsci_07_files/figure-html/runs-vs-hrs-1.png" width="624" /></p>
<p>The plot shows a strong association: teams with more HRs tend to score more runs. Now let’s examine the relationship between stolen bases and runs:</p>
<pre class="r"><code>Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;%
  mutate(SB_per_game = SB / G, R_per_game = R / G) %&gt;%
  ggplot(aes(SB_per_game, R_per_game)) + 
  geom_point(alpha = 0.5)</code></pre>
<p><img src="datsci_07_files/figure-html/runs-vs-sb-1.png" width="624" /></p>
<p>Here the relationship is not as clear. Finally, let’s examine the relationship between BB and runs:</p>
<pre class="r"><code>Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;%
  mutate(BB_per_game = BB/G, R_per_game = R/G) %&gt;%
  ggplot(aes(BB_per_game, R_per_game)) + 
  geom_point(alpha = 0.5)</code></pre>
<p><img src="datsci_07_files/figure-html/runs-vs-bb-1.png" width="624" /></p>
<p>Here again we see a clear association. But does this mean that increasing a team’s BBs <strong>causes</strong> an increase in runs? One of the most important lessons you learn in this course is that <strong>association is not causation.</strong></p>
<p>In fact, it looks like BBs and HRs are also associated:</p>
<pre class="r"><code>Teams %&gt;% filter(yearID %in% 1961:2001 ) %&gt;%
  mutate(HR_per_game = HR/G, BB_per_game = BB/G) %&gt;%
  ggplot(aes(HR_per_game, BB_per_game)) + 
  geom_point(alpha = 0.5)</code></pre>
<p><img src="datsci_07_files/figure-html/bb-vs-hrs-1.png" width="624" /></p>
<p>We know that HRs cause runs because, as the name “home run” implies, when a player hits a HR they are guaranteed at least one run. Could it be that HRs also cause BB and this makes it appear as if BB cause runs? When this happens we say there is <em>confounding</em>, an important concept we will learn more about throughout this section.</p>
<p>Linear regression will help us parse all this out and quantify the associations. This will then help us determine what players to recruit. Specifically, we will try to predict things like how many more runs will a team score if we increase the number of BBs, but keep the HRs fixed? Regression will help us answer questions like this one.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>The visualization of choice when exploring the relationship between two variables like home runs and runs is a scatterplot.</li>
</ul>
</div>
</div>
<div id="section-assessment-baseball-as-a-motivating-example" class="section level3">
<h3>1.1 Assessment: Baseball as a Motivating Example</h3>
<p>Insert assessment here</p>
</div>
<div id="section-correlation" class="section level3">
<h3>Correlation</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/regression.html#case-study-is-height-hereditary">textbook section - 17.1 Case study: is height hereditary?</a>.</p>
</div>
<p>Up to this point, this course has focused mainly on single variables. However, in data science applications, it is very common to be interested in the relationship between two or more variables. For instance, in the textbook <a href="https://rafalab.github.io/dsbook/linear-models.html">(Section - 18 Linear models)</a> we will use a data-driven approach that examines the relationship between player statistics and success to guide the building of a baseball team with a limited budget. Before delving into this more complex example, we introduce necessary concepts needed to understand regression using a simpler illustration. We actually use the dataset from which regression was born.</p>
<p>The example is from genetics. <a href="https://en.wikipedia.org/wiki/Francis_Galton">Francis Galton</a> studied the variation and heredity of human traits. Among many other traits, Galton collected and studied height data from families to try to understand heredity. While doing this, he developed the concepts of correlation and regression, as well as a connection to pairs of data that follow a normal distribution. Of course, at the time this data was collected our knowledge of genetics was quite limited compared to what we know today. A very specific question Galton tried to answer was: how well can we predict a child’s height based on the parents’ height? The technique he developed to answer this question, regression, can also be applied to our baseball question. Regression can be applied in many other circumstances as well.</p>
<p><strong>Historical note</strong>: Galton made important contributions to statistics and genetics, but he was also one of the first proponents of eugenics, a scientifically flawed philosophical movement favored by many biologists of Galton’s time but with horrific historical consequences. You can read more about it here: <a href="https://pged.org/history-eugenics-and-genetics/">https://pged.org/history-eugenics-and-genetics/</a>.</p>
</div>
<div id="section-case-study-is-height-hereditary" class="section level3">
<h3>Case study: is height hereditary?</h3>
<p>We have access to Galton’s family height data through the <strong>HistData</strong> package. This data contains heights on several dozen families: mothers, fathers, daughters, and sons. To imitate Galton’s analysis, we will create a dataset with the heights of fathers and a randomly selected son of each family:</p>
<pre class="r"><code>library(tidyverse)
library(HistData)
data(&quot;GaltonFamilies&quot;)

set.seed(1983)
galton_heights &lt;- GaltonFamilies %&gt;%
  filter(gender == &quot;male&quot;) %&gt;%
  group_by(family) %&gt;%
  sample_n(1) %&gt;%
  ungroup() %&gt;%
  select(father, childHeight) %&gt;%
  rename(son = childHeight)</code></pre>
<p>In the exercises, we will look at other relationships including mothers and daughters.</p>
<p>Suppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:</p>
<pre class="r"><code>galton_heights %&gt;% 
  summarize(mean(father), sd(father), mean(son), sd(son))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["mean(father)"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["sd(father)"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["mean(son)"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["sd(son)"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"69.09888","2":"2.546555","3":"69.16927","4":"2.710965"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>However, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son.</p>
<pre class="r"><code>galton_heights %&gt;% ggplot(aes(father, son)) + 
  geom_point(alpha = 0.5)</code></pre>
<p><img src="datsci_07_files/figure-html/scatterplot-1.png" width="40%" /></p>
<p>We will learn that the correlation coefficient is an informative summary of how two variables move together and then see how this can be used to predict one variable using the other.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Galton tried to predict sons’ heights based on fathers’ heights.</p></li>
<li><p>The mean and standard errors are insufficient for describing an important characteristic of the data: the trend that the taller the father, the taller the son.</p></li>
<li><p>The correlation coefficient is an informative summary of how two variables move together that can be used to predict one variable using the other.</p></li>
</ul>
</div>
</div>
<div id="section-correlation-coefficient" class="section level3">
<h3>Correlation Coefficient</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/regression.html#the-correlation-coefficient">textbook section - 17 Regression</a>.</p>
</div>
<p>The correlation coefficient is defined for a list of pairs <span class="math inline">\((x_1, y_1), \dots, (x_n,y_n)\)</span> as the average of the product of the standardized values:</p>
<p><span class="math display">\[
\rho = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)
\]</span> with <span class="math inline">\(\mu_x, \mu_y\)</span> the averages of <span class="math inline">\(x_1,\dots, x_n\)</span> and <span class="math inline">\(y_1, \dots, y_n\)</span>, respectively, and <span class="math inline">\(\sigma_x, \sigma_y\)</span> the standard deviations. The Greek letter <span class="math inline">\(\rho\)</span> is commonly used in statistics books to denote the correlation. The Greek letter for <span class="math inline">\(r\)</span>, <span class="math inline">\(\rho\)</span>, because it is the first letter of regression. Soon we learn about the connection between correlation and regression. We can represent the formula above with R code using:</p>
<pre class="r"><code>rho &lt;- mean(scale(x) * scale(y))</code></pre>
<p>To understand why this equation does in fact summarize how two variables move together, consider the <span class="math inline">\(i\)</span>-th entry of <span class="math inline">\(x\)</span> is <span class="math inline">\(\left( \frac{x_i-\mu_x}{\sigma_x} \right)\)</span> SDs away from the average. Similarly, the <span class="math inline">\(y_i\)</span> that is paired with <span class="math inline">\(x_i\)</span>, is <span class="math inline">\(\left( \frac{y_1-\mu_y}{\sigma_y} \right)\)</span> SDs away from the average <span class="math inline">\(y\)</span>. If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are unrelated, the product <span class="math inline">\(\left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)\)</span> will be positive ( <span class="math inline">\(+ \times +\)</span> and <span class="math inline">\(- \times -\)</span> ) as often as negative (<span class="math inline">\(+ \times -\)</span> and <span class="math inline">\(- \times +\)</span>) and will average out to about 0. This correlation is the average and therefore unrelated variables will have 0 correlation. If instead the quantities vary together, then we are averaging mostly positive products ( <span class="math inline">\(+ \times +\)</span> and <span class="math inline">\(- \times -\)</span>) and we get a positive correlation. If they vary in opposite directions, we get a negative correlation.</p>
<p>The correlation coefficient is always between -1 and 1. We can show this mathematically: consider that we can’t have higher correlation than when we compare a list to itself (perfect correlation) and in this case the correlation is:</p>
<p><span class="math display">\[
\rho = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i-\mu_x}{\sigma_x} \right)^2 = 
\frac{1}{\sigma_x^2} \frac{1}{n} \sum_{i=1}^n \left( x_i-\mu_x \right)^2 = 
\frac{1}{\sigma_x^2} \sigma^2_x = 
1
\]</span></p>
<p>A similar derivation, but with <span class="math inline">\(x\)</span> and its exact opposite, proves the correlation has to be bigger or equal to -1.</p>
<p>For other pairs, the correlation is in between -1 and 1. The correlation between father and son’s heights is about 0.5:</p>
<pre class="r"><code>galton_heights %&gt;% summarize(r = cor(father, son)) %&gt;% pull(r)</code></pre>
<pre><code>## [1] 0.4334102</code></pre>
<p>To see what data looks like for different values of <span class="math inline">\(\rho\)</span>, here are six examples of pairs with correlations ranging from -0.9 to 0.99:</p>
<p><img src="datsci_07_files/figure-html/what-correlation-looks-like-1.png" width="624" /></p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The correlation coefficient is defined for a list of pairs <span class="math inline">\((x_1, y_1), \dots, (x_n,y_n)\)</span> as the product of the standardized values: <span class="math inline">\(\left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)\)</span>.</p></li>
<li><p>The correlation coefficient essentially conveys how two variables move together.</p></li>
<li><p>The correlation coefficient is always between -1 and 1.</p></li>
</ul>
</div>
</div>
<div id="section-sample-correlation-is-a-random-variable" class="section level3">
<h3>Sample Correlation is a Random Variable</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/regression.html#sample-correlation-is-a-random-variable">textbook section - 17.2.1 Sample correlation is a random variable</a>.</p>
</div>
<p>Before we continue connecting correlation to regression, let’s remind ourselves about random variability.</p>
<p>In most data science applications, we observe data that includes random variation. For example, in many cases, we do not observe data for the entire population of interest but rather for a random sample. As with the average and standard deviation, the <em>sample correlation</em> is the most commonly used estimate of the population correlation. This implies that the correlation we compute and use as a summary is a random variable.</p>
<p>By way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population. A less fortunate geneticist can only afford measurements from a random sample of 25 pairs. The sample correlation can be computed with:</p>
<pre class="r"><code>R &lt;- sample_n(galton_heights, 25, replace = TRUE) %&gt;% 
  summarize(r = cor(father, son)) %&gt;% pull(r)</code></pre>
<p><code>R</code> is a random variable. We can run a Monte Carlo simulation to see its distribution:</p>
<pre class="r"><code>B &lt;- 1000
N &lt;- 25
R &lt;- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %&gt;% 
    summarize(r=cor(father, son)) %&gt;% 
    pull(r)
})
qplot(R, geom = &quot;histogram&quot;, binwidth = 0.05, color = I(&quot;black&quot;))</code></pre>
<p><img src="datsci_07_files/figure-html/sample-correlation-distribution-1.png" width="624" /></p>
<p>We see that the expected value of <code>R</code> is the population correlation:</p>
<pre class="r"><code>mean(R)</code></pre>
<pre><code>## [1] 0.4307393</code></pre>
<p>and that it has a relatively high standard error relative to the range of values <code>R</code> can take:</p>
<pre class="r"><code>sd(R)</code></pre>
<pre><code>## [1] 0.1609393</code></pre>
<p>So, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.</p>
<p>Also, note that because the sample correlation is an average of independent draws, the central limit actually applies. Therefore, for large enough <span class="math inline">\(N\)</span>, the distribution of <code>R</code> is approximately normal with expected value <span class="math inline">\(\rho\)</span>. The standard deviation, which is somewhat complex to derive, is <span class="math inline">\(\sqrt{\frac{1-r^2}{N-2}}\)</span>.</p>
<p>In our example, <span class="math inline">\(N=25\)</span> does not seem to be large enough to make the approximation a good one:</p>
<pre class="r"><code>ggplot(aes(sample=R), data = data.frame(R)) + 
  stat_qq() + 
  geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2)))</code></pre>
<p><img src="datsci_07_files/figure-html/small-sample-correlation-not-normal-1.png" width="40%" /></p>
<p>If you increase <span class="math inline">\(N\)</span>, you will see the distribution converging to normal.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The correlation that we compute and use as a summary is a random variable.</p></li>
<li><p>When interpreting correlations, it is important to remember that correlations derived from samples are estimates containing uncertainty.</p></li>
<li><p>Because the sample correlation is an average of independent draws, the central limit theorem applies.</p></li>
</ul>
</div>
</div>
<div id="section-assessment-correlation" class="section level3">
<h3>1.2 Assessment: Correlation</h3>
<p>Insert assessment here</p>
</div>
<div id="section-anscombes-quartetstratification" class="section level3">
<h3>Anscombe’s Quartet/Stratification</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the textbook sections</p>
<ul>
<li><p><a href="https://rafalab.github.io/dsbook/regression.html#correlation-is-not-always-a-useful-summary">17.2.2 Correlation is not always a useful summary</a></p></li>
<li><p><a href="https://rafalab.github.io/dsbook/regression.html#conditional-expectation">17.3 Conditional expectations</a></p></li>
<li><p><a href="https://rafalab.github.io/dsbook/regression.html#the-regression-line">17.4 The regression line</a></p></li>
</ul>
</div>
<p>Correlation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82:</p>
<p><img src="datsci_07_files/figure-html/ascombe-quartet-1.png" width="624" /></p>
<p>Correlation is only meaningful in a particular context. To help us understand when it is that correlation is meaningful as a summary statistic, we will return to the example of predicting a son’s height using his father’s height. This will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction.</p>
</div>
<div id="section-conditional-expectations" class="section level3">
<h3>Conditional expectations</h3>
<p>Suppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height. Because the distribution of sons’ heights is approximately normal, we know the average height, 69.2, is the value with the highest proportion and would be the prediction with the highest chance of minimizing the error. But what if we are told that the father is taller than average, say 72 inches tall, do we still guess 69.2 for the son?</p>
<p>It turns out that if we were able to collect data from a very large number of fathers that are 72 inches, the distribution of their sons’ heights would be normally distributed. This implies that the average of the distribution computed on this subset would be our best prediction.</p>
<p>In general, we call this approach <em>conditioning</em>. The general idea is that we stratify a population into groups and compute summaries in each group. Conditioning is therefore related to the concept of stratification described in the textbook <a href="https://rafalab.github.io/dsbook/distributions.html#stratification">(Section - 8.13 Stratification)</a>. To provide a mathematical description of conditioning, consider we have a population of pairs of values <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span>, for example all father and son heights in England. In the previous section we learned that if you take a random pair <span class="math inline">\((X,Y)\)</span>, the expected value and best predictor of <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mbox{E}(Y) = \mu_y\)</span>, the population average <span class="math inline">\(1/n\sum_{i=1}^n y_i\)</span>. However, we are no longer interested in the general population, instead we are interested in only the subset of a population with a specific <span class="math inline">\(x_i\)</span> value, 72 inches in our example. This subset of the population, is also a population and thus the same principles and properties we have learned apply. The <span class="math inline">\(y_i\)</span> in the subpopulation have a distribution, referred to as the <em>conditional distribution</em>, and this distribution has an expected value referred to as the <em>conditional expectation</em>. In our example, the conditional expectation is the average height of all sons in England with fathers that are 72 inches. The statistical notation for the conditional expectation is</p>
<p><span class="math display">\[
\mbox{E}(Y \mid X = x)
\]</span></p>
<p>with <span class="math inline">\(x\)</span> representing the fixed value that defines that subset, for example 72 inches. Similarly, we denote the standard deviation of the strata with</p>
<p><span class="math display">\[
\mbox{SD}(Y \mid X = x) = \sqrt{\mbox{Var}(Y \mid X = x)}
\]</span></p>
<p>Because the conditional expectation <span class="math inline">\(E(Y\mid X=x)\)</span> is the best predictor for the random variable <span class="math inline">\(Y\)</span> for an individual in the strata defined by <span class="math inline">\(X=x\)</span>, many data science challenges reduce to estimating this quantity. The conditional standard deviation quantifies the precision of the prediction.</p>
<p>In the example we have been considering, we are interested in computing the average son height <em>conditioned</em> on the father being 72 inches tall. We want to estimate <span class="math inline">\(E(Y|X=72)\)</span> using the sample collected by Galton. We previously learned that the sample average is the preferred approach to estimating the population average. However, a challenge when using this approach to estimating conditional expectations is that for continuous data we don’t have many data points matching exactly one value in our sample. For example, we have only:</p>
<pre class="r"><code>sum(galton_heights$father == 72)</code></pre>
<pre><code>## [1] 8</code></pre>
<p>fathers that are exactly 72-inches. If we change the number to 72.5, we get even fewer data points:</p>
<pre class="r"><code>sum(galton_heights$father == 72.5)</code></pre>
<pre><code>## [1] 1</code></pre>
<p>A practical way to improve these estimates of the conditional expectations, is to define strata of with similar values of <span class="math inline">\(x\)</span>. In our example, we can round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall:</p>
<pre class="r"><code>conditional_avg &lt;- galton_heights %&gt;% 
  filter(round(father) == 72) %&gt;%
  summarize(avg = mean(son)) %&gt;% 
  pull(avg)
conditional_avg</code></pre>
<pre><code>## [1] 70.5</code></pre>
<p>Note that a 72-inch father is taller than average – specifically, 72 - 69.1/2.5 = 1.1 standard deviations taller than the average father. Our prediction 70.5 is also taller than average, but only 0.49 standard deviations larger than the average son. The sons of 72-inch fathers have <em>regressed</em> some to the average height. We notice that the reduction in how many SDs taller is about 0.5, which happens to be the correlation. As we will see in a later section, this is not a coincidence.</p>
<p>If we want to make a prediction of any height, not just 72, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group:</p>
<pre class="r"><code>galton_heights %&gt;% mutate(father_strata = factor(round(father))) %&gt;% 
  ggplot(aes(father_strata, son)) + 
  geom_boxplot() + 
  geom_point()</code></pre>
<p><img src="datsci_07_files/figure-html/boxplot-1-1.png" width="40%" /></p>
<p>Not surprisingly, the centers of the groups are increasing with height. Furthermore, these centers appear to follow a linear relationship. Below we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:</p>
<p><img src="datsci_07_files/figure-html/conditional-averages-follow-line-1.png" width="40%" /></p>
<p>The fact that these conditional averages follow a line is not a coincidence. In the next section, we explain that the line these averages follow is what we call the <em>regression line</em>, which improves the precision of our estimates. However, it is not always appropriate to estimate conditional expectations with the regression line so we also describe Galton’s theoretical justification for using the regression line.</p>
</div>
<div id="section-the-regression-line" class="section level3">
<h3>The regression line</h3>
<p>If we are predicting a random variable <span class="math inline">\(Y\)</span> knowing the value of another <span class="math inline">\(X=x\)</span> using a regression line, then we predict that for every standard deviation, <span class="math inline">\(\sigma_X\)</span>, that <span class="math inline">\(x\)</span> increases above the average <span class="math inline">\(\mu_X\)</span>, <span class="math inline">\(Y\)</span> increase <span class="math inline">\(\rho\)</span> standard deviations <span class="math inline">\(\sigma_Y\)</span> above the average <span class="math inline">\(\mu_Y\)</span> with <span class="math inline">\(\rho\)</span> the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The formula for the regression is therefore:</p>
<p><span class="math display">\[ 
\left( \frac{Y-\mu_Y}{\sigma_Y} \right) = \rho \left( \frac{x-\mu_X}{\sigma_X} \right)
\]</span></p>
<p>We can rewrite it like this:</p>
<p><span class="math display">\[ 
Y = \mu_Y + \rho \left( \frac{x-\mu_X}{\sigma_X} \right) \sigma_Y
\]</span></p>
<p>If there is perfect correlation, the regression line predicts an increase that is the same number of SDs. If there is 0 correlation, then we don’t use <span class="math inline">\(x\)</span> at all for the prediction and simply predict the average <span class="math inline">\(\mu_Y\)</span>. For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.</p>
<p>Note that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, <span class="math inline">\(x\)</span>, is to the average of the <span class="math inline">\(x\)</span>s. This is why we call it <em>regression</em>: the son regresses to the average height. In fact, the title of Galton’s paper was: <em>Regression toward mediocrity in hereditary stature</em>. To add regression lines to plots, we will need the above formula in the form:</p>
<p><span class="math display">\[
y= b + mx \mbox{ with slope } m = \rho \frac{\sigma_y}{\sigma_x} \mbox{ and intercept } b=\mu_y - m \mu_x
\]</span></p>
<p>Here we add the regression line to the original data:</p>
<pre class="r"><code>mu_x &lt;- mean(galton_heights$father)
mu_y &lt;- mean(galton_heights$son)
s_x &lt;- sd(galton_heights$father)
s_y &lt;- sd(galton_heights$son)
r &lt;- cor(galton_heights$father, galton_heights$son)

galton_heights %&gt;% 
  ggplot(aes(father, son)) + 
  geom_point(alpha = 0.5) +
  geom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x) </code></pre>
<p><img src="datsci_07_files/figure-html/regression-line-1.png" width="40%" /></p>
<p>The regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation <span class="math inline">\(\rho\)</span>. You can make same plot, but using standard units like this:</p>
<pre class="r"><code>galton_heights %&gt;% 
  ggplot(aes(scale(father), scale(son))) + 
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = r) </code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Correlation is not always a good summary of the relationship between two variables.</p></li>
<li><p>The general idea of conditional expectation is that we stratify a population into groups and compute summaries in each group.</p></li>
<li><p>A practical way to improve the estimates of the conditional expectations is to define strata of with similar values of x.</p></li>
<li><p>If there is perfect correlation, the regression line predicts an increase that is the same number of SDs for both variables. If there is 0 correlation, then we don’t use x at all for the prediction and simply predict the average 𝜇𝑦. For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.</p></li>
</ul>
</div>
</div>
<div id="section-bivariate-normal-distribution" class="section level3">
<h3>Bivariate Normal Distribution</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/regression.html#bivariate-normal-distribution-advanced">textbook section - 17.4.2 Bivariate normal distribution (advanced)</a>.</p>
</div>
<p>Correlation and the regression slope are a widely used summary statistic, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases of dataset in which summarizing with correlation would be a mistake. But there are many more real-life examples.</p>
<p>The main way we motivate the use of correlation involves what is called the <em>bivariate normal distribution</em>.</p>
<p>When a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals. As we saw in the textbook <a href="https://rafalab.github.io/dsbook/regression.html#corr-coef">(Section - 17.2 The correlation coefficient)</a>, they can be thin (high correlation) or circle-shaped (no correlation).</p>
<!--
<img src="datsci_07_files/figure-html/bivariate-ovals-1.png" width="624" />
-->
<p>A more technical way to define the bivariate normal distribution is the following: if <span class="math inline">\(X\)</span> is a normally distributed random variable, <span class="math inline">\(Y\)</span> is also a normally distributed random variable, and the conditional distribution of <span class="math inline">\(Y\)</span> for any <span class="math inline">\(X=x\)</span> is approximately normal, then the pair is approximately bivariate normal.</p>
<p>If we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold:</p>
<pre class="r"><code>galton_heights %&gt;%
  mutate(z_father = round((father - mean(father)) / sd(father))) %&gt;%
  filter(z_father %in% -2:2) %&gt;%
  ggplot() +  
  stat_qq(aes(sample = son)) +
  facet_wrap( ~ z_father) </code></pre>
<p><img src="datsci_07_files/figure-html/qqnorm-of-strata-1.png" width="624" /></p>
<p>Now we come back to defining correlation. Galton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of <span class="math inline">\(x\)</span>, the expected value of the <span class="math inline">\(Y\)</span> in pairs for which <span class="math inline">\(X=x\)</span> is:</p>
<p><span class="math display">\[ 
\mbox{E}(Y | X=x) = \mu_Y +  \rho \frac{X-\mu_X}{\sigma_X}\sigma_Y
\]</span></p>
<p>This is the regression line, with slope <span class="math display">\[\rho \frac{\sigma_Y}{\sigma_X}\]</span> and intercept <span class="math inline">\(\mu_y - m\mu_X\)</span>. It is equivalent to the regression equation we showed earlier which can be written like this:</p>
<p><span class="math display">\[
\frac{\mbox{E}(Y \mid X=x)  - \mu_Y}{\sigma_Y} = \rho \frac{x-\mu_X}{\sigma_X}
\]</span></p>
<p>This implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict.</p>
<p>In summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of <span class="math inline">\(Y\)</span> given we know the value of <span class="math inline">\(X\)</span>, is given by the regression line.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>When a pair of random variables are approximated by the bivariate normal distribution, scatterplots look like ovals. They can be thin (high correlation) or circle-shaped (no correlation).</p></li>
<li><p>When two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations.</p></li>
<li><p>We can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to make predictions.</p></li>
</ul>
</div>
</div>
<div id="section-variance-explained" class="section level3">
<h3>Variance Explained</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/regression.html#variance-explained">textbook section - 17.4.3 Variance explained</a>.</p>
</div>
<p>The bivariate normal theory also tells us that the standard deviation of the <em>conditional</em> distribution described above is:</p>
<p><span class="math display">\[
\mbox{SD}(Y \mid X=x ) = \sigma_Y \sqrt{1-\rho^2} 
\]</span></p>
<p>To see why this is intuitive, notice that without conditioning, <span class="math inline">\(\mbox{SD}(Y) = \sigma_Y\)</span>, we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with a tall, 72-inch, father. This group will all tend to be somewhat tall so the standard deviation is reduced.</p>
<p>Specifically, it is reduced to <span class="math inline">\(\sqrt{1-\rho^2} = \sqrt{1 - 0.25}\)</span> = 0.87 of what it was originally. We could say that father heights “explain” 13% of the variability observed in son heights.</p>
<p>The statement “<span class="math inline">\(X\)</span> explains such and such percent of the variability” is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by <span class="math inline">\(1-\rho^2\)</span>, so we say that <span class="math inline">\(X\)</span> explains <span class="math inline">\(1- (1-\rho^2)=\rho^2\)</span> (the correlation squared) of the variance.</p>
<p>But it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Conditioning on a random variable X can help to reduce variance of response variable Y.</p></li>
<li><p>The standard deviation of the conditional distribution is <span class="math inline">\(\mbox{SD}(Y \mid X=x ) = \sigma_Y \sqrt{1-\rho^2}\)</span>, which is smaller than the standard deviation without conditioning <span class="math inline">\(\sigma_Y\)</span>.</p></li>
<li><p>Because variance is the standard deviation squared, the variance of the conditional distribution is <span class="math inline">\(\sigma_y^2(1-\rho^2)\)</span>.</p></li>
<li><p>In the statement “X explains such and such percent of the variability,” the percent value refers to the variance. The variance decreases by <span class="math inline">\(\rho^2\)</span> percent.</p></li>
<li><p>The “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution.</p></li>
</ul>
</div>
</div>
<div id="section-there-are-two-regression-lines" class="section level3">
<h3>There are Two Regression Lines</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/regression.html#warning-there-are-two-regression-lines">textbook section - 17.4.4 Warning: there are two regression lines</a>.</p>
</div>
<p>We computed a regression line to predict the son’s height from father’s height. We used these calculations:</p>
<pre class="r"><code>mu_x &lt;- mean(galton_heights$father)
mu_y &lt;- mean(galton_heights$son)
s_x &lt;- sd(galton_heights$father)
s_y &lt;- sd(galton_heights$son)
r &lt;- cor(galton_heights$father, galton_heights$son)
m_1 &lt;-  r * s_y / s_x
b_1 &lt;- mu_y - m_1*mu_x</code></pre>
<p>which gives us the function <span class="math inline">\(\mbox{E}(Y\mid X=x) =\)</span> 37.3 + 0.46 <span class="math inline">\(x\)</span>.</p>
<p>What if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function: <span class="math inline">\(x = \{ \mbox{E}(Y\mid X=x) -\)</span> 37.3 <span class="math inline">\(\} /\)</span> 0.5.</p>
<p>We need to compute <span class="math inline">\(\mbox{E}(X \mid Y=y)\)</span>. Since the data is approximately bivariate normal, the theory described above tells us that this conditional expectation will follow a line with slope and intercept:</p>
<pre class="r"><code>m_2 &lt;-  r * s_x / s_y
b_2 &lt;- mu_x - m_2 * mu_y</code></pre>
<p>So we get <span class="math inline">\(\mbox{E}(X \mid Y=y) =\)</span> 40.9 + 0.41y. Again we see regression to the average: the prediction for the father is closer to the father average than the son heights <span class="math inline">\(y\)</span> is to the son average.</p>
<p>Here is a plot showing the two regression lines, with blue for the predicting son heights with father heights and red for predicting father heights with son heights:</p>
<pre class="r"><code>galton_heights %&gt;% 
  ggplot(aes(father, son)) + 
  geom_point(alpha = 0.5) + 
  geom_abline(intercept = b_1, slope = m_1, col = &quot;blue&quot;) +
  geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = &quot;red&quot;) </code></pre>
<p><img src="datsci_07_files/figure-html/two-regression-lines-1.png" width="40%" /></p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>There are two different regression lines depending on whether we are taking the expectation of Y given X or taking the expectation of X given Y.</li>
</ul>
</div>
</div>
<div id="section-assessment-stratification-and-variance-explained-part-1" class="section level3">
<h3>1.3 Assessment: Stratification and Variance Explained, Part 1</h3>
<p>Insert assessment here</p>
</div>
<div id="section-assessment-stratification-and-variance-explained-part-2" class="section level3">
<h3>1.3 Assessment: Stratification and Variance Explained, Part 2</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-2-linear-models" class="section level2">
<h2>Section 2: Linear Models</h2>
<p>In the <strong>Linear Models</strong> section, you will learn how to do linear regression.</p>
<p>After completing this section, you will be able to:</p>
<ul>
<li><p>Use <strong>multivariate regression</strong> to adjust for confounders.</p></li>
<li><p>Write <strong>linear models</strong> to describe the relationship between two or more variables.</p></li>
<li><p>Calculate the <strong>least squares estimates</strong> for a regression model using the <code>lm</code> function.</p></li>
<li><p>Understand the differences between <strong>tibbles</strong> and <strong>data frames</strong>.</p></li>
<li><p>Use the <code>do()</code> function to bridge <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> functions and the tidyverse.</p></li>
<li><p>Use the <code>tidy()</code>, <code>glance()</code>, and <code>augment()</code> functions from the broom package.</p></li>
<li><p>Apply linear regression to <strong>measurement error models</strong>.</p></li>
</ul>
<p>This section has four parts: <strong>Introduction to Linear Models, Least Squares Estimates, Tibbles, do, and broom, and Regression and Baseball</strong>. There are comprehension checks at the end of each part, along with an assessment on linear models at the end of the whole section.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your own learning.</p>
<div id="section-confounding-are-bbs-more-predictive" class="section level3">
<h3>Confounding: Are BBs More Predictive?</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#confounding">textbook section - 18.2 Confounding</a>.</p>
</div>
<p>Previously, we noted a strong relationship between Runs and BB. If we find the regression line for predicting runs from bases on balls, we a get slope of:</p>
<pre class="r"><code>library(tidyverse)
library(Lahman)
get_slope &lt;- function(x, y) cor(x, y) * sd(y) / sd(x)

bb_slope &lt;- Teams %&gt;% 
  filter(yearID %in% 1961:2001 ) %&gt;% 
  mutate(BB_per_game = BB/G, R_per_game = R/G) %&gt;% 
  summarize(slope = get_slope(BB_per_game, R_per_game))

bb_slope </code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["slope"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.7353288"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
So does this mean that if we go and hire low salary players with many BB, and who therefore increase the number of walks per game by 2, our team will score
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["slope"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"1.5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>more runs per game?</p>
We are again reminded that association is not causation. The data does provide strong evidence that a team with two more BB per game than the average team, scores
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["slope"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"1.5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>runs per game. But this does not mean that BB are the cause.</p>
<p>Note that if we compute the regression line slope for singles we get:</p>
<pre class="r"><code>singles_slope &lt;- Teams %&gt;% 
  filter(yearID %in% 1961:2001 ) %&gt;%
  mutate(Singles_per_game = (H-HR-X2B-X3B)/G, R_per_game = R/G) %&gt;%
  summarize(slope = get_slope(Singles_per_game, R_per_game))

singles_slope </code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["slope"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.4494253"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>which is a lower value than what we obtain for BB.</p>
<p>Also, notice that a single gets you to first base just like a BB. Those that know about baseball will tell you that with a single, runners on base have a better chance of scoring than with a BB. So how can BB be more predictive of runs? The reason this happen is because of confounding. Here we show the correlation between HR, BB, and singles:</p>
<pre class="r"><code>Teams %&gt;% 
  filter(yearID %in% 1961:2001 ) %&gt;% 
  mutate(Singles = (H-HR-X2B-X3B)/G, BB = BB/G, HR = HR/G) %&gt;%  
  summarize(cor(BB, HR), cor(Singles, HR), cor(BB, Singles))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["cor(BB, HR)"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["cor(Singles, HR)"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["cor(BB, Singles)"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.4039313","2":"-0.1737435","3":"-0.05603822"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>It turns out that pitchers, afraid of HRs, will sometimes avoid throwing strikes to HR hitters. As a result, HR hitters tend to have more BBs and a team with many HRs will also have more BBs. Although it may appear that BBs cause runs, it is actually the HRs that cause most of these runs. We say that BBs are <em>confounded</em> with HRs. Nonetheless, could it be that BBs still help? To find out, we somehow have to adjust for the HR effect. Regression can help with this as well.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Association is not causation!</p></li>
<li><p>Although it may appear that BB cause runs, it is actually the HR that cause most of these runs. We say that BB are <strong>confounded</strong> with HR.</p></li>
<li><p>Regression can help us account for confounding.</p></li>
</ul>
</div>
</div>
<div id="section-stratification-and-multivariate-regression" class="section level3">
<h3>Stratification and Multivariate Regression</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#understanding-confounding-through-stratification">textbook section - 18.2.1 Multivariate regression</a>.</p>
</div>
<p>A first approach is to keep HRs fixed at a certain value and then examine the relationship between BB and runs. As we did when we stratified fathers by rounding to the closest inch, here we can stratify HR per game to the closest ten. We filter out the strata with few points to avoid highly variable estimates:</p>
<pre class="r"><code>dat &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;%
  mutate(HR_strata = round(HR/G, 1), 
         BB_per_game = BB / G,
         R_per_game = R / G) %&gt;%
  filter(HR_strata &gt;= 0.4 &amp; HR_strata &lt;=1.2) </code></pre>
<p>and then make a scatterplot for each strata:</p>
<pre class="r"><code>dat %&gt;% 
  ggplot(aes(BB_per_game, R_per_game)) +  
  geom_point(alpha = 0.5) +
  geom_smooth(method = &quot;lm&quot;) +
  facet_wrap( ~ HR_strata) </code></pre>
<p><img src="datsci_07_files/figure-html/runs-vs-bb-by-hr-strata-1.png" width="80%" /></p>
Remember that the regression slope for predicting runs with BB was
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["slope"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.7"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>. Once we stratify by HR, these slopes are substantially reduced:</p>
<pre class="r"><code>dat %&gt;%  
  group_by(HR_strata) %&gt;%
  summarize(slope = get_slope(BB_per_game, R_per_game))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["HR_strata"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["slope"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.4","2":"0.7342910"},{"1":"0.5","2":"0.5659067"},{"1":"0.6","2":"0.4119129"},{"1":"0.7","2":"0.2853933"},{"1":"0.8","2":"0.3650361"},{"1":"0.9","2":"0.2608882"},{"1":"1.0","2":"0.5115687"},{"1":"1.1","2":"0.4539252"},{"1":"1.2","2":"0.4403274"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
The slopes are reduced, but they are not 0, which indicates that BBs are helpful for producing runs, just not as much as previously thought. In fact, the values above are closer to the slope we obtained from singles,
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["slope"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.45"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>, which is more consistent with our intuition. Since both singles and BB get us to first base, they should have about the same predictive power.</p>
<p>Although our understanding of the application tells us that HR cause BB but not the other way around, we can still check if stratifying by BB makes the effect of BB go down. To do this, we use the same code except that we swap HR and BBs to get this plot:</p>
<p><img src="datsci_07_files/figure-html/runs-vs-hr-by-bb-strata-1.png" width="100%" /></p>
<p>In this case, the slopes do not change much from the original:</p>
<pre class="r"><code>dat %&gt;% group_by(BB_strata) %&gt;%
   summarize(slope = get_slope(HR_per_game, R_per_game))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["BB_strata"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["slope"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"2.8","2":"1.518056"},{"1":"2.9","2":"1.567879"},{"1":"3.0","2":"1.518179"},{"1":"3.1","2":"1.494498"},{"1":"3.2","2":"1.582159"},{"1":"3.3","2":"1.560302"},{"1":"3.4","2":"1.481832"},{"1":"3.5","2":"1.631314"},{"1":"3.6","2":"1.829929"},{"1":"3.7","2":"1.451895"},{"1":"3.8","2":"1.704564"},{"1":"3.9","2":"1.302576"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>They are reduced a bit, which is consistent with the fact that BB do in fact cause some runs.</p>
<pre class="r"><code>hr_slope &lt;- Teams %&gt;% 
  filter(yearID %in% 1961:2001 ) %&gt;% 
  mutate(HR_per_game = HR/G, R_per_game = R/G) %&gt;% 
  summarize(slope = get_slope(HR_per_game, R_per_game))

hr_slope</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["slope"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"1.844824"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Regardless, it seems that if we stratify by HR, we have bivariate distributions for runs versus BB. Similarly, if we stratify by BB, we have approximate bivariate normal distributions for HR versus runs.</p>
</div>
<div id="section-multivariate-regression" class="section level3">
<h3>Multivariate regression</h3>
<p>It is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this:</p>
<p><span class="math display">\[
\mbox{E}[R \mid BB = x_1, \, HR = x_2] = \beta_0 + \beta_1(x_2) x_1 + \beta_2(x_1) x_2
\]</span></p>
<p>with the slopes for <span class="math inline">\(x_1\)</span> changing for different values of <span class="math inline">\(x_2\)</span> and vice versa. But is there an easier approach?</p>
<p>If we take random variability into account, the slopes in the strata don’t appear to change much. If these slopes are in fact the same, this implies that <span class="math inline">\(\beta_1(x_2)\)</span> and <span class="math inline">\(\beta_2(x_1)\)</span> are constants. This in turn implies that the expectation of runs conditioned on HR and BB can be written like this:</p>
<p><span class="math display">\[
\mbox{E}[R \mid BB = x_1, \, HR = x_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span></p>
<p>This model suggests that if the number of HR is fixed at <span class="math inline">\(x_2\)</span>, we observe a linear relationship between runs and BB with an intercept of <span class="math inline">\(\beta_0 + \beta_2 x_2\)</span>. Our exploratory data analysis suggested this. The model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by <span class="math inline">\(\beta_1 x_1\)</span>.</p>
<p>In this analysis, referred to as <em>multivariate regression</em>, you will often hear people say that the BB slope <span class="math inline">\(\beta_1\)</span> is <em>adjusted</em> for the HR effect. If the model is correct then confounding has been accounted for. But how do we estimate <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> from the data? For this, we learn about linear models and least squares estimates.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>A first approach to check confounding is to keep HRs fixed at a certain value and then examine the relationship between BB and runs.</p></li>
<li><p>The slopes of BB after stratifying on HR are reduced, but they are not 0, which indicates that BB are helpful for producing runs, just not as much as previously thought.</p></li>
</ul>
</div>
</div>
<div id="section-linear-models" class="section level3">
<h3>Linear Models</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#lse">textbook section - 18.3 Least squares estimates</a>.</p>
</div>
<p>We have described how if data is bivariate normal then the conditional expectations follow the regression line. The fact that the conditional expectation is a line is not an extra assumption but rather a derived result. However, in practice it is common to explicitly write down a model that describes the relationship between two or more variables using a <em>linear model</em>.</p>
<p>We note that “linear” here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combination of known quantities. In mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables. For example, <span class="math inline">\(3x - 4y + 5z\)</span> is a linear combination of <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span>. We can also add a constant so <span class="math inline">\(2 + 3x - 4y + 5z\)</span> is also linear combination of <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span>.</p>
<p>So <span class="math inline">\(\beta_0 + \beta_1 x_1 + \beta_2 x_2\)</span>, is a linear combination of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. The simplest linear model is a constant <span class="math inline">\(\beta_0\)</span>; the second simplest is a line <span class="math inline">\(\beta_0 + \beta_1 x\)</span>. If we were to specify a linear model for Galton’s data, we would denote the <span class="math inline">\(N\)</span> observed father heights with <span class="math inline">\(x_1, \dots, x_n\)</span>, then we model the <span class="math inline">\(N\)</span> son heights we are trying to predict with:</p>
<p><span class="math display">\[ 
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \, i=1,\dots,N. 
\]</span></p>
<p>Here <span class="math inline">\(x_i\)</span> is the father’s height, which is fixed (not random) due to the conditioning, and <span class="math inline">\(Y_i\)</span> is the random son’s height that we want to predict. We further assume that <span class="math inline">\(\varepsilon_i\)</span> are independent from each other, have expected value 0 and the standard deviation, call it <span class="math inline">\(\sigma\)</span>, does not depend on <span class="math inline">\(i\)</span>.</p>
<p>In the above model, we know the <span class="math inline">\(x_i\)</span>, but to have a useful model for prediction, we need <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. We estimate these from the data. Once we do this, we can predict son’s heights for any father’s height <span class="math inline">\(x\)</span>. We show how to do this in the next section.</p>
<p>Note that if we further assume that the <span class="math inline">\(\varepsilon\)</span> is normally distributed, then this model is exactly the same one we derived earlier by assuming bivariate normal data. A somewhat nuanced difference is that in the first approach we assumed the data was bivariate normal and that the linear model was derived, not assumed. In practice, linear models are just assumed without necessarily assuming normality: the distribution of the <span class="math inline">\(\varepsilon\)</span>s is not specified. Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model.</p>
</div>
<div id="section-interpreting-linear-models" class="section level3">
<h3>Interpreting linear models</h3>
<p>One reason linear models are popular is that they are interpretable. In the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by <span class="math inline">\(\beta_1\)</span> for each inch we increase the father’s height <span class="math inline">\(x\)</span>. Because not all sons with fathers of height <span class="math inline">\(x\)</span> are of equal height, we need the term <span class="math inline">\(\varepsilon\)</span>, which explains the remaining variability. This remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness.</p>
<p>Given how we wrote the model above, the intercept <span class="math inline">\(\beta_0\)</span> is not very interpretable as it is the predicted height of a son with a father with no height. Due to regression to the mean, the prediction will usually be a bit larger than 0. To make the slope parameter more interpretable, we can rewrite the model slightly as:</p>
<p><span class="math display">\[ 
Y_i = \beta_0 + \beta_1 (x_i - \bar{x}) + \varepsilon_i, \, i=1,\dots,N 
\]</span></p>
<p>with <span class="math inline">\(\bar{x} = 1/N \sum_{i=1}^N x_i\)</span> the average of the <span class="math inline">\(x\)</span>. In this case <span class="math inline">\(\beta_0\)</span> represents the height when <span class="math inline">\(x_i = \bar{x}\)</span>, which is the height of the son of an average father.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>“Linear” here does not refer to lines, but rather to the fact that the conditional expectation is a linear combination of known quantities.</p></li>
<li><p>In Galton’s model, we assume <span class="math inline">\(Y\)</span> (son’s height) is a linear combination of a constant and <span class="math inline">\(X\)</span> (father’s height) plus random noise. We further assume that <span class="math inline">\(\varepsilon_i\)</span> are independent from each other, have expected value 0 and the standard deviation <span class="math inline">\(\sigma\)</span> which does not depend on i.</p></li>
<li><p>Note that if we further assume that <span class="math inline">\(\varepsilon\)</span> is normally distributed, then the model is exactly the same one we derived earlier by assuming bivariate normal data.</p></li>
</ul>
<p>-We can subtract the mean from <span class="math inline">\(X\)</span> to make <span class="math inline">\(\beta_0\)</span> more interpretable.</p>
</div>
</div>
<div id="section-assessment-introduction-to-linear-models" class="section level3">
<h3>2.1 Assessment: Introduction to Linear Models</h3>
<p>Insert assessment here</p>
</div>
<div id="section-least-squares-estimates-lse" class="section level3">
<h3>Least Squares Estimates (LSE)</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#lse">textbook section - 18.3 Least squares estimates</a>.</p>
</div>
<p>For linear models to be useful, we have to estimate the unknown <span class="math inline">\(\beta\)</span>s. The standard approach in science is to find the values that minimize the distance of the fitted model to the data. The following is called the least squares (LS) equation and we will see it often in this section. For Galton’s data, we would write:</p>
<p><span class="math display">\[ 
RSS = \sum_{i=1}^n \left\{  y_i - \left(\beta_0 + \beta_1 x_i \right)\right\}^2 
\]</span></p>
<p>This quantity is called the residual sum of squares (RSS). Once we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them with <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>. Let’s demonstrate this with the previously defined dataset:</p>
<pre class="r"><code>library(HistData)
data(&quot;GaltonFamilies&quot;)
set.seed(1983)
galton_heights &lt;- GaltonFamilies %&gt;%
  filter(gender == &quot;male&quot;) %&gt;%
  group_by(family) %&gt;%
  sample_n(1) %&gt;%
  ungroup() %&gt;%
  select(father, childHeight) %&gt;%
  rename(son = childHeight)</code></pre>
<p>Let’s write a function that computes the RSS for any pair of values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<pre class="r"><code>rss &lt;- function(beta0, beta1, data){
  resid &lt;- galton_heights$son - (beta0+beta1*galton_heights$father)
  return(sum(resid^2))
}</code></pre>
<p>So for any pair of values, we get an RSS. Here is a plot of the RSS as a function of <span class="math inline">\(\beta_1\)</span> when we keep the <span class="math inline">\(\beta_0\)</span> fixed at 25.</p>
<pre class="r"><code>beta1 = seq(0, 1, len=nrow(galton_heights))
results &lt;- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 25))
results %&gt;% ggplot(aes(beta1, rss)) + geom_line() + 
  geom_line(aes(beta1, rss))</code></pre>
<p><img src="datsci_07_files/figure-html/rss-versus-estimate-1.png" width="624" /></p>
<p>We can see a clear minimum for <span class="math inline">\(\beta_1\)</span> at around 0.65. However, this minimum for <span class="math inline">\(\beta_1\)</span> is for when <span class="math inline">\(\beta_0 = 25\)</span>, a value we arbitrarily picked. We don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs.</p>
<p>Trial and error is not going to work in this case. We could search for a minimum within a fine grid of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> values, but this is unnecessarily time-consuming since we can use calculus: take the partial derivatives, set them to 0 and solve for <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>. Of course, if we have many parameters, these equations can get rather complex. But there are functions in R that do these calculations for us. We will learn these next. To learn the mathematics behind this, you can consult a book on linear models.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>For regression, we aim to find the coefficient values that minimize the distance of the fitted model to the data.</p></li>
<li><p>Residual sum of squares (RSS) measures the distance between the true value and the predicted value given by the regression line. The values that minimize the RSS are called the least squares estimates (LSE).</p></li>
<li><p>We can use partial derivatives to get the values for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in Galton’s data.</p></li>
</ul>
</div>
</div>
<div id="section-the-lm-function" class="section level3">
<h3>The lm Function</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#the-lm-function">textbook section - 18.3.3 The lm function</a>.</p>
</div>
<p>In R, we can obtain the least squares estimates using the <code>lm</code> function. To fit the model:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
\]</span></p>
<p>with <span class="math inline">\(Y_i\)</span> the son’s height and <span class="math inline">\(x_i\)</span> the father’s height, we can use this code to obtain the least squares estimates.</p>
<pre class="r"><code>fit &lt;- lm(son ~ father, data = galton_heights)
fit$coef</code></pre>
<pre><code>## (Intercept)      father 
##   37.287605    0.461392</code></pre>
<p>The most common way we use <code>lm</code> is by using the character <code>~</code> to let <code>lm</code> know which is the variable we are predicting (left of <code>~</code>) and which we are using to predict (right of <code>~</code>). The intercept is added automatically to the model that will be fit.</p>
<p>The object <code>fit</code> includes more information about the fit. We can use the function <code>summary</code> to extract more of this information (not shown):</p>
<pre class="r"><code>summary(fit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = son ~ father, data = galton_heights)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.3543 -1.5657 -0.0078  1.7263  9.4150 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 37.28761    4.98618   7.478 3.37e-12 ***
## father       0.46139    0.07211   6.398 1.36e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.45 on 177 degrees of freedom
## Multiple R-squared:  0.1878, Adjusted R-squared:  0.1833 
## F-statistic: 40.94 on 1 and 177 DF,  p-value: 1.36e-09</code></pre>
<p>To understand some of the information included in this summary we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>When calling the <code>lm()</code> function, the variable that we want to predict is put to the left of the ~ symbol, and the variables that we use to predict is put to the right of the ~ symbol. The intercept is added automatically.</p></li>
<li><p>LSEs are random variables.</p></li>
</ul>
</div>
</div>
<div id="section-lse-are-random-variables" class="section level3">
<h3>LSE are Random Variables</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#lse-are-random-variables">textbook section - 18.3.4 LSE are random variables</a>.</p>
</div>
<p>The LSE is derived from the data <span class="math inline">\(y_1,\dots,y_N\)</span>, which are a realization of random variables <span class="math inline">\(Y_1, \dots, Y_N\)</span>. This implies that our estimates are random variables. To see this, we can run a Monte Carlo simulation in which we assume the son and father height data defines a population, take a random sample of size <span class="math inline">\(N=50\)</span>, and compute the regression slope coefficient for each one:</p>
<pre class="r"><code>B &lt;- 1000
N &lt;- 50
lse &lt;- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %&gt;% 
    lm(son ~ father, data = .) %&gt;% 
    .$coef 
})
lse &lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) </code></pre>
<p>We can see the variability of the estimates by plotting their distributions:</p>
<p><img src="datsci_07_files/figure-html/lse-distributions-1.png" width="100%" /></p>
<p>The reason these look normal is because the central limit theorem applies here as well: for large enough <span class="math inline">\(N\)</span>, the least squares estimates will be approximately normal with expected value <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, respectively. The standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the <code>lm</code> function. Here it is for one of our simulated data sets:</p>
<pre class="r"><code> sample_n(galton_heights, N, replace = TRUE) %&gt;% 
  lm(son ~ father, data = .) %&gt;% 
  summary %&gt;% .$coef</code></pre>
<pre><code>##               Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 19.2791952 11.6564590 1.653950 0.1046637693
## father       0.7198756  0.1693834 4.249977 0.0000979167</code></pre>
<p>You can see that the standard errors estimates reported by the <code>summary</code> are close to the standard errors from the simulation:</p>
<pre class="r"><code>lse %&gt;% summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["se_0"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["se_1"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"8.83591","2":"0.1278812"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The <code>summary</code> function also reports t-statistics (<code>t value</code>) and p-values (<code>Pr(&gt;|t|)</code>). The t-statistic is not actually based on the central limit theorem but rather on the assumption that the <span class="math inline">\(\varepsilon\)</span>s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, <span class="math inline">\(\hat{\beta}_0 / \hat{\mbox{SE}}(\hat{\beta}_0 )\)</span> and <span class="math inline">\(\hat{\beta}_1 / \hat{\mbox{SE}}(\hat{\beta}_1 )\)</span>, follow a t-distribution with <span class="math inline">\(N-p\)</span> degrees of freedom, with <span class="math inline">\(p\)</span> the number of parameters in our model. In the case of height <span class="math inline">\(p=2\)</span>, the two p-values are testing the null hypothesis that <span class="math inline">\(\beta_0 = 0\)</span> and <span class="math inline">\(\beta_1=0\)</span>, respectively.</p>
<p>Remember that, as we described in the textbook <a href="https://rafalab.github.io/dsbook/models.html#t-dist">(Section - 16.10 The t-distribution)</a> for large enough <span class="math inline">\(N\)</span>, the CLT works and the t-distribution becomes almost the same as the normal distribution. Also, notice that we can construct confidence intervals, but we will soon learn about <strong>broom</strong>, an add-on package that makes this easy.</p>
<p>Although we do not show examples in this course, hypothesis testing with regression models is commonly used in epidemiology and economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y, and Z”. However, several assumptions have to hold for these statements to be true.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Because they are derived from the samples, LSE are random variables.</p></li>
<li><p><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> appear to be normally distributed because the central limit theorem plays a role.</p></li>
<li><p>The t-statistic depends on the assumption that <span class="math inline">\(\varepsilon\)</span> follows a normal distribution.</p></li>
</ul>
</div>
</div>
<div id="section-predicted-variables-are-random-variables" class="section level3">
<h3>Predicted Variables are Random Variables</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#predicted-values-are-random-variables">textbook section - 18.3.5 Predicted values are random variables</a>.</p>
</div>
<p>Once we fit our model, we can obtain prediction of <span class="math inline">\(Y\)</span> by plugging in the estimates into the regression model. For example, if the father’s height is <span class="math inline">\(x\)</span>, then our prediction <span class="math inline">\(\hat{Y}\)</span> for the son’s height will be:</p>
<p><span class="math display">\[\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 x\]</span></p>
<p>When we plot <span class="math inline">\(\hat{Y}\)</span> versus <span class="math inline">\(x\)</span>, we see the regression line.</p>
<p>Keep in mind that the prediction <span class="math inline">\(\hat{Y}\)</span> is also a random variable and mathematical theory tells us what the standard errors are. If we assume the errors are normal, or have a large enough sample size, we can use theory to construct confidence intervals as well. In fact, the <strong>ggplot2</strong> layer <code>geom_smooth(method = "lm")</code> that we previously used plots <span class="math inline">\(\hat{Y}\)</span> and surrounds it by confidence intervals:</p>
<pre class="r"><code>galton_heights %&gt;% ggplot(aes(son, father)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="datsci_07_files/figure-html/father-son-regression-1.png" width="624" /></p>
<p>The R function <code>predict</code> takes an <code>lm</code> object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided:</p>
<pre class="r"><code>fit &lt;- galton_heights %&gt;% lm(son ~ father, data = .) 

y_hat &lt;- predict(fit, se.fit = TRUE)

names(y_hat)</code></pre>
<pre><code>## [1] &quot;fit&quot;            &quot;se.fit&quot;         &quot;df&quot;             &quot;residual.scale&quot;</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The predicted value is often denoted as <span class="math inline">\(\hat{Y}\)</span>, which is a random variable. Mathematical theory tells us what the standard error of the predicted value is.</p></li>
<li><p>The <code>predict()</code> function in R can give us predictions directly.</p></li>
</ul>
</div>
</div>
<div id="section-assessment-least-squares-estimates-part-1" class="section level3">
<h3>2.1 Assessment: Least Squares Estimates, part 1</h3>
<p>Insert assessment here</p>
</div>
<div id="section-assessment-least-squares-estimates-part-2" class="section level3">
<h3>2.1 Assessment: Least Squares Estimates, part 2</h3>
<p>Insert assessment here</p>
</div>
<div id="section-advanced-dplyr-tibbles" class="section level3">
<h3>Advanced dplyr: Tibbles</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#linear-regression-in-the-tidyverse">textbook section - 18.5 Linear regression in the tidyverse</a>.</p>
</div>
<p>To see how we use the <code>lm</code> function in a more complex analysis, let’s go back to the baseball example. In a previous example, we estimated regression lines to predict runs for BB in different HR strata. We first constructed a data frame similar to this:</p>
<pre class="r"><code>dat &lt;- Teams %&gt;% filter(yearID %in% 1961:2001) %&gt;%
  mutate(HR = round(HR/G, 1), 
         BB = BB/G,
         R = R/G) %&gt;%
  select(HR, BB, R) %&gt;%
  filter(HR &gt;= 0.4 &amp; HR&lt;=1.2) </code></pre>
<p>Since we didn’t know the <code>lm</code> function, to compute the regression line in each strata, we used the formula directly like this:</p>
<pre class="r"><code>get_slope &lt;- function(x, y) cor(x, y) * sd(y) / sd(x)
dat %&gt;%  
  group_by(HR) %&gt;%
  summarize(slope = get_slope(BB, R))</code></pre>
<p>We argued that the slopes are similar and that the differences were perhaps due to random variation. To provide a more rigorous defense of the slopes being the same, which led to our multivariate model, we could compute confidence intervals for each slope. We have not learned the formula for this, but the <code>lm</code> function provides enough information to construct them.</p>
<p>First, note that if we try to use the <code>lm</code> function to get the estimated slope like this:</p>
<pre class="r"><code>dat %&gt;%  
  group_by(HR) %&gt;%
  lm(R ~ BB, data = .) %&gt;% .$coef</code></pre>
<pre><code>## (Intercept)          BB 
##   2.1983658   0.6378804</code></pre>
<p>we don’t get the result we want. The <code>lm</code> function ignores the <code>group_by</code>. This is expected because <code>lm</code> is not part of the <strong>tidyverse</strong> and does not know how to handle the outcome of a grouped tibble.</p>
<p>Notice that there are no columns with this information. But, if you look closely at the output above, you see the line <code>A tibble</code> followd by dimensions. We can learn the class of the returned object using:</p>
<pre class="r"><code>data(murders)
murders %&gt;% group_by(region) %&gt;% class()</code></pre>
<p>The <code>tbl</code>, pronounced tibble, is a special kind of data frame. The functions <code>group_by</code> and <code>summarize</code> always return this type of data frame. The <code>group_by</code> function returns a special kind of <code>tbl</code>, the <code>grouped_df</code>. We will say more about these later. For consistency, the <strong>dplyr</strong> manipulation verbs (<code>select</code>, <code>filter</code>, <code>mutate</code>, and <code>arrange</code>) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble. For example, in the textbook <a href="https://rafalab.github.io/dsbook/importing-data.html">(Section - 5 Importing data)</a> we will see that tidyverse functions used to import data create tibbles.</p>
<p>Tibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Tibbles can be regarded as a modern version of data frames and are the default data structure in the tidyverse.</p></li>
<li><p>Some functions that do not work properly with data frames do work with tibbles.</p></li>
</ul>
</div>
</div>
<div id="section-tibbles-differences-from-data-frames" class="section level3">
<h3>Tibbles: Differences from Data Frames</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/tidyverse.html#tibbles">textbook section - 4.10 Tibbles</a>.</p>
</div>
<p>The print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing <code>murders</code> and the output of murders if we convert it to a tibble. We can do this using <code>as_tibble(murders)</code>. If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.</p>
<div id="section-subsets-of-tibbles-are-tibbles" class="section level4">
<h4><strong>Subsets of tibbles are tibbles</strong></h4>
<p>If you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:</p>
<pre class="r"><code>class(murders[,4])</code></pre>
<p>is not a data frame. With tibbles this does not happen:</p>
<pre class="r"><code>class(as_tibble(murders)[,4])</code></pre>
<p>This is useful in the tidyverse since functions require data frames as input.</p>
<p>With tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor <code>$</code>:</p>
<pre class="r"><code>class(as_tibble(murders)$population)</code></pre>
<p>A related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write <code>Population</code> instead of <code>population</code> this:</p>
<pre class="r"><code>murders$Population</code></pre>
<p>returns a <code>NULL</code> with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:</p>
<pre class="r"><code>as_tibble(murders)$Population</code></pre>
</div>
</div>
<div id="section-tibbles-can-have-complex-entries" class="section level3">
<h3>Tibbles can have complex entries</h3>
<p>While data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:</p>
<pre class="r"><code>tibble(id = c(1, 2, 3), func = c(mean, median, sd))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["id"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["func"],"name":[2],"type":["list"],"align":["right"]}],"data":[{"1":"1","2":"<fun>"},{"1":"2","2":"<fun>"},{"1":"3","2":"<fun>"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div id="section-tibbles-can-be-grouped" class="section level3">
<h3>Tibbles can be grouped</h3>
<p>The function <code>group_by</code> returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the <code>summarize</code> function, are aware of the group information.</p>
</div>
<div id="section-create-a-tibble-using-tibble-instead-of-data.frame" class="section level3">
<h3>Create a tibble using <code>tibble</code> instead of <code>data.frame</code></h3>
<p>It is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the <code>tibble</code> function.</p>
<pre class="r"><code>grades &lt;- tibble(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), 
                     exam_1 = c(95, 80, 90, 85), 
                     exam_2 = c(90, 85, 85, 90))</code></pre>
<p>Note that base R (without packages loaded) has a function with a very similar name, <code>data.frame</code>, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default <code>data.frame</code> coerces characters into factors without providing a warning or message:</p>
<pre class="r"><code>grades &lt;- data.frame(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), 
                     exam_1 = c(95, 80, 90, 85), 
                     exam_2 = c(90, 85, 85, 90))
class(grades$names)</code></pre>
<pre><code>## [1] &quot;character&quot;</code></pre>
<p>To avoid this, we use the rather cumbersome argument <code>stringsAsFactors</code>:</p>
<pre class="r"><code>grades &lt;- data.frame(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), 
                     exam_1 = c(95, 80, 90, 85), 
                     exam_2 = c(90, 85, 85, 90),
                     stringsAsFactors = FALSE)
class(grades$names)</code></pre>
<pre><code>## [1] &quot;character&quot;</code></pre>
<p>To convert a regular data frame to a tibble, you can use the <code>as_tibble</code> function.</p>
<pre class="r"><code>as_tibble(grades) %&gt;% class()</code></pre>
<pre><code>## [1] &quot;tbl_df&quot;     &quot;tbl&quot;        &quot;data.frame&quot;</code></pre>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Tibbles are more readable than data frames.</p></li>
<li><p>If you subset a data frame, you may not get a data frame. If you subset a tibble, you always get a tibble.</p></li>
<li><p>Tibbles can hold more complex objects such as lists or functions.</p></li>
<li><p>Tibbles can be grouped.</p></li>
</ul>
</div>
</div>
<div id="section-do" class="section level3">
<h3>do</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#linear-regression-in-the-tidyverse">textbook section - 18.5 Linear regression in the tidyverse</a>.</p>
</div>
<p>The <strong>tidyverse</strong> functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe <code>%&gt;%</code>, <strong>tidyverse</strong> functions consistently return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The <code>lm</code> function is an example. The <code>do</code> functions serves as a bridge between R functions, such as <code>lm</code>, and the <strong>tidyverse</strong>. The <code>do</code> function understands grouped tibbles and always returns a data frame.</p>
<p>So, let’s try to use the <code>do</code> function to fit a regression line to each HR strata:</p>
<pre class="r"><code>dat %&gt;%  
  group_by(HR) %&gt;%
  do(fit = lm(R ~ BB, data = .))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["HR"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["fit"],"name":[2],"type":["list"],"align":["right"]}],"data":[{"1":"0.4","2":"<S3: lm>"},{"1":"0.5","2":"<S3: lm>"},{"1":"0.6","2":"<S3: lm>"},{"1":"0.7","2":"<S3: lm>"},{"1":"0.8","2":"<S3: lm>"},{"1":"0.9","2":"<S3: lm>"},{"1":"1.0","2":"<S3: lm>"},{"1":"1.1","2":"<S3: lm>"},{"1":"1.2","2":"<S3: lm>"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Notice that we did in fact fit a regression line to each strata. The <code>do</code> function will create a data frame with the first column being the strata value and a column named <code>fit</code> (we chose the name, but it can be anything). The column will contain the result of the <code>lm</code> call. Therefore, the returned tibble has a column with <code>lm</code> objects, which is not very useful.</p>
<p>Also, if we do not name a column (note above we named it <code>fit</code>), then <code>do</code> will return the actual output of <code>lm</code>, not a data frame, and this will result in an error since <code>do</code> is expecting a data frame as output.</p>
<pre class="r"><code>dat %&gt;%  
  group_by(HR) %&gt;%
  do(lm(R ~ BB, data = .))</code></pre>
<p><code>Error: Results 1, 2, 3, 4, 5, ... must be data frames, not lm</code></p>
<p>For a useful data frame to be constructed, the output of the function must be a data frame too. We could build a function that returns only what we want in the form of a data frame:</p>
<pre class="r"><code>get_slope &lt;- function(data){
  fit &lt;- lm(R ~ BB, data = data)
  data.frame(slope = fit$coefficients[2], 
             se = summary(fit)$coefficient[2,2])
}</code></pre>
<p>And then use <code>do</code> <strong>without</strong> naming the output, since we are already getting a data frame:</p>
<pre class="r"><code>dat %&gt;%  
  group_by(HR) %&gt;%
  do(get_slope(.))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["HR"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["slope"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["se"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.4","2":"0.7342910","3":"0.20759215"},{"1":"0.5","2":"0.5659067","3":"0.11018732"},{"1":"0.6","2":"0.4119129","3":"0.09738718"},{"1":"0.7","2":"0.2853933","3":"0.07049603"},{"1":"0.8","2":"0.3650361","3":"0.06529659"},{"1":"0.9","2":"0.2608882","3":"0.07512866"},{"1":"1.0","2":"0.5115687","3":"0.07508505"},{"1":"1.1","2":"0.4539252","3":"0.08550090"},{"1":"1.2","2":"0.4403274","3":"0.08006835"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>If we name the output, then we get something we do not want, a column containing data frames:</p>
<pre class="r"><code>dat %&gt;%  
  group_by(HR) %&gt;%
  do(slope = get_slope(.))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["HR"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["slope"],"name":[2],"type":["list"],"align":["right"]}],"data":[{"1":"0.4","2":"<data.frame [1 × 2]>"},{"1":"0.5","2":"<data.frame [1 × 2]>"},{"1":"0.6","2":"<data.frame [1 × 2]>"},{"1":"0.7","2":"<data.frame [1 × 2]>"},{"1":"0.8","2":"<data.frame [1 × 2]>"},{"1":"0.9","2":"<data.frame [1 × 2]>"},{"1":"1.0","2":"<data.frame [1 × 2]>"},{"1":"1.1","2":"<data.frame [1 × 2]>"},{"1":"1.2","2":"<data.frame [1 × 2]>"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>This is not very useful, so let’s cover one last feature of <code>do</code>. If the data frame being returned has more than one row, these will be concatenated appropriately. Here is an example in which we return both estimated parameters:</p>
<pre class="r"><code>get_lse &lt;- function(data){
  fit &lt;- lm(R ~ BB, data = data)
  data.frame(term = names(fit$coefficients),
    slope = fit$coefficients, 
    se = summary(fit)$coefficient[,2])
}

dat %&gt;%  
  group_by(HR) %&gt;%
  do(get_lse(.))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["HR"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["term"],"name":[2],"type":["chr"],"align":["left"]},{"label":["slope"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["se"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.4","2":"(Intercept)","3":"1.3594765","4":"0.63066134"},{"1":"0.4","2":"BB","3":"0.7342910","4":"0.20759215"},{"1":"0.5","2":"(Intercept)","3":"2.0072118","4":"0.34367703"},{"1":"0.5","2":"BB","3":"0.5659067","4":"0.11018732"},{"1":"0.6","2":"(Intercept)","3":"2.5328026","4":"0.30457218"},{"1":"0.6","2":"BB","3":"0.4119129","4":"0.09738718"},{"1":"0.7","2":"(Intercept)","3":"3.2076555","4":"0.22478012"},{"1":"0.7","2":"BB","3":"0.2853933","4":"0.07049603"},{"1":"0.8","2":"(Intercept)","3":"3.0698585","4":"0.21336231"},{"1":"0.8","2":"BB","3":"0.3650361","4":"0.06529659"},{"1":"0.9","2":"(Intercept)","3":"3.5424214","4":"0.25077792"},{"1":"0.9","2":"BB","3":"0.2608882","4":"0.07512866"},{"1":"1.0","2":"(Intercept)","3":"2.8811064","4":"0.25588441"},{"1":"1.0","2":"BB","3":"0.5115687","4":"0.07508505"},{"1":"1.1","2":"(Intercept)","3":"3.2107425","4":"0.29973450"},{"1":"1.1","2":"BB","3":"0.4539252","4":"0.08550090"},{"1":"1.2","2":"(Intercept)","3":"3.3950688","4":"0.29139873"},{"1":"1.2","2":"BB","3":"0.4403274","4":"0.08006835"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>If you think this is all a bit too complicated, you are not alone. To simplify things, we introduce the <strong>broom</strong> package which was designed to facilitate the use of model fitting functions, such as <code>lm</code>, with the <strong>tidyverse</strong>.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The <code>do()</code> function serves as a bridge between <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> functions, such as <code>lm()</code>, and the tidyverse.</p></li>
<li><p>We have to specify a column when using the <code>do()</code> function, otherwise we will get an error.</p></li>
<li><p>If the data frame being returned has more than one row, the rows will be concatenated appropriately.</p></li>
</ul>
</div>
</div>
<div id="section-broom" class="section level3">
<h3>broom</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#the-broom-package">textbook section - 18.5.1 The broom package</a>.</p>
</div>
<p>Our original task was to provide an estimate and confidence interval for the slope estimates of each strata. The <strong>broom</strong> package will make this quite easy.</p>
<p>The <strong>broom</strong> package has three main functions, all of which extract information from the object returned by <code>lm</code> and return it in a <strong>tidyverse</strong> friendly data frame. These functions are <code>tidy</code>, <code>glance</code>, and <code>augment</code>. The <code>tidy</code> function returns estimates and related information as a data frame:</p>
<pre class="r"><code>library(broom)
fit &lt;- lm(R ~ BB, data = dat)
tidy(fit)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["estimate"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["std.error"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["statistic"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["p.value"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"2.1983658","3":"0.11348875","4":"19.37078","5":"1.120201e-70"},{"1":"BB","2":"0.6378804","3":"0.03444016","4":"18.52141","5":"1.351142e-65"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>We can add other important summaries, such as confidence intervals:</p>
<pre class="r"><code>tidy(fit, conf.int = TRUE)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["estimate"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["std.error"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["statistic"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["p.value"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["conf.low"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["conf.high"],"name":[7],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"2.1983658","3":"0.11348875","4":"19.37078","5":"1.120201e-70","6":"1.9756473","7":"2.4210843"},{"1":"BB","2":"0.6378804","3":"0.03444016","4":"18.52141","5":"1.351142e-65","6":"0.5702926","7":"0.7054683"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Because the outcome is a data frame, we can immediately use it with <code>do</code> to string together the commands that produce the table we are after. Because a data frame is returned, we can filter and select the rows and columns we want, which facilitates working with <strong>ggplot2</strong>:</p>
<pre class="r"><code>dat %&gt;%  
  group_by(HR) %&gt;%
  do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) %&gt;%
  filter(term == &quot;BB&quot;) %&gt;%
  select(HR, estimate, conf.low, conf.high) %&gt;%
  ggplot(aes(HR, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point()</code></pre>
<p><img src="datsci_07_files/figure-html/do-tidy-example-1.png" width="624" /></p>
<p>Now we return to discussing our original task of determining if slopes changed. The plot we just made, using <code>do</code> and <code>tidy</code>, shows that the confidence intervals overlap, which provides a nice visual confirmation that our assumption that the slope does not change is safe.</p>
<p>The other functions provided by <strong>broom</strong>, <code>glance</code>, and <code>augment</code>, relate to model-specific and observation-specific outcomes, respectively. Here, we can see the model fit summaries <code>glance</code> returns:</p>
<pre class="r"><code>glance(fit)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["r.squared"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["adj.r.squared"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["sigma"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["statistic"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["p.value"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["df"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["logLik"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["AIC"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["BIC"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["deviance"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["df.residual"],"name":[11],"type":["int"],"align":["right"]},{"label":["nobs"],"name":[12],"type":["int"],"align":["right"]}],"data":[{"1":"0.2659158","2":"0.2651406","3":"0.4541478","4":"343.0427","5":"1.351142e-65","6":"1","7":"-596.4951","8":"1198.99","9":"1213.556","10":"195.319","11":"947","12":"949","_row":"value"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>You can learn more about these summaries in any regression text book.</p>
<p>We will see an example of <code>augment</code> in the next sections.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The <strong>broom</strong> package has three main functions, all of which extract information from the object returned by <code>lm</code> and return it in a <strong>tidyverse</strong> friendly data frame.</p></li>
<li><p>The <code>tidy()</code> function returns estimates and related information as a data frame.</p></li>
<li><p>The functions <code>glance()</code> and <code>augment()</code> relate to model specific and observation specific outcomes respectively.</p></li>
</ul>
</div>
</div>
<div id="section-assessment-tibbles-do-and-broom-part-1" class="section level3">
<h3>2.3 Assessment: Tibbles, do, and broom, part 1</h3>
<p>Insert assessment here</p>
</div>
<div id="section-assessment-tibbles-do-and-broom-part-2" class="section level3">
<h3>2.3 Assessment: Tibbles, do, and broom, part 2</h3>
<p>Insert assessment here</p>
</div>
<div id="section-building-a-better-offensive-metric-for-baseball" class="section level3">
<h3>Building a Better Offensive Metric for Baseball</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#case-study-moneyball-continued">textbook section - 18.7 Case study: Moneyball (continued)</a>.</p>
</div>
<p>In trying to answer how well BBs predict runs, data exploration led us to a model:</p>
<p><span class="math display">\[
\mbox{E}[R \mid BB = x_1, HR = x_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span></p>
<p>Here, the data is approximately normal and conditional distributions were also normal. Thus, we are justified in using a linear model:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i
\]</span></p>
<p>with <span class="math inline">\(Y_i\)</span> runs per game for team <span class="math inline">\(i\)</span>, <span class="math inline">\(x_{i,1}\)</span> walks per game, and <span class="math inline">\(x_{i,2}\)</span>. To use <code>lm</code> here, we need to let the function know we have two predictor variables. So we use the <code>+</code> symbol as follows:</p>
<pre class="r"><code>fit &lt;- Teams %&gt;% 
  filter(yearID %in% 1961:2001) %&gt;% 
  mutate(BB = BB/G, HR = HR/G,  R = R/G) %&gt;%  
  lm(R ~ BB + HR, data = .)</code></pre>
<p>We can use <code>tidy</code> to see a nice summary:</p>
<pre class="r"><code>tidy(fit, conf.int = TRUE) </code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["estimate"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["std.error"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["statistic"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["p.value"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["conf.low"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["conf.high"],"name":[7],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"1.7443011","3":"0.08235597","4":"21.18002","5":"7.617810e-83","6":"1.5826952","7":"1.9059071"},{"1":"BB","2":"0.3874238","3":"0.02701124","4":"14.34306","5":"1.198143e-42","6":"0.3344201","7":"0.4404276"},{"1":"HR","2":"1.5611689","3":"0.04896000","4":"31.88662","5":"1.777062e-155","6":"1.4650954","7":"1.6572424"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
When we fit the model with only one variable, the estimated slopes were
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["slope"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.7353288"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
and
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["slope"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"1.844824"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>for BB and HR, respectively. Note that when fitting the multivariate model both go down, with the BB effect decreasing much more.</p>
<p>Now we want to construct a metric to pick players, we need to consider singles, doubles, and triples as well. Can we build a model that predicts runs based on all these outcomes?</p>
<p>We now are going to take somewhat of a “leap of faith” and assume that these five variables are jointly normal. This means that if we pick any one of them, and hold the other four fixed, the relationship with the outcome is linear and the slope does not depend on the four values held constant. If this is true, then a linear model for our data is:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \beta_3 x_{i,3}+ \beta_4 x_{i,4} + \beta_5 x_{i,5} + \varepsilon_i
\]</span></p>
<p>with <span class="math inline">\(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\)</span> representing BB, singles, doubles, triples, and HR respectively.</p>
<p>Using <code>lm</code>, we can quickly find the LSE for the parameters using:</p>
<pre class="r"><code>fit &lt;- Teams %&gt;% 
  filter(yearID %in% 1961:2001) %&gt;% 
  mutate(BB = BB / G, 
         singles = (H - X2B - X3B - HR) / G, 
         doubles = X2B / G, 
         triples = X3B / G, 
         HR = HR / G,
         R = R / G) %&gt;%  
  lm(R ~ BB + singles + doubles + triples + HR, data = .)</code></pre>
<p>We can see the coefficients using <code>tidy</code>:</p>
<pre class="r"><code>coefs &lt;- tidy(fit, conf.int = TRUE)

coefs</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["estimate"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["std.error"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["statistic"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["p.value"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["conf.low"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["conf.high"],"name":[7],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"-2.7691857","3":"0.08618398","4":"-32.13110","5":"4.764197e-157","6":"-2.9383039","7":"-2.6000676"},{"1":"BB","2":"0.3712147","3":"0.01174237","4":"31.61327","5":"1.873650e-153","6":"0.3481727","7":"0.3942567"},{"1":"singles","2":"0.5193923","3":"0.01272014","4":"40.83227","5":"8.667916e-217","6":"0.4944317","7":"0.5443530"},{"1":"doubles","2":"0.7711444","3":"0.02260506","4":"34.11379","5":"8.440085e-171","6":"0.7267867","7":"0.8155021"},{"1":"triples","2":"1.2399696","3":"0.07679453","4":"16.14659","5":"2.124754e-52","6":"1.0892763","7":"1.3906629"},{"1":"HR","2":"1.4433701","3":"0.02434907","4":"59.27824","5":"0.000000e+00","6":"1.3955901","7":"1.4911501"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>To see how well our metric actually predicts runs, we can predict the number of runs for each team in 2002 using the function <code>predict</code>, then make a plot:</p>
<pre class="r"><code>Teams %&gt;% 
  filter(yearID %in% 2002) %&gt;% 
  mutate(BB = BB/G, 
         singles = (H-X2B-X3B-HR)/G, 
         doubles = X2B/G, 
         triples =X3B/G, 
         HR=HR/G,
         R=R/G)  %&gt;% 
  mutate(R_hat = predict(fit, newdata = .)) %&gt;%
  ggplot(aes(R_hat, R, label = teamID)) + 
  geom_point() +
  geom_text(nudge_x=0.1, cex = 2) + 
  geom_abline()</code></pre>
<p><img src="datsci_07_files/figure-html/model-predicts-runs-1.png" width="624" /></p>
<p>Our model does quite a good job as demonstrated by the fact that points from the observed versus predicted plot fall close to the identity line.</p>
<p>So instead of using batting average, or just number of HR, as a measure of picking players, we can use our fitted model to form a metric that relates more directly to run production. Specifically, to define a metric for player A, we imagine a team made up of players just like player A and use our fitted regression model to predict how many runs this team would produce. The formula would look like this: -2.7691857 + 0.3712147 <span class="math inline">\(\times\)</span> BB + 0.5193923 <span class="math inline">\(\times\)</span> singles + 0.7711444 <span class="math inline">\(\times\)</span> doubles + 1.2399696 <span class="math inline">\(\times\)</span> triples + 1.4433701 <span class="math inline">\(\times\)</span> HR.</p>
<p>To define a player-specific metric, we have a bit more work to do. A challenge here is that we derived the metric for teams, based on team-level summary statistics. For example, the HR value that is entered into the equation is HR per game for the entire team. If we compute the HR per game for a player, it will be much lower since the total is accumulated by 9 batters. Furthermore, if a player only plays part of the game and gets fewer opportunities than average, it is still considered a game played. For players, a rate that takes into account opportunities is the per-plate-appearance rate.</p>
<p>To make the per-game team rate comparable to the per-plate-appearance player rate, we compute the average number of team plate appearances per game:</p>
<pre class="r"><code>pa_per_game &lt;- Batting %&gt;% filter(yearID == 2002) %&gt;% 
  group_by(teamID) %&gt;%
  summarize(pa_per_game = sum(AB+BB)/max(G)) %&gt;% 
  pull(pa_per_game) %&gt;% 
  mean</code></pre>
<p>We compute the per-plate-appearance rates for players available in 2002 on data from 1997-2001. To avoid small sample artifacts, we filter players with less than 200 plate appearances per year. Here is the entire calculation in one line:</p>
<pre class="r"><code>players &lt;- Batting %&gt;% filter(yearID %in% 1997:2001) %&gt;% 
  group_by(playerID) %&gt;%
  mutate(PA = BB + AB) %&gt;%
  summarize(G = sum(PA)/pa_per_game,
    BB = sum(BB)/G,
    singles = sum(H-X2B-X3B-HR)/G,
    doubles = sum(X2B)/G, 
    triples = sum(X3B)/G, 
    HR = sum(HR)/G,
    AVG = sum(H)/sum(AB),
    PA = sum(PA)) %&gt;%
  filter(PA &gt;= 1000) %&gt;%
  select(-G) %&gt;%
  mutate(R_hat = predict(fit, newdata = .))</code></pre>
<p>The player-specific predicted runs computed here can be interpreted as the number of runs we predict a team will score if all batters are exactly like that player. The distribution shows that there is wide variability across players:</p>
<pre class="r"><code>qplot(R_hat, data = players, binwidth = 0.5, color = I(&quot;black&quot;))</code></pre>
<p><img src="datsci_07_files/figure-html/r-hat-hist-1.png" width="624" /></p>
</div>
<div id="section-adding-salary-and-position-information" class="section level3">
<h3>Adding salary and position information</h3>
<p>To actually build the team, we will need to know their salaries as well as their defensive position. For this, we join the <code>players</code> data frame we just created with the player information data frame included in some of the other Lahman data tables. We will learn more about the join function we learned in the textbook <a href="https://rafalab.github.io/dsbook/joining-tables.html#joins">(Section - 22.1 Joins)</a>.</p>
<p>Start by adding the 2002 salary of each player:</p>
<pre class="r"><code>players &lt;- Salaries %&gt;% 
  filter(yearID == 2002) %&gt;%
  select(playerID, salary) %&gt;%
  right_join(players, by=&quot;playerID&quot;)</code></pre>
<p>Next, we add their defensive position. This is a somewhat complicated task because players play more than one position each year. The <strong>Lahman</strong> package table <code>Appearances</code> tells how many games each player played in each position, so we can pick the position that was most played using <code>which.max</code> on each row. We use <code>apply</code> to do this. However, because some players are traded, they appear more than once on the table, so we first sum their appearances across teams. Here, we pick the one position the player most played using the <code>top_n</code> function. To make sure we only pick one position, in the case of ties, we pick the first row of the resulting data frame. We also remove the <code>OF</code> position which stands for outfielder, a generalization of three positions: left field (LF), center field (CF), and right field (RF). We also remove pitchers since they don’t bat in the league in which the A’s play.</p>
<pre class="r"><code>position_names &lt;- 
  paste0(&quot;G_&quot;, c(&quot;p&quot;,&quot;c&quot;,&quot;1b&quot;,&quot;2b&quot;,&quot;3b&quot;,&quot;ss&quot;,&quot;lf&quot;,&quot;cf&quot;,&quot;rf&quot;, &quot;dh&quot;))

tmp &lt;- Appearances %&gt;% 
  filter(yearID == 2002) %&gt;% 
  group_by(playerID) %&gt;%
  summarize_at(position_names, sum) %&gt;%
  ungroup()
  
pos &lt;- tmp %&gt;%
  select(position_names) %&gt;%
  apply(., 1, which.max) 

players &lt;- tibble(playerID = tmp$playerID, POS = position_names[pos]) %&gt;%
  mutate(POS = str_to_upper(str_remove(POS, &quot;G_&quot;))) %&gt;%
  filter(POS != &quot;P&quot;) %&gt;%
  right_join(players, by=&quot;playerID&quot;) %&gt;%
  filter(!is.na(POS)  &amp; !is.na(salary))</code></pre>
<p>Finally, we add their first and last name:</p>
<pre class="r"><code>players &lt;- Master %&gt;%
  select(playerID, nameFirst, nameLast, debut) %&gt;%
  mutate(debut = as.Date(debut)) %&gt;%
  right_join(players, by=&quot;playerID&quot;)</code></pre>
<p>If you are a baseball fan, you will recognize the top 10 players:</p>
<pre class="r"><code>players %&gt;% select(nameFirst, nameLast, POS, salary, R_hat) %&gt;% 
  arrange(desc(R_hat)) %&gt;% top_n(10) </code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["nameFirst"],"name":[1],"type":["chr"],"align":["left"]},{"label":["nameLast"],"name":[2],"type":["chr"],"align":["left"]},{"label":["POS"],"name":[3],"type":["chr"],"align":["left"]},{"label":["salary"],"name":[4],"type":["int"],"align":["right"]},{"label":["R_hat"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"Barry","2":"Bonds","3":"LF","4":"15000000","5":"8.441480"},{"1":"Larry","2":"Walker","3":"RF","4":"12666667","5":"8.344316"},{"1":"Todd","2":"Helton","3":"1B","4":"5000000","5":"7.764649"},{"1":"Manny","2":"Ramirez","3":"LF","4":"15462727","5":"7.714582"},{"1":"Sammy","2":"Sosa","3":"RF","4":"15000000","5":"7.559582"},{"1":"Jeff","2":"Bagwell","3":"1B","4":"11000000","5":"7.405572"},{"1":"Mike","2":"Piazza","3":"C","4":"10571429","5":"7.343984"},{"1":"Jason","2":"Giambi","3":"1B","4":"10428571","5":"7.263690"},{"1":"Edgar","2":"Martinez","3":"DH","4":"7086668","5":"7.259399"},{"1":"Jim","2":"Thome","3":"1B","4":"8000000","5":"7.231955"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div id="section-picking-nine-players" class="section level3">
<h3>Picking nine players</h3>
<p>On average, players with a higher metric have higher salaries:</p>
<pre class="r"><code>players %&gt;% ggplot(aes(salary, R_hat, color = POS)) + 
  geom_point() +
  scale_x_log10()</code></pre>
<p><img src="datsci_07_files/figure-html/predicted-runs-vs-salary-1.png" width="624" /></p>
<p>Notice the very high salaries for most players. We do see some low-cost players with very high metrics. These will be great for our team. Some of these are likely young players that have not yet been able to negotiate a salary and are unavailable.</p>
<p>Here we remake plot without players that debuted before 1998. We use the <strong>lubridate</strong> function <code>year</code>, introduced in the textbook <a href="https://rafalab.github.io/dsbook/parsing-dates-and-times.html#lubridate">(Section - 25.2 The lubridate package)</a>.</p>
<pre class="r"><code>library(lubridate)
players %&gt;% filter(year(debut) &lt; 1998) %&gt;%
 ggplot(aes(salary, R_hat, color = POS)) + 
  geom_point() +
  scale_x_log10()</code></pre>
<p><img src="datsci_07_files/figure-html/predicted-runs-vs-salary-no-rookies-1.png" width="624" /></p>
<p>We can search for good deals by looking at players who produce many more runs than others with similar salaries. We can use this table to decide what players to pick and keep our total salary below the 40 million dollars Billy Beane had to work with. This can be done using what computer scientists call linear programming. This is not something we teach, but here are the position players selected with this approach:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
nameFirst
</th>
<th style="text-align:left;">
nameLast
</th>
<th style="text-align:left;">
POS
</th>
<th style="text-align:right;">
salary
</th>
<th style="text-align:right;">
R_hat
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Todd
</td>
<td style="text-align:left;">
Helton
</td>
<td style="text-align:left;">
1B
</td>
<td style="text-align:right;">
5000000
</td>
<td style="text-align:right;">
7.764649
</td>
</tr>
<tr>
<td style="text-align:left;">
Mike
</td>
<td style="text-align:left;">
Piazza
</td>
<td style="text-align:left;">
C
</td>
<td style="text-align:right;">
10571429
</td>
<td style="text-align:right;">
7.343984
</td>
</tr>
<tr>
<td style="text-align:left;">
Edgar
</td>
<td style="text-align:left;">
Martinez
</td>
<td style="text-align:left;">
DH
</td>
<td style="text-align:right;">
7086668
</td>
<td style="text-align:right;">
7.259399
</td>
</tr>
<tr>
<td style="text-align:left;">
Jim
</td>
<td style="text-align:left;">
Edmonds
</td>
<td style="text-align:left;">
CF
</td>
<td style="text-align:right;">
7333333
</td>
<td style="text-align:right;">
6.552456
</td>
</tr>
<tr>
<td style="text-align:left;">
Jeff
</td>
<td style="text-align:left;">
Kent
</td>
<td style="text-align:left;">
2B
</td>
<td style="text-align:right;">
6000000
</td>
<td style="text-align:right;">
6.391614
</td>
</tr>
<tr>
<td style="text-align:left;">
Phil
</td>
<td style="text-align:left;">
Nevin
</td>
<td style="text-align:left;">
3B
</td>
<td style="text-align:right;">
2600000
</td>
<td style="text-align:right;">
6.163936
</td>
</tr>
<tr>
<td style="text-align:left;">
Matt
</td>
<td style="text-align:left;">
Stairs
</td>
<td style="text-align:left;">
RF
</td>
<td style="text-align:right;">
500000
</td>
<td style="text-align:right;">
6.062372
</td>
</tr>
<tr>
<td style="text-align:left;">
Henry
</td>
<td style="text-align:left;">
Rodriguez
</td>
<td style="text-align:left;">
LF
</td>
<td style="text-align:right;">
300000
</td>
<td style="text-align:right;">
5.938315
</td>
</tr>
<tr>
<td style="text-align:left;">
John
</td>
<td style="text-align:left;">
Valentin
</td>
<td style="text-align:left;">
SS
</td>
<td style="text-align:right;">
550000
</td>
<td style="text-align:right;">
5.273441
</td>
</tr>
</tbody>
</table>
<p>We see that all these players have above average BB and most have above average HR rates, while the same is not true for singles. Here is a table with statistics standardized across players so that, for example, above average HR hitters have values above 0.</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
nameLast
</th>
<th style="text-align:right;">
BB
</th>
<th style="text-align:right;">
singles
</th>
<th style="text-align:right;">
doubles
</th>
<th style="text-align:right;">
triples
</th>
<th style="text-align:right;">
HR
</th>
<th style="text-align:right;">
AVG
</th>
<th style="text-align:right;">
R_hat
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Helton
</td>
<td style="text-align:right;">
0.9088340
</td>
<td style="text-align:right;">
-0.2147828
</td>
<td style="text-align:right;">
2.6489997
</td>
<td style="text-align:right;">
-0.3105275
</td>
<td style="text-align:right;">
1.5221254
</td>
<td style="text-align:right;">
2.6704562
</td>
<td style="text-align:right;">
2.5316660
</td>
</tr>
<tr>
<td style="text-align:left;">
Piazza
</td>
<td style="text-align:right;">
0.3281058
</td>
<td style="text-align:right;">
0.4231217
</td>
<td style="text-align:right;">
0.2037161
</td>
<td style="text-align:right;">
-1.4181571
</td>
<td style="text-align:right;">
1.8253653
</td>
<td style="text-align:right;">
2.1990055
</td>
<td style="text-align:right;">
2.0890701
</td>
</tr>
<tr>
<td style="text-align:left;">
Martinez
</td>
<td style="text-align:right;">
2.1352215
</td>
<td style="text-align:right;">
-0.0051702
</td>
<td style="text-align:right;">
1.2649044
</td>
<td style="text-align:right;">
-1.2242578
</td>
<td style="text-align:right;">
0.8079817
</td>
<td style="text-align:right;">
2.2032836
</td>
<td style="text-align:right;">
2.0000756
</td>
</tr>
<tr>
<td style="text-align:left;">
Edmonds
</td>
<td style="text-align:right;">
1.0706548
</td>
<td style="text-align:right;">
-0.5579104
</td>
<td style="text-align:right;">
0.7912381
</td>
<td style="text-align:right;">
-1.1517126
</td>
<td style="text-align:right;">
0.9730052
</td>
<td style="text-align:right;">
0.8543566
</td>
<td style="text-align:right;">
1.2562767
</td>
</tr>
<tr>
<td style="text-align:left;">
Kent
</td>
<td style="text-align:right;">
0.2316321
</td>
<td style="text-align:right;">
-0.7322902
</td>
<td style="text-align:right;">
2.0113988
</td>
<td style="text-align:right;">
0.4483097
</td>
<td style="text-align:right;">
0.7658693
</td>
<td style="text-align:right;">
0.7871932
</td>
<td style="text-align:right;">
1.0870488
</td>
</tr>
<tr>
<td style="text-align:left;">
Nevin
</td>
<td style="text-align:right;">
0.3066863
</td>
<td style="text-align:right;">
-0.9051225
</td>
<td style="text-align:right;">
0.4787634
</td>
<td style="text-align:right;">
-1.1908955
</td>
<td style="text-align:right;">
1.1927055
</td>
<td style="text-align:right;">
0.1048721
</td>
<td style="text-align:right;">
0.8475017
</td>
</tr>
<tr>
<td style="text-align:left;">
Stairs
</td>
<td style="text-align:right;">
1.0996635
</td>
<td style="text-align:right;">
-1.5127562
</td>
<td style="text-align:right;">
-0.0460876
</td>
<td style="text-align:right;">
-1.1285395
</td>
<td style="text-align:right;">
1.1209081
</td>
<td style="text-align:right;">
-0.5608456
</td>
<td style="text-align:right;">
0.7406428
</td>
</tr>
<tr>
<td style="text-align:left;">
Rodriguez
</td>
<td style="text-align:right;">
0.2011513
</td>
<td style="text-align:right;">
-1.5963595
</td>
<td style="text-align:right;">
0.3324557
</td>
<td style="text-align:right;">
-0.7823620
</td>
<td style="text-align:right;">
1.3202734
</td>
<td style="text-align:right;">
-0.6723416
</td>
<td style="text-align:right;">
0.6101181
</td>
</tr>
<tr>
<td style="text-align:left;">
Valentin
</td>
<td style="text-align:right;">
0.1802855
</td>
<td style="text-align:right;">
-0.9287069
</td>
<td style="text-align:right;">
1.7940379
</td>
<td style="text-align:right;">
-0.4348410
</td>
<td style="text-align:right;">
-0.0452462
</td>
<td style="text-align:right;">
-0.4717038
</td>
<td style="text-align:right;">
-0.0894187
</td>
</tr>
</tbody>
</table>
</div>
<div id="section-on-base-plus-slugging-ops" class="section level3">
<h3>On Base Plus Slugging (OPS)</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#exercises-36">textbook section - 18.10 Excersises</a>.</p>
</div>
<p>Since the 1980s, sabermetricians have used a summary statistic different from batting average to evaluate players. They realized walks were important and that doubles, triples, and HRs, should be weighed more than singles. As a result, they proposed the following metric:</p>
<p><span class="math display">\[
\frac{\mbox{BB}}{\mbox{PA}} + \frac{\mbox{Singles} + 2 \mbox{Doubles} + 3 \mbox{Triples} + 4\mbox{HR}}{\mbox{AB}}
\]</span></p>
<p>They called this on-base-percentage plus slugging percentage (OPS). Although the sabermetricians probably did not use regression, here we show how this metric is close to what one gets with regression.</p>
</div>
<div id="section-regression-fallacy" class="section level3">
<h3>Regression Fallacy</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#the-regression-fallacy">textbook section - 18.8 The regression fallacy</a>.</p>
</div>
<p>Wikipedia defines the <em>sophomore slump</em> as:</p>
<blockquote>
<p>A sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels).</p>
</blockquote>
<p>In Major League Baseball, the rookie of the year (ROY) award is given to the first-year player who is judged to have performed the best. The <em>sophmore slump</em> phrase is used to describe the observation that ROY award winners don’t do as well during their second year. For example, this <a href="http://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715">Fox Sports article</a> asks “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”.</p>
<p>Does the data confirm the existence of a sophomore slump? Let’s take a look. Examining the data for batting average, we see that this observation holds true for the top performing ROYs:</p>
<!--The data is available in the Lahman library, but we have to do some work to create a table with the statistics for all the ROY. First we create a table with player ID, their names, and their most played position.-->
<!--
Now, we will create a table with only the ROY award winners and add their batting statistics. We filter out pitchers, since pitchers are not given awards for batting and we are going to focus on offense. Specifically, we will focus on batting average since it is the summary that most pundits talk about when discussing the sophomore slump:
-->
<!--
We also will keep only the rookie and sophomore seasons and remove players that did not play sophomore seasons:
-->
<!--
Finally, we will use the `spread` function to have one column for the rookie and sophomore years batting averages:
-->
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
nameFirst
</th>
<th style="text-align:left;">
nameLast
</th>
<th style="text-align:right;">
rookie_year
</th>
<th style="text-align:right;">
rookie
</th>
<th style="text-align:right;">
sophomore
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Willie
</td>
<td style="text-align:left;">
McCovey
</td>
<td style="text-align:right;">
1959
</td>
<td style="text-align:right;">
0.3541667
</td>
<td style="text-align:right;">
0.2384615
</td>
</tr>
<tr>
<td style="text-align:left;">
Ichiro
</td>
<td style="text-align:left;">
Suzuki
</td>
<td style="text-align:right;">
2001
</td>
<td style="text-align:right;">
0.3497110
</td>
<td style="text-align:right;">
0.3214838
</td>
</tr>
<tr>
<td style="text-align:left;">
Al
</td>
<td style="text-align:left;">
Bumbry
</td>
<td style="text-align:right;">
1973
</td>
<td style="text-align:right;">
0.3370787
</td>
<td style="text-align:right;">
0.2333333
</td>
</tr>
<tr>
<td style="text-align:left;">
Fred
</td>
<td style="text-align:left;">
Lynn
</td>
<td style="text-align:right;">
1975
</td>
<td style="text-align:right;">
0.3314394
</td>
<td style="text-align:right;">
0.3136095
</td>
</tr>
<tr>
<td style="text-align:left;">
Albert
</td>
<td style="text-align:left;">
Pujols
</td>
<td style="text-align:right;">
2001
</td>
<td style="text-align:right;">
0.3288136
</td>
<td style="text-align:right;">
0.3135593
</td>
</tr>
</tbody>
</table>
<p>In fact, the proportion of players that have a lower batting average their sophomore year is 0.6862745.</p>
<p>So is it “jitters” or “jinx”? To answer this question, let’s turn our attention to all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year).</p>
<!--We perform similar operations to what we did above: -->
<p>The same pattern arises when we look at the top performers: batting averages go down for most of the top performers.</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
nameFirst
</th>
<th style="text-align:left;">
nameLast
</th>
<th style="text-align:right;">
2013
</th>
<th style="text-align:right;">
2014
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Miguel
</td>
<td style="text-align:left;">
Cabrera
</td>
<td style="text-align:right;">
0.3477477
</td>
<td style="text-align:right;">
0.3126023
</td>
</tr>
<tr>
<td style="text-align:left;">
Hanley
</td>
<td style="text-align:left;">
Ramirez
</td>
<td style="text-align:right;">
0.3453947
</td>
<td style="text-align:right;">
0.2828508
</td>
</tr>
<tr>
<td style="text-align:left;">
Michael
</td>
<td style="text-align:left;">
Cuddyer
</td>
<td style="text-align:right;">
0.3312883
</td>
<td style="text-align:right;">
0.3315789
</td>
</tr>
<tr>
<td style="text-align:left;">
Scooter
</td>
<td style="text-align:left;">
Gennett
</td>
<td style="text-align:right;">
0.3239437
</td>
<td style="text-align:right;">
0.2886364
</td>
</tr>
<tr>
<td style="text-align:left;">
Joe
</td>
<td style="text-align:left;">
Mauer
</td>
<td style="text-align:right;">
0.3235955
</td>
<td style="text-align:right;">
0.2769231
</td>
</tr>
</tbody>
</table>
<p>But these are not rookies! Also, look at what happens to the worst performers of 2013:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
nameFirst
</th>
<th style="text-align:left;">
nameLast
</th>
<th style="text-align:right;">
2013
</th>
<th style="text-align:right;">
2014
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Danny
</td>
<td style="text-align:left;">
Espinosa
</td>
<td style="text-align:right;">
0.1582278
</td>
<td style="text-align:right;">
0.2192192
</td>
</tr>
<tr>
<td style="text-align:left;">
Dan
</td>
<td style="text-align:left;">
Uggla
</td>
<td style="text-align:right;">
0.1785714
</td>
<td style="text-align:right;">
0.1489362
</td>
</tr>
<tr>
<td style="text-align:left;">
Jeff
</td>
<td style="text-align:left;">
Mathis
</td>
<td style="text-align:right;">
0.1810345
</td>
<td style="text-align:right;">
0.2000000
</td>
</tr>
<tr>
<td style="text-align:left;">
B. J.
</td>
<td style="text-align:left;">
Upton
</td>
<td style="text-align:right;">
0.1841432
</td>
<td style="text-align:right;">
0.2080925
</td>
</tr>
<tr>
<td style="text-align:left;">
Adam
</td>
<td style="text-align:left;">
Rosales
</td>
<td style="text-align:right;">
0.1904762
</td>
<td style="text-align:right;">
0.2621951
</td>
</tr>
</tbody>
</table>
<p>Their batting averages mostly go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as the sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect:</p>
<p><img src="datsci_07_files/figure-html/regression-fallacy-1.png" width="40%" /></p>
<p>The correlation is 0.460254 and the data look very much like a bivariate normal distribution, which means we predict a 2014 batting average <span class="math inline">\(Y\)</span> for any given player that had a 2013 batting average <span class="math inline">\(X\)</span> with:</p>
<p><span class="math display">\[ \frac{Y - .255}{.032} = 0.46 \left( \frac{X - .261}{.023}\right) \]</span></p>
<p>Because the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It’s not a jinx; it’s just due to chance. The ROY are selected from the top values of <span class="math inline">\(X\)</span> so it is expected that <span class="math inline">\(Y\)</span> will regress to the mean.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Regression can bring about errors in reasoning, especially when interpreting individual observations.</p></li>
<li><p>The example showed in this section demonstrates that the <strong>“sophomore slump”</strong> observed in the data is caused by regressing to the mean.</p></li>
</ul>
</div>
</div>
<div id="section-measurement-error-models" class="section level3">
<h3>Measurement Error Models</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/linear-models.html#measurement-error-models">textbook section - 18.9 Measurement error models</a>.</p>
</div>
<p>Up to now, all our linear regression examples have been applied to two or more random variables. We assume the pairs are bivariate normal and use this to motivate a linear model. This approach covers most real-life examples of linear regression. The other major application comes from measurement errors models. In these applications, it is common to have a non-random covariate, such as time, and randomness is introduced from measurement error rather than sampling or natural variability.</p>
<p>To understand these models, imagine you are Galileo in the 16th century trying to describe the velocity of a falling object. An assistant climbs the Tower of Pisa and drops a ball, while several other assistants record the position at different times. Let’s simulate some data using the equations we know today and adding some measurement error. The <strong>dslabs</strong> function <code>rfalling_object</code> generates these simulations:</p>
<pre class="r"><code>library(dslabs)
falling_object &lt;- rfalling_object()</code></pre>
<p>The assistants hand the data to Galileo and this is what he sees:</p>
<pre class="r"><code>falling_object %&gt;% 
  ggplot(aes(time, observed_distance)) + 
  geom_point() +
  ylab(&quot;Distance in meters&quot;) + 
  xlab(&quot;Time in seconds&quot;)</code></pre>
<p><img src="datsci_07_files/figure-html/gravity-1.png" width="624" /></p>
<p>Galileo does not know the exact equation, but by looking at the plot above, he deduces that the position should follow a parabola, which we can write like this:</p>
<p><span class="math display">\[ f(x) = \beta_0 + \beta_1 x + \beta_2 x^2\]</span></p>
<p>The data does not fall exactly on a parabola. Galileo knows this is due to measurement error. His helpers make mistakes when measuring the distance. To account for this, he models the data with:</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i, i=1,\dots,n \]</span></p>
<p>with <span class="math inline">\(Y_i\)</span> representing distance in meters, <span class="math inline">\(x_i\)</span> representing time in seconds, and <span class="math inline">\(\varepsilon\)</span> accounting for measurement error. The measurement error is assumed to be random, independent from each other, and having the same distribution for each <span class="math inline">\(i\)</span>. We also assume that there is no bias, which means the expected value <span class="math inline">\(\mbox{E}[\varepsilon] = 0\)</span>.</p>
<p>Note that this is a linear model because it is a linear combination of known quantities (<span class="math inline">\(x\)</span> and <span class="math inline">\(x^2\)</span> are known) and unknown parameters (the <span class="math inline">\(\beta\)</span>s are unknown parameters to Galileo). Unlike our previous examples, here <span class="math inline">\(x\)</span> is a fixed quantity; we are not conditioning.</p>
<p>To pose a new physical theory and start making predictions about other falling objects, Galileo needs actual numbers, rather than unknown parameters. Using LSE seems like a reasonable approach. How do we find the LSE?</p>
<p>LSE calculations do not require the errors to be approximately normal. The <code>lm</code> function will find the <span class="math inline">\(\beta\)</span> s that will minimize the residual sum of squares:</p>
<pre class="r"><code>fit &lt;- falling_object %&gt;% 
  mutate(time_sq = time^2) %&gt;% 
  lm(observed_distance~time+time_sq, data=.)
tidy(fit)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["estimate"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["std.error"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["statistic"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["p.value"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"56.1260576","3":"0.5915639","4":"94.8774156","5":"2.226192e-17"},{"1":"time","2":"-0.7859522","3":"0.8452432","4":"-0.9298534","5":"3.724035e-01"},{"1":"time_sq","2":"-4.5308725","3":"0.2507522","4":"-18.0691250","5":"1.583669e-09"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Let’s check if the estimated parabola fits the data. The <strong>broom</strong> function <code>augment</code> lets us do this easily:</p>
<pre class="r"><code>augment(fit) %&gt;% 
  ggplot() +
  geom_point(aes(time, observed_distance)) + 
  geom_line(aes(time, .fitted), col = &quot;blue&quot;)</code></pre>
<p><img src="datsci_07_files/figure-html/falling-object-fit-1.png" width="624" /></p>
<p>Thanks to my high school physics teacher, I know that the equation for the trajectory of a falling object is:</p>
<p><span class="math display">\[d = h_0 + v_0 t -  0.5 \times 9.8 t^2\]</span></p>
<p>with <span class="math inline">\(h_0\)</span> and <span class="math inline">\(v_0\)</span> the starting height and velocity, respectively. The data we simulated above followed this equation and added measurement error to simulate <code>n</code> observations for dropping the ball <span class="math inline">\((v_0=0)\)</span> from the tower of Pisa <span class="math inline">\((h_0=55.86)\)</span>.</p>
<p>These are consistent with the parameter estimates:</p>
<pre class="r"><code>tidy(fit, conf.int = TRUE)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["estimate"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["std.error"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["statistic"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["p.value"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["conf.low"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["conf.high"],"name":[7],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"56.1260576","3":"0.5915639","4":"94.8774156","5":"2.226192e-17","6":"54.824034","7":"57.428081"},{"1":"time","2":"-0.7859522","3":"0.8452432","4":"-0.9298534","5":"3.724035e-01","6":"-2.646320","7":"1.074415"},{"1":"time_sq","2":"-4.5308725","3":"0.2507522","4":"-18.0691250","5":"1.583669e-09","6":"-5.082774","7":"-3.978971"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The Tower of Pisa height is within the confidence interval for <span class="math inline">\(\beta_0\)</span>, the initial velocity 0 is in the confidence interval for <span class="math inline">\(\beta_1\)</span> (note the p-value is larger than 0.05), and the acceleration constant is in a confidence interval for <span class="math inline">\(-2 \times \beta_2\)</span>.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Up to now, all our linear regression examples have been applied to two or more random variables. We assume the pairs are bivariate normal and use this to motivate a linear model.</p></li>
<li><p>Another use for linear regression is with <strong>measurement error models</strong>, where it is common to have a non-random covariate (such as time). Randomness is introduced from measurement error rather than sampling or natural variability.</p></li>
</ul>
</div>
</div>
<div id="section-assessment-regression-and-baseball-part-1" class="section level3">
<h3>2.4 Assessment: Regression and Baseball, part 1</h3>
<p>Insert assessment here</p>
</div>
<div id="section-assessment-regression-and-baseball-part-2" class="section level3">
<h3>2.4 Assessment: Regression and Baseball, part 2</h3>
<p>Insert assessment here</p>
</div>
<div id="section-assessment-linear-models" class="section level3">
<h3>2.4 Assessment: Linear Models</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-3-confounding" class="section level2">
<h2>Section 3: Confounding</h2>
<p>In the <strong>Confounding</strong> section, you will learn what is perhaps the most important lesson of statistics: that correlation is not causation.</p>
<p>After completing this section, you will be able to:</p>
<ul>
<li><p>Identify examples of <strong>spurious correlation</strong> and explain how <strong>data dredging</strong> can lead to spurious correlation.</p></li>
<li><p>Explain how <strong>outliers</strong> can drive correlation and learn to adjust for outliers using <strong>Spearman correlation</strong>.</p></li>
<li><p>Explain how <strong>reversing cause and effect</strong> can lead to associations being confused with causation.</p></li>
<li><p>Understand how <strong>confounders</strong> can lead to the misinterpretation of associations.</p></li>
<li><p>Explain and give examples of <strong>Simpson’s Paradox</strong>.</p></li>
</ul>
<p>This section has one part: <strong>Correlation is Not Causation</strong>. There is a comprehension checks at the end of this part, along with an assessment at the end of the section.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your own learning.</p>
<div id="section-correlation-is-not-causation-spurious-correlation" class="section level3">
<h3>3. Correlation is Not Causation: Spurious Correlation</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/association-is-not-causation.html#spurious-correlation">textbook section - 19.1 Spurious correlation</a>.</p>
</div>
<p><em>Association is not causation</em> is perhaps the most important lesson one learns in a statistics class. <em>Correlation is not causation</em> is another way to say this. Throughout the Statistics part of the textbook, we have described tools useful for quantifying associations between variables. However, we must be careful not to over-interpret these associations.</p>
<p>There are many reasons that a variable <span class="math inline">\(X\)</span> can be correlated with a variable <span class="math inline">\(Y\)</span> without having any direct effect on <span class="math inline">\(Y\)</span>. Here we examine four common ways that can lead to misinterpreting data.</p>
</div>
<div id="section-spurious-correlation" class="section level3">
<h3>Spurious correlation</h3>
<p>The following comical example underscores that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption.</p>
<p><img src="datsci_07_files/figure-html/divorce-versus-margarine-1.png" width="624" /></p>
<p>Does this mean that margarine causes divorces? Or do divorces cause people to eat more margarine? Of course the answer to both these questions is no. This is just an example of what we call a <em>spurious correlation</em>.</p>
<p>You can see many more absurd examples on the <a href="http://tylervigen.com/spurious-correlations">Spurious Correlations website</a>.</p>
<p>The cases presented in the spurious correlation site are all instances of what is generally called <em>data dredging</em>, <em>data fishing</em>, or <em>data snooping</em>. It’s basically a form of what in the US they call <em>cherry picking</em>. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.</p>
<p>A Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble:</p>
<pre class="r"><code>N &lt;- 25
g &lt;- 1000000
sim_data &lt;- tibble(group = rep(1:g, each=N), 
                   x = rnorm(N * g), 
                   y = rnorm(N * g))</code></pre>
<p>The first column denotes group. We created groups and for each one we generated a pair of independent vectors, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, with 25 observations each, stored in the second and third columns. Because we constructed the simulation, we know that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not correlated.</p>
<p>Next, we compute the correlation between <code>X</code> and <code>Y</code> for each group and look at the max:</p>
<pre class="r"><code>res &lt;- sim_data %&gt;% 
  group_by(group) %&gt;% 
  summarize(r = cor(x, y)) %&gt;% 
  arrange(desc(r))
res</code></pre>
<p>We see a maximum correlation of 0.8037382 and if you just plot the data from the group achieving this correlation, it shows a convincing plot that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are in fact correlated:</p>
<pre class="r"><code>sim_data %&gt;% filter(group == res$group[which.max(res$r)]) %&gt;%
  ggplot(aes(x, y)) +
  geom_point() + 
  geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="images/pic1.png" width="654" /></p>
<p>Remember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation:</p>
<pre class="r"><code>res %&gt;% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = &quot;black&quot;)</code></pre>
<p><img src="images/pic2.png" width="668" /></p>
<p>It’s just a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.204, the largest one will be close to 1.</p>
<p>If we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:</p>
<pre class="r"><code>library(broom)
sim_data %&gt;% 
  filter(group == res$group[which.max(res$r)]) %&gt;%
  do(tidy(lm(y ~ x, data = .))) %&gt;% 
  filter(term == &quot;x&quot;)
# # A tibble: 1 x 5
#   term  estimate std.error statistic    p.value
#   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
# 1 x        0.659     0.105      6.27 0.00000212</code></pre>
<p>This particular form of data dredging is referred to as <em>p-hacking</em>. P-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results. In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and several exposures and report only the one exposure that resulted in a small p-value. Furthermore, they might try fitting several different models to account for confounding and pick the one that yields the smallest p-value. In experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported. This does not necessarily happen due to unethical behavior, but rather as a result of statistical ignorance or wishful thinking. In advanced statistics courses, you can learn methods to adjust for these multiple comparisons.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Association/correlation is not causation.</p></li>
<li><p>p-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results.</p></li>
</ul>
</div>
</div>
<div id="section-correlation-is-not-causation-outliers" class="section level3">
<h3>Correlation is Not Causation: Outliers</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/association-is-not-causation.html#outliers-1">textbook section - 19.2 Outliers</a>.</p>
</div>
<p>Suppose we take measurements from two independent outcomes, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and we standardize the measurements. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using:</p>
<pre class="r"><code>set.seed(1985)
x &lt;- rnorm(100,100,1)
y &lt;- rnorm(100,84,1)
x[-23] &lt;- scale(x[-23])
y[-23] &lt;- scale(y[-23])</code></pre>
<p>The data look like this:</p>
<pre class="r"><code>qplot(x, y)</code></pre>
<p><img src="datsci_07_files/figure-html/outlier-1.png" width="624" /></p>
<p>Not surprisingly, the correlation is very high:</p>
<pre class="r"><code>cor(x,y)</code></pre>
<pre><code>## [1] 0.9878382</code></pre>
<p>But this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:</p>
<pre class="r"><code>cor(x[-23], y[-23])</code></pre>
<pre><code>## [1] -0.04419032</code></pre>
<p>In the textbook <a href="https://rafalab.github.io/dsbook/robust-summaries.html">(Section - 11 Robust summaries)</a> we described alternatives to the average and standard deviation that are robust to outliers. There is also an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called <em>Spearman correlation</em>. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:</p>
<pre class="r"><code>qplot(rank(x), rank(y))</code></pre>
<p><img src="datsci_07_files/figure-html/scatter-plot-of-ranks-1.png" width="624" /></p>
<p>The outlier is no longer associated with a very large value and the correlation comes way down:</p>
<pre class="r"><code>cor(rank(x), rank(y))</code></pre>
<pre><code>## [1] 0.002508251</code></pre>
<p>Spearman correlation can also be calculated like this:</p>
<pre class="r"><code>cor(x, y, method = &quot;spearman&quot;)</code></pre>
<pre><code>## [1] 0.002508251</code></pre>
<p>There are also methods for robust fitting of linear models which you can learn about in, for instance, this book: Robust Statistics: Edition 2 by Peter J. Huber &amp; Elvezio M. Ronchetti.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>A Correlations can be caused by <strong>outliers</strong>.</p></li>
<li><p>The <strong>Spearman correlation</strong> is calculated based on the ranks of data.</p></li>
</ul>
</div>
</div>
<div id="section-correlation-is-not-causation-reversing-cause-and-effect" class="section level3">
<h3>Correlation is Not Causation: Reversing Cause and Effect</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/association-is-not-causation.html#reversing-cause-and-effect">textbook section - 19.3 Reversing cause and effect</a>.</p>
</div>
<p>Another way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.</p>
<p>A form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is <a href="https://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated">Overrated</a>. Consider this quote from the article:</p>
<blockquote>
<blockquote>
<p>When we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades… Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.</p>
</blockquote>
</blockquote>
<p>A very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school.</p>
<p>We can easily construct an example of cause and effect reversal using the father and son height data. If we fit the model:</p>
<p><span class="math display">\[X_i = \beta_0 + \beta_1 y_i + \varepsilon_i, i=1, \dots, N\]</span></p>
<p>to the father and son height data, with <span class="math inline">\(X_i\)</span> the father height and <span class="math inline">\(y_i\)</span> the son height, we do get a statistically significant result:</p>
<pre class="r"><code>library(HistData)
data(&quot;GaltonFamilies&quot;)
GaltonFamilies %&gt;%
  filter(childNum == 1 &amp; gender == &quot;male&quot;) %&gt;%
  select(father, childHeight) %&gt;%
  rename(son = childHeight) %&gt;% 
  do(tidy(lm(father ~ son, data = .)))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["estimate"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["std.error"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["statistic"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["p.value"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"33.9653853","3":"4.56815126","4":"7.435258","5":"4.312028e-12"},{"1":"son","2":"0.4986676","3":"0.06479567","4":"7.696002","5":"9.473336e-13"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted so as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it’s the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the interpretation.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Another way association can be confused with causation is when the <strong>cause and effect are reversed</strong>.</p></li>
<li><p>As discussed in this section, in the Galton data, when father and son were reversed in the regression, the model was technically correct. The estimates and p-values were obtained correctly as well. What was incorrect was the <strong>interpretation</strong> of the model.</p></li>
</ul>
</div>
</div>
<div id="section-correlation-is-not-causation-confounders" class="section level3">
<h3>Correlation is Not Causation: Confounders</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/association-is-not-causation.html#confounders">textbook section - 19.4 Confounders</a>.</p>
</div>
<p>Confounders are perhaps the most common reason that leads to associations begin misinterpreted.</p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are correlated, we call <span class="math inline">\(Z\)</span> a <em>confounder</em> if changes in <span class="math inline">\(Z\)</span> causes changes in both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Earlier, when studying baseball data, we saw how Home Runs was a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs. In some cases, we can use linear models to account for confounders. However, this is not always the case.</p>
<p>Incorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions.</p>
</div>
<div id="section-example-uc-berkeley-admissions" class="section level3">
<h3>Example: UC Berkeley admissions</h3>
<p>Admission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). We can load the data and <!--compute the percent of men and women that were accepted like this:


```r
data(admissions)
admissions %>% group_by(gender) %>% 
  summarize(percentage = 
              round(sum(admitted*applicants)/sum(applicants),1))
```

<div data-pagedtable="false">
  <script data-pagedtable-source type="application/json">
{"columns":[{"label":["gender"],"name":[1],"type":["chr"],"align":["left"]},{"label":["percentage"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"men","2":"44.5"},{"1":"women","2":"30.3"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
--> a statistical test, which clearly rejects the hypothesis that gender and admission are independent:</p>
<pre class="r"><code>data(admissions)
admissions %&gt;% group_by(gender) %&gt;% 
  summarize(total_admitted = round(sum(admitted / 100 * applicants)), 
            not_admitted = sum(applicants) - sum(total_admitted)) %&gt;%
  select(-gender) %&gt;% 
  do(tidy(chisq.test(.))) %&gt;% .$p.value</code></pre>
<pre><code>## [1] 1.055797e-21</code></pre>
<p>But closer inspection shows a paradoxical result. Here are the percent admissions by major:</p>
<pre class="r"><code>admissions %&gt;% select(major, gender, admitted) %&gt;%
  spread(gender, admitted) %&gt;%
  mutate(women_minus_men = women - men)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["major"],"name":[1],"type":["chr"],"align":["left"]},{"label":["men"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["women"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["women_minus_men"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"A","2":"62","3":"82","4":"20"},{"1":"B","2":"63","3":"68","4":"5"},{"1":"C","2":"37","3":"34","4":"-3"},{"1":"D","2":"33","3":"35","4":"2"},{"1":"E","2":"28","3":"24","4":"-4"},{"1":"F","2":"6","3":"7","4":"1"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Four out of the six majors favor women. More importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals.</p>
<p>The paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability.</p>
<p>So let’s define three variables: <span class="math inline">\(X\)</span> is 1 for men and 0 for women, <span class="math inline">\(Y\)</span> is 1 for those admitted and 0 otherwise, and <span class="math inline">\(Z\)</span> quantifies the selectivity of the major. A gender bias claim would be based on the fact that <span class="math inline">\(\mbox{Pr}(Y=1 | X = x)\)</span> is higher for <span class="math inline">\(x=1\)</span> than <span class="math inline">\(x=0\)</span>. However, <span class="math inline">\(Z\)</span> is an important confounder to consider. Clearly <span class="math inline">\(Z\)</span> is associated with <span class="math inline">\(Y\)</span>, as the more selective a major, the lower <span class="math inline">\(\mbox{Pr}(Y=1 | Z = z)\)</span>. But is major selectivity <span class="math inline">\(Z\)</span> associated with gender <span class="math inline">\(X\)</span>?</p>
<p>One way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants:</p>
<pre class="r"><code>admissions %&gt;% 
  group_by(major) %&gt;% 
  summarize(major_selectivity = sum(admitted * applicants)/sum(applicants),
            percent_women_applicants = sum(applicants * (gender==&quot;women&quot;)) /
                                             sum(applicants) * 100) %&gt;%
  ggplot(aes(major_selectivity, percent_women_applicants, label = major)) +
  geom_text()</code></pre>
<p><img src="datsci_07_files/figure-html/uc-berkeley-majors-1.png" width="624" /></p>
<p>There seems to be association. The plot suggests that women were much more likely to apply to the two “hard” majors: gender and major’s selectivity are confounded. Compare, for example, major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women, while less than 30% of the applicants of major B were women.</p>
</div>
<div id="section-confounding-explained-graphically" class="section level3">
<h3>Confounding explained graphically</h3>
<p>The following plot shows the number of applicants that were admitted and those that were not by:</p>
<p><img src="datsci_07_files/figure-html/confounding-1.png" width="624" /></p>
<!--

```r
admissions %>% 
  mutate(percent_admitted = admitted * applicants/sum(applicants)) %>%
  ggplot(aes(gender, y = percent_admitted, fill = major)) +
  geom_bar(stat = "identity", position = "stack")
```

<img src="datsci_07_files/figure-html/confounding-2-1.png" width="624" />
-->
<p>It also breaks down the acceptances by major. This breakdown allows us to see that the majority of accepted men came from two majors: A and B. It also lets us see that few women applied to these majors.</p>
</div>
<div id="section-average-after-stratifying" class="section level3">
<h3>Average after stratifying</h3>
<p>In this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away:</p>
<pre class="r"><code>admissions %&gt;% 
  ggplot(aes(major, admitted, col = gender, size = applicants)) +
  geom_point()</code></pre>
<p><img src="datsci_07_files/figure-html/admission-by-major-1.png" width="624" /></p>
<p>Now we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest majors, A and B.</p>
<p>If we average the difference by major, we find that the percent is actually 3.5% higher for women.</p>
<pre class="r"><code>admissions %&gt;%  group_by(gender) %&gt;% summarize(average = mean(admitted))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["gender"],"name":[1],"type":["chr"],"align":["left"]},{"label":["average"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"men","2":"38.16667"},{"1":"women","2":"41.66667"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>If X and Y are correlated, we call Z a <strong>confounder</strong> if changes in Z causes changes in both X and Y.</li>
</ul>
</div>
</div>
<div id="section-simpsons-paradox" class="section level3">
<h3>Simpson’s Paradox</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The content is discussed within the <a href="https://rafalab.github.io/dsbook/association-is-not-causation.html#simpsons-paradox">textbook section - 19.5 Simpson’s paradox</a>.</p>
</div>
<p>The case we have just covered is an example of Simpson’s paradox. It is called a paradox because we see the sign of the correlation flip when comparing the entire publication and specific strata. As an illustrative example, suppose you have three random variables <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, and <span class="math inline">\(Z\)</span> and that we observe realizations of these. Here is a plot of simulated observations for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> along with the sample correlation:</p>
<p><img src="datsci_07_files/figure-html/simpsons-paradox-1.png" width="624" /></p>
<p>You can see that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are negatively correlated. However, once we stratify by <span class="math inline">\(Z\)</span> (shown in different colors below) another pattern emerges:</p>
<p><img src="datsci_07_files/figure-html/simpsons-paradox-explained-1.png" width="624" /></p>
<p>It is really <span class="math inline">\(Z\)</span> that is negatively correlated with <span class="math inline">\(X\)</span>. If we stratify by <span class="math inline">\(Z\)</span>, the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are actually positively correlated as seen in the plot above.</p>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>Simpson’s Paradox happens when we see the sign of the correlation flip when comparing the entire dataset with specific strata.</li>
</ul>
</div>
</div>
<div id="section-assessment-correlation-is-not-causation" class="section level3">
<h3>3.1 Assessment: Correlation is Not Causation</h3>
<p>Insert assessment here</p>
</div>
<div id="section-assessment-confounding" class="section level3">
<h3>3.2 Assessment: Confounding</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-acknowledgement" class="section level2">
<h2>Acknowledgement</h2>
<p>I am extremely grateful to <a href="http://rafalab.github.io/pages/about.html">Prof Rafael Irizarry</a> for his support and encouragement to create this interactive tutorial which is based on his freely available textbook <a href="https://rafalab.github.io/dsbook/">Introduction to Data Science</a>. The textbook has been developed as the basis for the associated edX Course Series <em>HarvardX Professional Certificate in Data Science</em> and this tutorial follows the structure of this online course. I’m further very grateful to <a href="https://profiles.sussex.ac.uk/p9846-andy-field">Andy Field</a> for his generous permission to use his <code>discovr</code> package as a basis for the development of this tutorial. Thanks to his amazing <code>discovr</code> package I also indirectly benefited from the work of <a href="https://www.allisonhorst.com/">Allison Horst</a> and her very informative blog post on <a href="https://education.rstudio.com/blog/2020/05/learnr-for-remote/">styling learnr tutorials with CSS</a> as well as her CSS template file which I adapted here.</p>

<script type="application/shiny-prerendered" data-context="server-start">
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

library(learnr) #necessary to render tutorial correctly

library(forcats)
library(ggplot2)
library(htmltools)
library(kableExtra)
library(lubridate)
library(magrittr)
library(tibble)


source("./www/datsci_helpers.R")
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::register_http_handlers(session, metadata = NULL)
</script>
 
<script type="application/shiny-prerendered" data-context="server">
session$onSessionEnded(function() {
        learnr:::session_stop_event(session)
      })
</script>
 <!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/lumen.min.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["navigation"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/navigation-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tabsets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["default.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["font-awesome"]},{"type":"character","attributes":{},"value":["5.1.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/fontawesome"]}]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["css/all.css","css/v4-shims.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootbox"]},{"type":"character","attributes":{},"value":["4.4.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/bootbox"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["bootbox.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["idb-keyvalue"]},{"type":"character","attributes":{},"value":["3.2.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/idb-keyval"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["idb-keyval-iife-compat.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99]}},"value":[{"type":"character","attributes":{},"value":["assertthat","backports","base","blob","broom","cellranger","cli","colorspace","compiler","crayon","curl","datasets","DBI","dbplyr","digest","dplyr","dslabs","ellipsis","evaluate","fansi","farver","fastmap","forcats","fs","generics","ggplot2","glue","graphics","grDevices","grid","gridExtra","gtable","haven","highr","HistData","hms","htmltools","htmlwidgets","httpuv","httr","jsonlite","kableExtra","knitr","labeling","Lahman","later","lattice","learnr","lifecycle","lpSolve","lubridate","magrittr","markdown","MASS","Matrix","methods","mgcv","mime","modelr","munsell","nlme","pillar","pkgconfig","plyr","png","promises","purrr","R6","Rcpp","readr","readxl","reprex","reshape2","rlang","rmarkdown","rprojroot","rstudioapi","rvest","scales","selectr","shiny","splines","stats","stringi","stringr","tibble","tidyr","tidyselect","tidyverse","tools","utils","vctrs","viridisLite","webshot","withr","xfun","xml2","xtable","yaml"]},{"type":"character","attributes":{},"value":["0.2.1","1.1.8","4.0.2","1.2.1","0.7.0","1.1.0","2.0.2","1.4-1","4.0.2","1.3.4","4.3","4.0.2","1.1.0","1.4.4","0.6.25","1.0.0","0.7.3","0.3.1","0.14","0.4.1","2.0.3","1.0.1","0.5.0","1.4.2","0.0.2","3.3.2","1.4.1","4.0.2","4.0.2","4.0.2","2.3","0.3.0","2.3.1","0.8","0.8-6","0.5.3","0.5.0","1.5.1","1.5.4","1.4.2","1.7.0","1.1.0","1.29","0.3","8.0-0","1.1.0.1","0.20-41","0.10.1","0.2.0","5.6.15","1.7.9","1.5","1.1","7.3-52","1.2-18","4.0.2","1.8-31","0.9","0.1.8","0.5.0","3.1-148","1.4.6","2.0.3","1.8.6","0.1-7","1.1.1","0.3.4","2.4.1","1.0.5","1.3.1","1.3.1","0.3.0","1.4.4","0.4.7","2.3","1.3-2","0.11","0.3.6","1.1.1","0.4-2","1.5.0","4.0.2","4.0.2","1.4.6","1.4.0","3.0.3","1.1.0","1.1.0","1.3.0","4.0.2","4.0.2","0.3.2","0.3.0","0.5.2","2.2.0","0.16","1.3.2","1.8-4","2.2.1"]}]}]}
</script>
<!--/html_preserve-->
</div>
</div>

</div> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h2 class="title toc-ignore" style="display:none;">datsci_07: Linear Regression</h2>
<h4 class="author"><em>Rafael Irizarry &amp; Fatih Uenal</em></h4>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</div> <!-- bandContent page -->
</div> <!-- pageContent band -->




<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>

</html>
