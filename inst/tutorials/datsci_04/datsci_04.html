<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Rafael Irizarry &amp; Fatih Uenal" />


<meta name="progressive" content="true" />
<meta name="allow-skip" content="false" />

<title>datsci_04: Inference and Modeling</title>


<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>

<!-- taken from https://github.com/rstudio/rmarkdown/blob/67b7f5fc779e4cfdfd0f021d3d7745b6b6e17149/inst/rmd/h/default.html#L296-L362 -->
<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("section-TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>
<!-- end tabsets -->


<link rel="stylesheet" href="css/datsci_style.css" type="text/css" />

</head>

<body>



<div class="pageContent band">
<div class="bandContent page">

<div class="topics">

<html lang="en">
<div id="section-datsci_04-inference-and-modeling" class="section level1">
<h1>datsci_04: Inference and Modeling</h1>
<div id="section-introduction-and-welcome" class="section level2">
<h2>Introduction and Welcome!</h2>
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:100px;height:100px;" class = "img_left"></p>
<p><strong>Usage:</strong> This tutorial accompanies the textbook <a href="https://rafalab.github.io/dsbook/">Introduction to Data Science</a> by <a href="http://rafalab.github.io/pages/about.html">Prof Rafael Irizarry</a>. It contains material from the textbook which is offered under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a>.</p>
</div>
<div id="section-welcome-to-data-science-inference-and-modeling" class="section level3">
<h3>Welcome to <em>Data Science: Inference and Modeling</em>!</h3>
<p>We’re excited to have you join us in this course, which is designed to teach you inference and modeling, two of the most widely used statistical tools in data analysis.</p>
</div>
<div id="section-course-overview" class="section level3">
<h3>Course Overview</h3>
<p>This is the fourth in a series of courses in the Introduction to Data Science program. The courses in the program are designed to prepare you to do data analysis in <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>, from simple computations to machine learning. The courses are designed to be taken in order. A prerequisite for this course is courses 1 and 2 of the series or equivalent knowledge of basic <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> coding and data visualization. We recommend that you complete the first three courses in the series (<em>Data Science: <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> Basics</em> (datsci_01), <em>Data Science: Visualization</em> (datsci_02), and <em>Data Science: Probability</em> (datsci_03)) before taking this course.</p>
<p>The textbook for the Data Science course series is <a href="https://rafalab.github.io/dsbook/">freely available online</a>. This course corresponds to the Probability section of textbook, starting here.</p>
<p>This course assumes you are comfortable with basic math, algebra, and logical operations. We have some assignments in <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> that allow you to program directly in a browser-based interface. You will further have access to additional exercises to be completed on your local installation of <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>.</p>
<p>For in-class students enrolled in my University course, we partnered with DataCamp for some assignments in R that allow you to program directly in a browser-based interface. There are also some assignments that require a local installation of <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>. If you are not in-class, you will need to obtain a subscription to DataCamp.</p>
<p>Using a combination of a guided introduction lectures and more independent in-depth exploration, you will get to practice your new <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> skills on real-life applications.</p>
<p>Statistical inference and modeling are indispensable for analyzing data affected by chance, and thus essential for data scientists. In this course, you will learn these key concepts through a motivating case study on election forecasting.</p>
<p>This course will show you how inference and modeling can be applied to develop the statistical approaches that make polls an effective tool and we’ll show you how to do this using R. You will learn concepts necessary to define estimates and margins of errors and learn how you can use these to make predictions relatively well and also provide an estimate of the precision of your forecast.</p>
<p>Once you learn this you will be able to understand two concepts that are ubiquitous in data science: confidence intervals and p-values.</p>
<p>Finally, to understand statements about the probability of a candidate winning, you will learn about Bayesian modeling. At the end of the course, we will put it all together to recreate a simplified version of an election forecast model and apply it to the 2016 US presidential election.</p>
<p>This course builds upon probability theory, covered in the previous course in this series (<em>datsci_03</em>).</p>
<p>The class notes for this course series can be found in Professor Irizarry’s <a href="https://rafalab.github.io/dsbook/">freely available Introduction to Data Science book</a>. The textbook is also freely available in PDF format on <a href="https://leanpub.com/datasciencebook">Leanpub</a>.</p>
<p>This course corresponds to the textbook chapters <a href="https://rafalab.github.io/dsbook/inference.html">Statistical Inference</a> and <a href="https://rafalab.github.io/dsbook/models.html">Statistical Models</a>.</p>
</div>
<div id="section-in-this-course-you-will-learn" class="section level3">
<h3>In this course, you will learn:</h3>
<ul>
<li>The concepts necessary to define estimates and margins of errors of populations, parameters, estimates, and standard errors in order to make predictions about data</li>
<li>How to use models to aggregate data from different sources</li>
<li>The very basics of Bayesian statistics and predictive modeling</li>
</ul>
</div>
<div id="section-course-overview-1" class="section level3">
<h3>Course overview</h3>
<p><strong>Section 1:</strong> Parameters and Estimates</p>
<ul>
<li>You will learn how to estimate population parameters.</li>
</ul>
<p><strong>Section 2:</strong> The Central Limit Theorem in Practice</p>
<ul>
<li>You will apply the central limit theorem to assess how close a sample estimate is to the population parameter of interest.</li>
</ul>
<p><strong>Section 3:</strong> Confidence Intervals and p-Values</p>
<ul>
<li>You will learn how to calculate confidence intervals and learn about the relationship between confidence intervals and p-values.</li>
</ul>
<p><strong>Section 4:</strong> Statistical Models</p>
<ul>
<li>You will learn about statistical models in the context of election forecasting.</li>
</ul>
<p><strong>Section 5:</strong> Bayesian Statistics</p>
<ul>
<li>You will learn about Bayesian statistics through looking at examples from rare disease diagnosis and baseball.</li>
</ul>
<p><strong>Section 6:</strong> Election Forecasting</p>
<ul>
<li>You will learn about election forecasting, building on what you’ve learned in the previous sections about statistical modeling and Bayesian statistics.</li>
</ul>
<p><strong>Section 7:</strong> Association Tests</p>
<ul>
<li>You will learn how to use association and chi-squared tests to perform inference for binary, categorical, and ordinal data through an example looking at research funding rates.</li>
</ul>
</div>
<div id="section-meet-the-course-instructor" class="section level3">
<h3>Meet the Course Instructor</h3>
<div class="infobox">
<p><img src="images/photo2014.jpg" alt="Dr Fatih Uenal." style="width:150px;height:200px;" class = "img_left" ></p>
<p><strong>Fatih Uenal</strong> is currenlty a Visitng Postdoctoral Researcher at the University of Cambridge, Department of Psychology, where he conducts research on the psychology of anthropocentrism and social and ecological dominance. Prior to his current position, he has worked as a postdoc at <a href="https://scholar.harvard.edu/fatih-uenal/home">Harvard University</a>. Together with <a href="http://rafalab.github.io/pages/about.html">Prof Rafael Irizarry</a> he programmed this interactive tutorial based on the the textbook <a href="https://rafalab.github.io/dsbook/"><em>Introduction to Data Science</em></a>. This interactive tutorial is developed using the <code>learnr</code> package. It has a general social scientists audience in mind and is suited for undergrad and graduate levels of study.</p>
<p>Webpage: <a href="https://scholar.harvard.edu/fatih-uenal/home" class="uri">https://scholar.harvard.edu/fatih-uenal/home</a></p>
</div>
<hr />
</div>
<div id="section-essential-course-information" class="section level3">
<h3>Essential Course Information</h3>
<div id="section-course-objectives" class="section level4">
<h4><strong>Course Objectives</strong></h4>
<p>“Data science” is a catch-all term used to describe the practice of working with and analyzing messy data sources to draw meaningful conclusions using techniques developed by computer scientists and computational statisticians. The purpose of this course is to give students who are training as quantitative social scientists a broad introduction to this skillset via the statistical programming language, <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>. You will learn how to conduct many statistical analyses such as univariate statistics (e.g., ANOVA, correlation, regression) in <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> that you may have already done in SPSS, Excel, or another such program. Additionally, we will build on this foundation to explore new skillsets uncommon in the social sciences, such as natural language processing, automated data curation, and machine learning.</p>
<p>At the end of this course you will be able to:</p>
<ul>
<li><p>To answer research questions in Social Sciences (e.g., Psychology) with data</p></li>
<li><p>Understand the basics of research designs in Social Sciences, and how they relate to data-analysis strategies</p></li>
<li><p>Develop an intuitive, practical, and conceptual understanding of strategies for asking and answering questions with data</p></li>
<li><p>To use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>, a free and open-source statistics software program.</p></li>
<li><p>Develop a basic understanding of frequently used Data Science Techniques.</p></li>
<li><p>Practice your newly acquired skills with interesting, interactive, and fun projects.</p></li>
</ul>
<p><strong>NOTE</strong>: The schedule and procedures described in this syllabus are subject to change depending on specific needs and requirements. You will always be notified of changes on the homepage (see “last update”).</p>
</div>
<div id="section-course-structure" class="section level4">
<h4><strong>Course Structure</strong></h4>
<p>This is the first module in a series of a 8 week-intensive course. and I suggest that you devote approx 10 hours a week to learning <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>, or if you are teaching graduate students, I’d recommend adopting the schedule below, which is designed for an intense but doable semester-long course, one module per week. It is intended to take the average graduate student roughly 10 hours per week to complete all required tasks.However, some number of students will find programming to be more challenging and may take up to 15 hours per week. Some will breeze through the material in 5.</p>
</div>
<div id="section-grading" class="section level4">
<h4><strong>Grading</strong></h4>
<p>Each Monday, lessons will be assigned from datacamp.com. Some of these lessons will be complete DataCamp courses, and others will be specific modules of courses. This will all be managed by assigning content to your (free) DataCamp account. The amount of content assigned will vary between one and two courses of content. DataCamp considers a course to be roughly 4 hours of lessons, which includes practice time. Realistically, the time you need will depend upon how intuitive you find <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to be. For students already familiar with other programming languages and those with previous <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> experience, “8 hours” of courses is realistically closer to 2 hours; for complete novices that also find the material difficult, 8 hours is a realistic estimate. It is strongly recommended that you stretch out DataCamp lessons across the assignment period, for example, allocating 1 hour each day. You will gain the most by treating this as a foreign language immersion course by using R every day, including for your own research. Remember that you can always go to the <strong>Slack Group</strong> for help.</p>
</div>
<div id="section-passing-rate" class="section level4">
<h4><strong>Passing Rate</strong></h4>
<p>The passing rate is 70%.</p>
</div>
</div>
<div id="section-installing-and-r-studio" class="section level3">
<h3>Installing <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> and R Studio</h3>
<div id="section-installing-r" class="section level4">
<h4><strong>Installing R</strong></h4>
<p>If you want to install <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to work on your own computer, you can download it freely from the <a href="https://cran.r-project.org/">Comprehensive R Archive Network (CRAN)</a>. Note that CRAN makes several versions of <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> available: versions for multiple operating systems and releases older than the current one. You want to read the CRAN instructions to assure you download the correct version. If you need further help, you can try the following resources:</p>
<ul>
<li><a href="https://github.com/genomicsclass/windows#installing-r">Installing <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> on Windows</a></li>
<li><a href="http://youtu.be/Icawuhf0Yqo">Installing <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> on Mac</a></li>
<li><a href="https://cran.r-project.org/bin/linux/ubuntu/README">Installing <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> on Ubuntu</a></li>
</ul>
</div>
<div id="section-installing-rstudio" class="section level4">
<h4><strong>Installing RStudio</strong></h4>
<p>RStudio is an integrated development environment (IDE). We highly recommend installing and using RStudio to edit and test your code. You can install RStudio through the <a href="https://www.rstudio.com/products/rstudio/download/">RStudio website</a>. Their <a href="https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf">cheatsheet</a> is a great resource. You need to install <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> first.</p>
</div>
</div>
<div id="section-pre-course-survey" class="section level3">
<h3>Pre-Course Survey</h3>
<p>Insert Survey Link here</p>
<p><em>If you cannot see the survey above, click this link to access it in a new window.</em></p>
</div>
</div>
<div id="section-introduction-to-inference" class="section level2">
<h2>Introduction to Inference</h2>
<div class="infobox">
<p><strong>Textbook link</strong> The contents are discussed within the <a href="https://rafalab.github.io/dsbook/inference.html">textbook section - 15 Statistical Inference</a>.</p>
</div>
<hr />
<p>In this module we will describe, in some detail, how poll aggregators such as FiveThirtyEight use data to predict election outcomes. To understand how they do this, we first need to learn the basics of <em>Statistical Inference</em>, the part of statistics that helps distinguish patterns arising from signal from those arising from chance. Statistical inference is a broad topic and here we go over the very basics using polls as a motivating example. To describe the concepts, we complement the mathematical formulas with Monte Carlo simulations and R code.</p>
<div id="section-polls" class="section level3">
<h3>Polls</h3>
<p>Opinion polling has been conducted since the 19th century. The general goal is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as <em>inference</em> and it is the main topic of this section.</p>
<p>Perhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know in which geographical locations to focus their “get out the vote” efforts.</p>
<p>Elections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results.</p>
<p>Although typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at such data.</p>
<p>Real <a href="http://www.realclearpolitics.com">Clear Politics</a> is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the <a href="http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html">2016 presidential election</a>:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Poll
</th>
<th style="text-align:left;">
Date
</th>
<th style="text-align:left;">
Sample
</th>
<th style="text-align:left;">
MoE
</th>
<th style="text-align:right;">
Clinton
</th>
<th style="text-align:right;">
Trump
</th>
<th style="text-align:left;">
Spread
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Final Results
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
48.2
</td>
<td style="text-align:right;">
46.1
</td>
<td style="text-align:left;">
Clinton +2.1
</td>
</tr>
<tr>
<td style="text-align:left;">
RCP Average
</td>
<td style="text-align:left;">
11/1 - 11/7
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
46.8
</td>
<td style="text-align:right;">
43.6
</td>
<td style="text-align:left;">
Clinton +3.2
</td>
</tr>
<tr>
<td style="text-align:left;">
Bloomberg
</td>
<td style="text-align:left;">
11/4 - 11/6
</td>
<td style="text-align:left;">
799 LV
</td>
<td style="text-align:left;">
3.5
</td>
<td style="text-align:right;">
46.0
</td>
<td style="text-align:right;">
43.0
</td>
<td style="text-align:left;">
Clinton +3
</td>
</tr>
<tr>
<td style="text-align:left;">
IBD
</td>
<td style="text-align:left;">
11/4 - 11/7
</td>
<td style="text-align:left;">
1107 LV
</td>
<td style="text-align:left;">
3.1
</td>
<td style="text-align:right;">
43.0
</td>
<td style="text-align:right;">
42.0
</td>
<td style="text-align:left;">
Clinton +1
</td>
</tr>
<tr>
<td style="text-align:left;">
Economist
</td>
<td style="text-align:left;">
11/4 - 11/7
</td>
<td style="text-align:left;">
3669 LV
</td>
<td style="text-align:left;">
–
</td>
<td style="text-align:right;">
49.0
</td>
<td style="text-align:right;">
45.0
</td>
<td style="text-align:left;">
Clinton +4
</td>
</tr>
<tr>
<td style="text-align:left;">
LA Times
</td>
<td style="text-align:left;">
11/1 - 11/7
</td>
<td style="text-align:left;">
2935 LV
</td>
<td style="text-align:left;">
4.5
</td>
<td style="text-align:right;">
44.0
</td>
<td style="text-align:right;">
47.0
</td>
<td style="text-align:left;">
Trump +3
</td>
</tr>
<tr>
<td style="text-align:left;">
ABC
</td>
<td style="text-align:left;">
11/3 - 11/6
</td>
<td style="text-align:left;">
2220 LV
</td>
<td style="text-align:left;">
2.5
</td>
<td style="text-align:right;">
49.0
</td>
<td style="text-align:right;">
46.0
</td>
<td style="text-align:left;">
Clinton +3
</td>
</tr>
<tr>
<td style="text-align:left;">
FOX News
</td>
<td style="text-align:left;">
11/3 - 11/6
</td>
<td style="text-align:left;">
1295 LV
</td>
<td style="text-align:left;">
2.5
</td>
<td style="text-align:right;">
48.0
</td>
<td style="text-align:right;">
44.0
</td>
<td style="text-align:left;">
Clinton +4
</td>
</tr>
<tr>
<td style="text-align:left;">
Monmouth
</td>
<td style="text-align:left;">
11/3 - 11/6
</td>
<td style="text-align:left;">
748 LV
</td>
<td style="text-align:left;">
3.6
</td>
<td style="text-align:right;">
50.0
</td>
<td style="text-align:right;">
44.0
</td>
<td style="text-align:left;">
Clinton +6
</td>
</tr>
<tr>
<td style="text-align:left;">
NBC News
</td>
<td style="text-align:left;">
11/3 - 11/5
</td>
<td style="text-align:left;">
1282 LV
</td>
<td style="text-align:left;">
2.7
</td>
<td style="text-align:right;">
48.0
</td>
<td style="text-align:right;">
43.0
</td>
<td style="text-align:left;">
Clinton +5
</td>
</tr>
<tr>
<td style="text-align:left;">
CBS News
</td>
<td style="text-align:left;">
11/2 - 11/6
</td>
<td style="text-align:left;">
1426 LV
</td>
<td style="text-align:left;">
3.0
</td>
<td style="text-align:right;">
47.0
</td>
<td style="text-align:right;">
43.0
</td>
<td style="text-align:left;">
Clinton +4
</td>
</tr>
<tr>
<td style="text-align:left;">
Reuters
</td>
<td style="text-align:left;">
11/2 - 11/6
</td>
<td style="text-align:left;">
2196 LV
</td>
<td style="text-align:left;">
2.3
</td>
<td style="text-align:right;">
44.0
</td>
<td style="text-align:right;">
39.0
</td>
<td style="text-align:left;">
Clinton +5
</td>
</tr>
</tbody>
</table>
<!-- (Source: [Real Clear Politics](https://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html)) -->
<p>Although in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC and we describe it in the textbook <a href="https://rafalab.github.io/dsbook/models.html#election-forecasting">(Section - 16.8 Case study: election forecasting)</a>.</p>
<p>Let’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different <em>spread</em>: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled <strong>MoE</strong> which stands for <em>margin of error</em>.</p>
<p>In this section, we will show how the probability concepts we learned in the previous section can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define <em>estimates</em> and <em>margins of errors</em>, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: <em>confidence intervals</em> and <em>p-values</em>. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modeling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election.</p>
<p>We start by connecting probability theory to the task of using polls to learn about a population.</p>
<div id="section-course-overview-2" class="section level4">
<h4><strong>Course overview</strong></h4>
<p>In this course, we will learn:</p>
<ul>
<li><p><em>statistical inference</em>, the process of deducing characteristics of a population using data from a random sample</p></li>
<li><p>the statistical concepts necessary to define <em>estimates</em> and <em>margins of errors</em></p></li>
<li><p>how to <em>forecast future results</em> and estimate the precision of our forecast</p></li>
<li><p>how to calculate and interpret <em>confidence intervals</em> and <em>p-values</em></p></li>
</ul>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Information gathered from a small random sample can be used to infer characteristics of the entire population.</p></li>
<li><p>Opinion polls are useful when asking everyone in the population is impossible.</p></li>
<li><p>A common use for opinion polls is determining voter preferences in political elections for the purposes of forecasting election results.</p></li>
<li><p>The spread of a poll is the estimated difference between support two candidates or options.</p></li>
</ul>
</div>
</div>
</div>
</div>
<div id="section-section-1-parameters-and-estimates" class="section level2">
<h2>Section 1: Parameters and Estimates</h2>
<p>Section 1 introduces you to parameters and estimates.</p>
<p>After completing Section 1, you will be able to:</p>
<ul>
<li><p>Understand how to use a sampling model to perform a poll.</p></li>
<li><p>Explain the terms <strong>population, parameter</strong>, and <strong>sample</strong> as they relate to statistical inference.</p></li>
<li><p>Use a sample to estimate the population proportion from the sample average.</p></li>
<li><p>Calculate the expected value and standard error of the sample average.</p></li>
</ul>
<p>There is 1 assignments for you to practice your coding skills.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your learning.</p>
<div id="section-sampling-model-parameters-and-estimates" class="section level3">
<h3>1. Sampling Model Parameters and Estimates</h3>
<p>To help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this urn (in this case, a pickle jar):</p>
<p><img src="images/urn.jpg" width="300" /></p>
<p>Before making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you $0.10 per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay $25 to collect your $25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.</p>
<p>The <strong>dslabs</strong> package includes a function that shows a random draw from this urn:</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)
take_poll(25)</code></pre>
<p><img src="datsci_04_files/figure-html/first-simulated-poll-1.png" width="624" /></p>
<p>Think about how you would construct your interval based on the data shown above.</p>
<p>We have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors. That is, that there are just two parties: Republican and Democratic.</p>
</div>
<div id="section-populations-samples-parameters-and-estimates" class="section level3">
<h3>Populations, samples, parameters, and estimates</h3>
<p>We want to predict the proportion of blue beads in the urn. Let’s call this quantity <span class="math inline">\(p\)</span>, which then tells us the proportion of red beads <span class="math inline">\(1-p\)</span>, and the spread <span class="math inline">\(p - (1-p)\)</span>, which simplifies to <span class="math inline">\(2p - 1\)</span>.</p>
<p>In statistical textbooks, the beads in the urn are called the <em>population</em>. The proportion of blue beads in the population <span class="math inline">\(p\)</span> is called a <em>parameter</em>. The 25 beads we see in the previous plot are called a <em>sample</em>. The task of statistical inference is to predict the parameter <span class="math inline">\(p\)</span> using the observed data in the sample.</p>
<p>Can we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that <span class="math inline">\(p\)</span> &gt; .9 or <span class="math inline">\(p\)</span> &lt; .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?</p>
<p>We want to construct an estimate of <span class="math inline">\(p\)</span> using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample <span class="math inline">\(0.48\)</span> must be at least related to the actual proportion <span class="math inline">\(p\)</span>. But do we simply predict <span class="math inline">\(p\)</span> to be 0.48? First, remember that the sample proportion is a random variable. If we run the command <code>take_poll(25)</code> four times, we get a different answer each time, since the sample proportion is a random variable.</p>
<p><img src="datsci_04_files/figure-html/four-simulated-polls-1.png" width="624" /></p>
<p>Note that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The task of statistical inference is to estimate an unknown population parameter using observed data from a sample.</p></li>
<li><p>In a sampling model, the collection of elements in the urn is called the <em>population.</em></p></li>
<li><p>A <em>parameter</em> is a number that summarizes data for an entire population.</p></li>
<li><p>A <em>sample</em> is observed data from a subset of the population.</p></li>
<li><p>An <em>estimate</em> is a summary of the observed data about a parameter that we believe is informative. It is a data-driven guess of the population parameter.</p></li>
<li><p>We want to predict the proportion of the blue beads in the urn, the parameter 𝑝. The proportion of red beads in the urn is 1 − 𝑝 and the spread is 2𝑝 − 1.</p></li>
<li><p>The sample proportion is a random variable. Sampling gives random results drawn from the population distribution.</p></li>
</ul>
</div>
</div>
<div id="section-the-sample-average" class="section level3">
<h3>The Sample Average</h3>
<p>Conducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an <em>estimate</em> of the parameter <span class="math inline">\(p\)</span>. Once we have this estimate, we can easily report an estimate for the spread <span class="math inline">\(2p-1\)</span>, but for simplicity we will illustrate the concepts for estimating <span class="math inline">\(p\)</span>. We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion <span class="math inline">\(p\)</span>.</p>
<p>We start by defining the random variable <span class="math inline">\(X\)</span> as: <span class="math inline">\(X=1\)</span> if we pick a blue bead at random and <span class="math inline">\(X=0\)</span> if it is red. This implies that the population is a list of 0s and 1s. If we sample <span class="math inline">\(N\)</span> beads, then the average of the draws <span class="math inline">\(X_1, \dots, X_N\)</span> is equivalent to the proportion of blue beads in our sample. This is because adding the <span class="math inline">\(X\)</span>s is equivalent to counting the blue beads and dividing this count by the total <span class="math inline">\(N\)</span> is equivalent to computing a proportion. We use the symbol <span class="math inline">\(\bar{X}\)</span> to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant <span class="math inline">\(1/N\)</span>:</p>
<p><span class="math display">\[\bar{X} = 1/N \times \sum_{i=1}^N X_i\]</span></p>
<p>For simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is <span class="math inline">\(N\)</span> times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be <span class="math inline">\(p\)</span>, the proportion of blue beads.</p>
<p>Here we encounter an important difference with what we did in the Probability section: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to <strong>estimate</strong> <span class="math inline">\(p\)</span>.</p>
</div>
<div id="section-parameters" class="section level3">
<h3>Parameters</h3>
<p>Just like we use variables to define unknowns in systems of equations, in statistical inference we define <em>parameters</em> to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters <span class="math inline">\(p\)</span> to represent this quantity. <span class="math inline">\(p\)</span> is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is <span class="math inline">\(p\)</span>, we are going to <em>estimate this parameter</em>.</p>
<p>The ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask, what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Many common data science tasks can be framed as estimating a parameter from a sample.</p></li>
<li><p>We illustrate statistical inference by walking through the process to estimate 𝑝. From the estimate of 𝑝, we can easily calculate an estimate of the spread, 2𝑝−1.</p></li>
<li><p>Consider the random variable 𝑋 that is 1 if a blue bead is chosen and 0 if a red bead is chosen. The proportion of blue beads in 𝑁 draws is the average of the draws 𝑋1,…,𝑋𝑁.</p></li>
</ul>
<p><span class="math display">\[\bar{X} = \frac{X_1 + X_2 + \dots + X_N}{N}\]</span></p>
</div>
</div>
<div id="section-polling-versus-forecasting" class="section level3">
<h3>Polling versus Forecasting</h3>
<p>Before we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the <span class="math inline">\(p\)</span> for that moment and not for election day. The <span class="math inline">\(p\)</span> for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section.</p>
</div>
<div id="section-properties-of-our-estimate" class="section level3">
<h3>Properties of Our Estimate</h3>
<p>To understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion <span class="math inline">\(\bar{X}\)</span>. Remember that <span class="math inline">\(\bar{X}\)</span> is the sum of independent draws so the rules we covered in the probability section apply.</p>
<p>Using what we have learned, the expected value of the sum <span class="math inline">\(N\bar{X}\)</span> is <span class="math inline">\(N \times\)</span> the average of the urn, <span class="math inline">\(p\)</span>. So dividing by the non-random constant <span class="math inline">\(N\)</span> gives us that the expected value of the average <span class="math inline">\(\bar{X}\)</span> is <span class="math inline">\(p\)</span>. We can write it using our mathematical notation:</p>
<p><span class="math display">\[
\mbox{E}(\bar{X}) = p
\]</span></p>
<p>We can also use what we learned to figure out the standard error: the standard error of the sum is <span class="math inline">\(\sqrt{N} \times\)</span> the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is <span class="math inline">\((1-0) \sqrt{p (1-p)}\)</span> = <span class="math inline">\(\sqrt{p (1-p)}\)</span>. Because we are dividing the sum by <span class="math inline">\(N\)</span>, we arrive at the following formula for the standard error of the average:</p>
<p><span class="math display">\[
\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}
\]</span></p>
<p>This result reveals the power of polls. The expected value of the sample proportion <span class="math inline">\(\bar{X}\)</span> is the parameter of interest <span class="math inline">\(p\)</span> and we can make the standard error as small as we want by increasing <span class="math inline">\(N\)</span>. The law of large numbers tells us that with a large enough poll, our estimate converges to <span class="math inline">\(p\)</span>.</p>
<p>If we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?</p>
<p>One problem is that we do not know <span class="math inline">\(p\)</span>, so we can’t compute the standard error. However, for illustrative purposes, let’s assume that <span class="math inline">\(p=0.51\)</span> and make a plot of the standard error versus the sample size <span class="math inline">\(N\)</span>:</p>
<p><img src="datsci_04_files/figure-html/standard-error-versus-sample-size-1.png" width="624" /></p>
<p>From the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and <span class="math inline">\(p=0.51\)</span>, the standard error is:</p>
<pre class="r"><code>sqrt(p*(1-p))/sqrt(1000)</code></pre>
<pre><code>## [1] 0.01580823</code></pre>
<p>or 1.5 percentage points. So even with large polls, for close elections, <span class="math inline">\(\bar{X}\)</span> can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the <span class="math inline">\(p\)</span> and we do that in the textbook <a href="https://rafalab.github.io/dsbook/inference.html#clt">(Section - 15.4 Central Limit Theorem in practice)</a>.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>When interpreting values of <span class="math inline">\(\bar{X}\)</span>, it is important to remember that <span class="math inline">\(\bar{X}\)</span> is a random variable with an expected value and standard error that represents the sample proportion of positive events.</p></li>
<li><p>The expected value of <span class="math inline">\(\bar{X}\)</span> is the parameter of interest 𝑝. This follows from the fact that <span class="math inline">\(\bar{X}\)</span> is the sum of independent draws of a random variable times a constant 1/𝑁.</p></li>
</ul>
<p><span class="math display">\[E(\bar{X}) = p\]</span></p>
<ul>
<li>As the number of draws 𝑁 increases, the standard error of our estimate <span class="math inline">\(\bar{X}\)</span> decreases. The standard error of the average of <span class="math inline">\(\bar{X}\)</span> over 𝑁 draws is:</li>
</ul>
<p><span class="math display">\[
SE(\bar{X}) =  \sqrt{p(1-p)/N}
\]</span></p>
<ul>
<li><p>In theory, we can get more accurate estimates of 𝑝 by increasing 𝑁. In practice, there are limits on the size of 𝑁 due to costs, as well as other factors we discuss later.</p></li>
<li><p>We can also use other random variable equations to determine the expected value of the sum of draws E(𝑆) and standard error of the sum of draws SE(𝑆).</p></li>
</ul>
<p><span class="math display">\[E(S) = Np\]</span></p>
<p><span class="math display">\[
SE(S) =  \sqrt{Np(1-p)}
\]</span></p>
</div>
</div>
<div id="section-assessment-parameters-and-estimates" class="section level3">
<h3>1.1 Assessment: Parameters and Estimates</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-2-the-central-limit-theorem-in-practice" class="section level2">
<h2>Section 2: The Central Limit Theorem in Practice</h2>
<p>In Section 2, you will look at the Central Limit Theorem in practice.</p>
<p>After completing Section 2, you will be able to:</p>
<ul>
<li><p>Use the Central Limit Theorem to calculate the probability that a sample estimate <span class="math inline">\(\bar{X}\)</span> is close to the population proportion 𝑝.</p></li>
<li><p>Run a Monte Carlo simulation to corroborate theoretical results built using probability theory.</p></li>
<li><p>Estimate the spread based on estimates of <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\hat{\mbox{SE}}(\bar{X})\)</span>.</p></li>
<li><p>Understand why bias can mean that larger sample sizes aren’t necessarily better.</p></li>
</ul>
<p>There is 1 assignment for you to practice your coding skills.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your learning.</p>
<div id="section-the-central-limit-theorem-in-practice" class="section level3">
<h3>2. The Central Limit Theorem in Practice</h3>
<p>The CLT tells us that the distribution function for a sum of draws is approximately normal. We also learned that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of <span class="math inline">\(\bar{X}\)</span> is approximately normal.</p>
<p>In summary, we have that <span class="math inline">\(\bar{X}\)</span> has an approximately normal distribution with expected value <span class="math inline">\(p\)</span> and standard error <span class="math inline">\(\sqrt{p(1-p)/N}\)</span>.</p>
<p>Now how does this help us? Suppose we want to know what is the probability that we are within 1% from <span class="math inline">\(p\)</span>. We are basically asking what is</p>
<p><span class="math display">\[
\mbox{Pr}(| \bar{X} - p| \leq .01)
\]</span> which is the same as:</p>
<p><span class="math display">\[
\mbox{Pr}(\bar{X}\leq p + .01) - \mbox{Pr}(\bar{X} \leq p - .01)
\]</span></p>
<p>Can we answer this question? We can use the mathematical trick we learned in the previous section. Subtract the expected value and divide by the standard error to get a standard normal random variable, call it <span class="math inline">\(Z\)</span>, on the left. Since <span class="math inline">\(p\)</span> is the expected value and <span class="math inline">\(\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}\)</span> is the standard error we get:</p>
<p><span class="math display">\[
\mbox{Pr}\left(Z \leq \frac{ \,.01} {\mbox{SE}(\bar{X})} \right) -
\mbox{Pr}\left(Z \leq - \frac{ \,.01} {\mbox{SE}(\bar{X})} \right) 
\]</span></p>
<p>One problem we have is that since we don’t know <span class="math inline">\(p\)</span>, we don’t know <span class="math inline">\(\mbox{SE}(\bar{X})\)</span>. But it turns out that the CLT still works if we estimate the standard error by using <span class="math inline">\(\bar{X}\)</span> in place of <span class="math inline">\(p\)</span>. We say that we <em>plug-in</em> the estimate. Our estimate of the standard error is therefore:</p>
<p><span class="math display">\[
\hat{\mbox{SE}}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N}
\]</span> In statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and <span class="math inline">\(N\)</span>.</p>
<p>Now we continue with our calculation, but dividing by <span class="math inline">\(\hat{\mbox{SE}}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N})\)</span> instead. In our first sample we had 12 blue and 13 red so <span class="math inline">\(\bar{X} = 0.48\)</span> and our estimate of standard error is:</p>
<pre class="r"><code>x_hat &lt;- 0.48
se &lt;- sqrt(x_hat*(1-x_hat)/25)
se</code></pre>
<pre><code>## [1] 0.09991997</code></pre>
<p>And now we can answer the question of the probability of being close to <span class="math inline">\(p\)</span>. The answer is:</p>
<pre class="r"><code>pnorm(0.01/se) - pnorm(-0.01/se)</code></pre>
<pre><code>## [1] 0.07971926</code></pre>
<p>Therefore, there is a small chance that we will be close.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Because <span class="math inline">\(\bar{X}\)</span> is the sum of random draws divided by a constant, the distribution of <span class="math inline">\(\bar{X}\)</span> is approximately normal.</p></li>
<li><p>We can convert <span class="math inline">\(\bar{X}\)</span> to a standard normal random variable 𝑍: <span class="math display">\[
𝑍 = \frac{\bar{X}−E(\bar{X})}{SE(\bar{X})}
\]</span></p></li>
<li><p>The probability that <span class="math inline">\(\bar{X}\)</span> is within .01 of the actual value of 𝑝 is: <span class="math display">\[
\mbox{Pr}\left(Z \leq \, .01/\,\sqrt{p(1-p)}  / N \right) -
\mbox{Pr}\left(Z \leq \, -.01/\,\sqrt{p(1-p)}  / N \right) 
\]</span></p></li>
<li><p>The Central Limit Theorem (CLT) still works if <span class="math inline">\(\bar{X}\)</span> is used in place of 𝑝. This is called a plug-in estimate. Hats over values denote estimates. Therefore:</p></li>
</ul>
<p><span class="math display">\[
\hat{\mbox{SE}}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N}
\]</span></p>
<ul>
<li>Using the CLT, the probability that <span class="math inline">\(\bar{X}\)</span> is within .01 of the actual value of 𝑝 is:</li>
</ul>
<p><span class="math display">\[
\mbox{Pr}\left(Z \leq \, .01/\,\sqrt{\bar{X}(1-\bar{X})}  / N \right) -
\mbox{Pr}\left(Z \leq \, -.01/\,\sqrt{\bar{X}(1-\bar{X})}  / N \right) 
\]</span></p>
</div>
</div>
<div id="section-margin-of-error" class="section level3">
<h3>2. Margin of Error</h3>
<p>A poll of only <span class="math inline">\(N=25\)</span> people is not really very useful, at least not for a close election.</p>
<p>Earlier we mentioned the <em>margin of error</em>. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is:</p>
<pre class="r"><code>1.96*se</code></pre>
<pre><code>## [1] 0.1958431</code></pre>
<p>Why do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from <span class="math inline">\(p\)</span>, we get:</p>
<p><span class="math display">\[
\mbox{Pr}\left(Z \leq \, 1.96\,\mbox{SE}(\bar{X})  / \mbox{SE}(\bar{X}) \right) -
\mbox{Pr}\left(Z \leq - 1.96\, \mbox{SE}(\bar{X}) / \mbox{SE}(\bar{X}) \right) 
\]</span> which is:</p>
<p><span class="math display">\[
\mbox{Pr}\left(Z \leq 1.96 \right) -
\mbox{Pr}\left(Z \leq - 1.96\right) 
\]</span></p>
<p>which we know is about 95%:</p>
<pre class="r"><code>pnorm(1.96)-pnorm(-1.96)</code></pre>
<pre><code>## [1] 0.9500042</code></pre>
<p>Hence, there is a 95% probability that <span class="math inline">\(\bar{X}\)</span> will be within <span class="math inline">\(1.96\times \hat{SE}(\bar{X})\)</span>, in our case within about 0.2, of <span class="math inline">\(p\)</span>. Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. We often round 1.96 up to 2 for simplicity of presentation.</p>
<p>In summary, the CLT tells us that our poll based on a sample size of <span class="math inline">\(25\)</span> is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.</p>
<p>From the table above, we see that typical sample sizes range from 700 to 3500. To see how this gives us a much more practical result, notice that if we had obtained a <span class="math inline">\(\bar{X}\)</span> = 0.48 with a sample size of 2,000, our standard error <span class="math inline">\(\hat{\mbox{SE}}(\bar{X})\)</span> would have been 0.0111714. So our result is an estimate of <code>48</code>% with a margin of error of 2%. In this case, the result is much more informative and would make us think that there are more red balls than blue. Keep in mind, however, that this is hypothetical. We did not take a poll of 2,000 since we don’t want to ruin the competition.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The <em>margin of error</em> is defined as 2 times the standard error of the estimate <span class="math inline">\(\bar{X}\)</span>.</p></li>
<li><p>There is about a 95% chance that <span class="math inline">\(\bar{X}\)</span> will be within two standard errors of the actual parameter 𝑝.</p></li>
</ul>
</div>
</div>
<div id="section-a-monte-carlo-simulation-for-the-clt" class="section level3">
<h3>2. A Monte Carlo Simulation for the CLT</h3>
<p>Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:</p>
<pre class="r"><code>B &lt;- 10000
N &lt;- 1000
x_hat &lt;- replicate(B, {
  x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})</code></pre>
<p>The problem is, of course, we don’t know <code>p</code>. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function <code>take_poll(n=1000)</code> instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.</p>
<p>One thing we therefore do to corroborate theoretical results is to pick one or several values of <code>p</code> and run the simulations. Let’s set <code>p=0.45</code>. We can then simulate a poll:</p>
<pre class="r"><code>p &lt;- 0.45
N &lt;- 1000

x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat &lt;- mean(x)</code></pre>
<p>In this particular sample, our estimate is <code>x_hat</code>. We can use that code to do a Monte Carlo simulation:</p>
<pre class="r"><code>B &lt;- 10000
x_hat &lt;- replicate(B, {
  x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})</code></pre>
<p>To review, the theory tells us that <span class="math inline">\(\bar{X}\)</span> is approximately normally distributed, has expected value <span class="math inline">\(p=\)</span> 0.45 and standard error <span class="math inline">\(\sqrt{p(1-p)/N}\)</span> = 0.0157321. The simulation confirms this:</p>
<pre class="r"><code>mean(x_hat)</code></pre>
<pre><code>## [1] 0.4500761</code></pre>
<pre class="r"><code>sd(x_hat)</code></pre>
<pre><code>## [1] 0.01579523</code></pre>
<p>A histogram and qq-plot confirm that the normal approximation is accurate as well:</p>
<p><img src="datsci_04_files/figure-html/normal-approximation-for-polls-1.png" width="100%" /></p>
<p>Of course, in real life we would never be able to run such an experiment because we don’t know <span class="math inline">\(p\)</span>. But we could run it for various values of <span class="math inline">\(p\)</span> and <span class="math inline">\(N\)</span> and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing <code>p</code> and <code>N</code>.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>We can run Monte Carlo simulations to compare with theoretical results assuming a value of 𝑝.</p></li>
<li><p>In practice, 𝑝 is unknown. We can corroborate theoretical results by running Monte Carlo simulations with one or several values of 𝑝.</p></li>
<li><p>One practical choice for 𝑝 when modeling is <span class="math inline">\(\bar{X}\)</span>, the observed value of <span class="math inline">\(\bar{X}\)</span> in a sample.</p></li>
</ul>
</div>
</div>
<div id="section-the-spread" class="section level3">
<h3>2. The Spread</h3>
<p>The competition is to predict the spread, not the proportion <span class="math inline">\(p\)</span>. However, because we are assuming there are only two parties, we know that the spread is <span class="math inline">\(p - (1-p) = 2p - 1\)</span>. As a result, everything we have done can easily be adapted to an estimate of <span class="math inline">\(2p - 1\)</span>. Once we have our estimate <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\hat{\mbox{SE}}(\bar{X})\)</span>, we estimate the spread with <span class="math inline">\(2\bar{X} - 1\)</span> and, since we are multiplying by 2, the standard error is <span class="math inline">\(2\hat{\mbox{SE}}(\bar{X})\)</span>. Note that subtracting 1 does not add any variability so it does not affect the standard error.</p>
<p>For our 25 item sample above, our estimate <span class="math inline">\(p\)</span> is <code>.48</code> with margin of error <code>.20</code> and our estimate of the spread is <code>0.04</code> with margin of error <code>.40</code>. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for <span class="math inline">\(p\)</span>, we have it for the spread <span class="math inline">\(2p-1\)</span>.</p>
</div>
<div id="section-bias-why-not-run-a-very-large-poll" class="section level3">
<h3>2. Bias: Why Not Run a Very Large Poll?</h3>
<p>For realistic values of <span class="math inline">\(p\)</span>, say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%:</p>
<p><img src="datsci_04_files/figure-html/standard-error-versus-p-1.png" width="624" /></p>
<p>One reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is <span class="math inline">\(p\)</span>. We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this in a later section.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>An extremely large poll would theoretically be able to predict election results almost perfectly.</p></li>
<li><p>These sample sizes are not practical. In addition to cost concerns, polling doesn’t reach everyone in the population (eventual voters) with equal probability, and it also may include data from outside our population (people who will not end up voting).</p></li>
<li><p>These systematic errors in polling are called bias. We will learn more about bias in the future.</p></li>
</ul>
</div>
</div>
<div id="section-assessment-introduction-to-inference" class="section level3">
<h3>2.1 Assessment: Introduction to Inference</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-3-confidence-intervals-and-p-values" class="section level2">
<h2>Section 3: Confidence Intervals and p-Values</h2>
<p>In Section 3, you will look at confidence intervals and p-values.</p>
<p>After completing Section 3, you will be able to:</p>
<ul>
<li><p>Calculate confidence intervals of difference sizes around an estimate.</p></li>
<li><p>Understand that a confidence interval is a random interval with the given probability of falling on top of the parameter.</p></li>
<li><p>Explain the concept of “power” as it relates to inference.</p></li>
<li><p>Understand the relationship between p-values and confidence intervals and explain why reporting confidence intervals is often preferable.</p></li>
</ul>
<p>There is 1 assignment for you to practice your coding skills.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your learning.</p>
<div id="section-confidence-intervals" class="section level3">
<h3>3. Confidence Intervals</h3>
<p>Confidence intervals are a very useful concept widely employed by data analysts. A version of these that are commonly seen come from the <code>ggplot</code> geometry <code>geom_smooth</code>. Here is an example using a temperature dataset available in R:</p>
<p><img src="datsci_04_files/figure-html/first-confidence-intervals-example-1.png" width="624" /></p>
<p>In the Machine Learning part we will learn how the curve is formed, but for now consider the shaded area around the curve. This is created using the concept of confidence intervals.</p>
<p>In our earlier competition, you were asked to give an interval. If the interval you submitted includes the <span class="math inline">\(p\)</span>, you get half the money you spent on your “poll” back and pass to the next stage of the competition. One way to pass to the second round is to report a very large interval. For example, the interval <span class="math inline">\([0,1]\)</span> is guaranteed to include <span class="math inline">\(p\)</span>. However, with an interval this big, we have no chance of winning the competition. Similarly, if you are an election forecaster and predict the spread will be between -100% and 100%, you will be ridiculed for stating the obvious. Even a smaller interval, such as saying the spread will be between -10 and 10%, will not be considered serious.</p>
<p>On the other hand, the smaller the interval we report, the smaller our chances are of winning the prize. Likewise, a bold pollster that reports very small intervals and misses the mark most of the time will not be considered a good pollster. We want to be somewhere in between.</p>
<p>We can use the statistical theory we have learned to compute the probability of any given interval including <span class="math inline">\(p\)</span>. If we are asked to create an interval with, say, a 95% chance of including <span class="math inline">\(p\)</span>, we can do that as well. These are called 95% confidence intervals.</p>
<p>When a pollster reports an estimate and a margin of error, they are, in a way, reporting a 95% confidence interval. Let’s show how this works mathematically.</p>
<p>We want to know the probability that the interval <span class="math inline">\([\bar{X} - 2\hat{\mbox{SE}}(\bar{X}), \bar{X} + 2\hat{\mbox{SE}}(\bar{X})]\)</span> contains the true proportion <span class="math inline">\(p\)</span>. First, consider that the start and end of these intervals are random variables: every time we take a sample, they change. To illustrate this, run the Monte Carlo simulation above twice. We use the same parameters as above:</p>
<pre class="r"><code>p &lt;- 0.45
N &lt;- 1000</code></pre>
<p>And notice that the interval here:</p>
<pre class="r"><code>x &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat &lt;- mean(x)
se_hat &lt;- sqrt(x_hat * (1 - x_hat) / N)
c(x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)</code></pre>
<pre><code>## [1] 0.4181713 0.4798287</code></pre>
<p>is different from this one:</p>
<pre class="r"><code>x &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
x_hat &lt;- mean(x)
se_hat &lt;- sqrt(x_hat * (1 - x_hat) / N)
c(x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)</code></pre>
<pre><code>## [1] 0.4211528 0.4828472</code></pre>
<p>Keep sampling and creating intervals and you will see the random variation.</p>
<p>To determine the probability that the interval includes <span class="math inline">\(p\)</span>, we need to compute this: <span class="math display">\[
\mbox{Pr}\left(\bar{X} - 1.96\hat{\mbox{SE}}(\bar{X}) \leq p \leq \bar{X} + 1.96\hat{\mbox{SE}}(\bar{X})\right)
\]</span></p>
<p>By subtracting and dividing the same quantities in all parts of the equation, we get that the above is equivalent to:</p>
<p><span class="math display">\[
\mbox{Pr}\left(-1.96 \leq \frac{\bar{X}- p}{\hat{\mbox{SE}}(\bar{X})} \leq  1.96\right)
\]</span></p>
<p>The term in the middle is an approximately normal random variable with expected value 0 and standard error 1, which we have been denoting with <span class="math inline">\(Z\)</span>, so we have:</p>
<p><span class="math display">\[
\mbox{Pr}\left(-1.96 \leq Z \leq  1.96\right)
\]</span></p>
<p>which we can quickly compute using :</p>
<pre class="r"><code>pnorm(1.96) - pnorm(-1.96)</code></pre>
<pre><code>## [1] 0.9500042</code></pre>
<p>proving that we have a 95% probability.</p>
<p>If we want to have a larger probability, say 99%, we need to multiply by whatever <code>z</code> satisfies the following:</p>
<p><span class="math display">\[
\mbox{Pr}\left(-z \leq Z \leq  z\right) = 0.99
\]</span></p>
<p>Using:</p>
<pre class="r"><code>z &lt;- qnorm(0.995)
z</code></pre>
<pre><code>## [1] 2.575829</code></pre>
<p>will achieve this because by definition <code>pnorm(qnorm(0.995))</code> is 0.995 and by symmetry <code>pnorm(1-qnorm(0.995))</code> is 1 - 0.995. As a consequence, we have that:</p>
<pre class="r"><code>pnorm(z) - pnorm(-z)</code></pre>
<pre><code>## [1] 0.99</code></pre>
<p>is <code>0.995 - 0.005 = 0.99</code>. We can use this approach for any proportion <span class="math inline">\(p\)</span>: we set <code>z = qnorm(1 - (1 - p)/2)</code> because <span class="math inline">\(1 - (1 - p)/2 + (1 - p)/2 = p\)</span>.</p>
<p>So, for example, for <span class="math inline">\(p=0.95\)</span>, <span class="math inline">\(1 - (1-p)/2 = 0.975\)</span> and we get the 1.96 we have been using:</p>
<pre class="r"><code>qnorm(0.975)</code></pre>
<pre><code>## [1] 1.959964</code></pre>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>We can use statistical theory to compute the probability that a given interval contains the true parameter 𝑝.</p></li>
<li><p>95% confidence intervals are intervals constructed to have a 95% chance of including 𝑝. The margin of error is approximately a 95% confidence interval.</p></li>
<li><p>The start and end of these confidence intervals are random variables.</p></li>
<li><p>To calculate any size confidence interval, we need to calculate the value 𝑧 for which <span class="math inline">\(\mbox{Pr}\left(-z \leq Z \leq z\right)\)</span> equals the desired confidence. For example, a 99% confidence interval requires calculating 𝑧 for <span class="math display">\[\mbox{Pr}\left(-z \leq Z \leq  z\right) = 0.99\]</span>.</p></li>
<li><p>For a confidence interval of size 𝑞, we solve for <span class="math inline">\(𝑧= 1−\frac{1−𝑞}{2}\)</span>.</p></li>
<li><p>To determine a 95% confidence interval, use <code>z &lt;- qnorm(0.975)</code>. This value is slightly smaller than 2 times the standard error.</p></li>
</ul>
</div>
</div>
<div id="section-a-monte-carlo-simulation-for-confidence-intervals" class="section level3">
<h3>3. A Monte Carlo Simulation for Confidence Intervals</h3>
<p>We can run a Monte Carlo simulation to confirm that, in fact, a 95% confidence interval includes <span class="math inline">\(p\)</span> 95% of the time.</p>
<pre class="r"><code>N &lt;- 1000
B &lt;- 10000
inside &lt;- replicate(B, {
  x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  x_hat &lt;- mean(x)
  se_hat &lt;- sqrt(x_hat * (1 - x_hat) / N)
  between(p, x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)
})
mean(inside)</code></pre>
<pre><code>## [1] 0.9482</code></pre>
<p>The following plot shows the first 100 confidence intervals. In this case, we created the simulation so the black line denotes the parameter we are trying to estimate:</p>
<p><img src="datsci_04_files/figure-html/confidence-interval-coverage-1.png" width="624" /></p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>We can run a Monte Carlo simulation to confirm that a 95% confidence interval contains the true value of 𝑝 95% of the time.</p></li>
<li><p>A plot of confidence intervals from this simulation demonstrates that most intervals include 𝑝, but roughly 5% of intervals miss the true value of 𝑝.</p></li>
</ul>
</div>
</div>
<div id="section-the-correct-language" class="section level3">
<h3>3. The Correct Language</h3>
<p>When using the theory we described above, it is important to remember that it is the intervals that are random, not <span class="math inline">\(p\)</span>. In the plot above, we can see the random intervals moving around and <span class="math inline">\(p\)</span>, represented with the vertical line, staying in the same place. The proportion of blue in the urn <span class="math inline">\(p\)</span> is not. So the 95% relates to the probability that this random interval falls on top of <span class="math inline">\(p\)</span>. Saying the <span class="math inline">\(p\)</span> has a 95% chance of being between this and that is technically an incorrect statement because <span class="math inline">\(p\)</span> is not random.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The 95% confidence intervals are random, but 𝑝 is not random.</p></li>
<li><p>95% refers to the probability that the random interval falls on top of 𝑝.</p></li>
<li><p>It is technically incorrect to state that 𝑝 has a 95% chance of being in between two values because that implies 𝑝 is random.</p></li>
</ul>
</div>
</div>
<div id="section-power" class="section level3">
<h3>3. Power</h3>
<p>Pollsters are not successful at providing correct confidence intervals, but rather at predicting who will win. When we took a 25 bead sample size, the confidence interval for the spread:</p>
<pre class="r"><code>N &lt;- 25
x_hat &lt;- 0.48
(2 * x_hat - 1) + c(-1.96, 1.96) * 2 * sqrt(x_hat * (1 - x_hat) / N)</code></pre>
<pre><code>## [1] -0.4316863  0.3516863</code></pre>
<p>includes 0. If this were a poll and we were forced to make a declaration, we would have to say it was a “toss-up”.</p>
<p>A problem with our poll results is that given the sample size and the value of <span class="math inline">\(p\)</span>, we would have to sacrifice on the probability of an incorrect call to create an interval that does not include 0.</p>
<p>This does not mean that the election is close. It only means that we have a small sample size. In statistical textbooks this is called lack of <em>power</em>. In the context of polls, <em>power</em> is the probability of detecting spreads different from 0.</p>
<p>By increasing our sample size, we lower our standard error and therefore have a much better chance of detecting the direction of the spread.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>If we are trying to predict the result of an election, then a confidence interval that includes a spread of 0 (a tie) is not helpful.</p></li>
<li><p>A confidence interval that includes a spread of 0 does not imply a close election, it means the sample size is too small.</p></li>
<li><p>Power is the probability of detecting an effect when there is a true effect to find. Power increases as sample size increases, because larger sample size means smaller standard error.</p></li>
</ul>
</div>
</div>
<div id="section-p-values" class="section level3">
<h3>p-Values</h3>
<p>p-values are ubiquitous in the scientific literature. They are related to confidence intervals so we introduce the concept here.</p>
<p>Let’s consider the blue and red beads. Suppose that rather than wanting an estimate of the spread or the proportion of blue, I am interested only in the question: are there more blue beads or red beads? I want to know if the spread <span class="math inline">\(2p-1 &gt; 0\)</span>.</p>
<p>Say we take a random sample of <span class="math inline">\(N=100\)</span> and we observe <span class="math inline">\(52\)</span> blue beads, which gives us <span class="math inline">\(2\bar{X}-1=0.04\)</span>. This seems to be pointing to the existence of more blue than red beads since 0.04 is larger than 0. However, as data scientists we need to be skeptical. We know there is chance involved in this process and we could get a 52 even when the actual spread is 0. We call the assumption that the spread is <span class="math inline">\(2p-1=0\)</span> a <em>null hypothesis</em>. The null hypothesis is the skeptic’s hypothesis. We have observed a random variable <span class="math inline">\(2*\bar{X}-1 = 0.04\)</span> and the p-value is the answer to the question: how likely is it to see a value this large, when the null hypothesis is true? So we write:</p>
<p><span class="math display">\[\mbox{Pr}(\mid \bar{X} - 0.5 \mid &gt; 0.02 ) \]</span></p>
<p>assuming the <span class="math inline">\(2p-1=0\)</span> or <span class="math inline">\(p=0.5\)</span>. Under the null hypothesis we know that:</p>
<p><span class="math display">\[
\sqrt{N}\frac{\bar{X} - 0.5}{\sqrt{0.5(1-0.5)}}
\]</span></p>
<p>is standard normal. We therefore can compute the probability above, which is the p-value.</p>
<p><span class="math display">\[\mbox{Pr}\left(\sqrt{N}\frac{\mid \bar{X} - 0.5\mid}{\sqrt{0.5(1-0.5)}} &gt; \sqrt{N} \frac{0.02}{ \sqrt{0.5(1-0.5)}}\right)\]</span></p>
<pre class="r"><code>N &lt;- 100
z &lt;- sqrt(N)*0.02/0.5
1 - (pnorm(z) - pnorm(-z))</code></pre>
<pre><code>## [1] 0.6891565</code></pre>
<p>In this case, there is actually a large chance of seeing 52 or larger under the null hypothesis.</p>
<p>Keep in mind that there is a close connection between p-values and confidence intervals. If a 95% confidence interval of the spread does not include 0, we know that the p-value must be smaller than 0.05.</p>
<p>To learn more about p-values, you can consult any statistics textbook. However, in general, we prefer reporting confidence intervals over p-values since it gives us an idea of the size of the estimate. If we just report the p-value we provide no information about the significance of the finding in the context of the problem.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The null hypothesis is the hypothesis that there is no effect. In this case, the null hypothesis is that the spread is 0, or 𝑝 = 0.5.</p></li>
<li><p>The p-value is the probability of detecting an effect of a certain size or larger when the null hypothesis is true.</p></li>
<li><p>We can convert the probability of seeing an observed value under the null hypothesis into a standard normal random variable. We compute the value of 𝑧 that corresponds to the observed result, and then use that 𝑧 to compute the p-value.</p></li>
<li><p>If a 95% confidence interval does not include our observed value, then the p-value must be smaller than 0.05.</p></li>
<li><p>It is preferable to report confidence intervals instead of p-values, as confidence intervals give information about the size of the estimate and p-values do not.</p></li>
</ul>
</div>
</div>
<div id="section-another-explanation-of-p-values" class="section level3">
<h3>Another Explanation of p-Values</h3>
<p>The p-value is the probability of observing a value as extreme or more extreme than the result given that the null hypothesis is true.</p>
<p>In the context of the normal distribution, this refers to the probability of observing a Z-score whose absolute value is as high or higher than the Z-score of interest.</p>
<p>Suppose we want to find the p-value of an observation 2 standard deviations larger than the mean. This means we are looking for anything with ∣𝑧∣ ≥ 2. ∣z∣ ≥ 2</p>
<p>Graphically, the p-value gives the probability of an observation that’s at least as far away from the mean or further. This plot shows a standard normal distribution (centered at z = 0 with a standard deviation of 1). The shaded tails are the region of the graph that are 2 standard deviations or more away from the mean.</p>
<center>
<figure>
<img src="images/pic10.png" alt="R Studio" style="width:700px">
</figure>
</center>
<p>The right tail can be found with 1-pnorm(2). We want to have both tails, though, because we want to find the probability of any observation as far away from the mean or farther, in either direction. (This is what’s meant by a two-tailed p-value.) Because the distribution is symmetrical, the right and left tails are the same size and we know that our desired value is just 2*(1-pnorm(2)).</p>
<p>Recall that, by default, pnorm() gives the CDF for a normal distribution with a mean of 𝜇 = 0 and standard deviation of 𝜎 = 1. To find p-values for a given z-score z in a normal distribution with mean mu and standard deviation sigma, use 2*(1-pnorm(z, mu, sigma)) instead.</p>
</div>
<div id="section-assessment-3.1-confidence-intervals-and-p-values" class="section level3">
<h3>Assessment 3.1: Confidence Intervals and p-Values</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-4-statistical-models" class="section level2">
<h2>Section 4: Statistical Models</h2>
<p>In Section 4, you will look at statistical models in the context of election polling and forecasting.</p>
<p>After completing Section 4, you will be able to:</p>
<ul>
<li><p>Understand how aggregating data from different sources, as poll aggregators do for poll data, can improve the precision of a prediction.</p></li>
<li><p>Understand how to fit a multilevel model to the data to forecast, for example, election results.</p></li>
<li><p>Explain why a simple aggregation of data is insufficient to combine results because of factors such as pollster bias.</p></li>
<li><p>Use a data-driven model to account for additional types of sampling variability such as pollster-to-pollster variability.</p></li>
</ul>
<p>There is 1 assignment for you to practice your coding skills.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your learning.</p>
<div id="section-poll-aggregators" class="section level3">
<h3>Poll Aggregators</h3>
<div class="infobox">
<p><strong>Textbook link</strong> The contents are discussed within the <a href="https://rafalab.github.io/dsbook/models.html">textbook section -16. Statistical models</a> and the <a href="https://rafalab.github.io/dsbook/models.html#poll-aggregators">textbook section - 16.1 Poll aggregators</a>.</p>
</div>
<hr />
<blockquote>
<blockquote>
<p>“All models are wrong, but some are useful.” –George E. P. Box</p>
</blockquote>
</blockquote>
<p>The day before the 2008 presidential election, Nate Silver’s FiveThirtyEight stated that “Barack Obama appears poised for a decisive electoral victory”. They went further and predicted that Obama would win the election with 349 electoral votes to 189, and the popular vote by a margin of 6.1%. FiveThirtyEight also attached a probabilistic statement to their prediction claiming that Obama had a 91% chance of winning the election. The predictions were quite accurate since, in the final results, Obama won the electoral college 365 to 173 and the popular vote by a 7.2% difference. Their performance in the 2008 election brought FiveThirtyEight to the attention of political pundits and TV personalities. Four years later, the week before the 2012 presidential election, FiveThirtyEight’s Nate Silver was giving Obama a 90% chance of winning despite many of the experts thinking the final results would be closer. <a href="https://www.youtube.com/watch?v=TbKkjm-gheY">Political commentator Joe Scarborough said during his show</a>:</p>
<blockquote>
<blockquote>
<p>Anybody that thinks that this race is anything but a toss-up right now is such an ideologue … they’re jokes.</p>
</blockquote>
</blockquote>
<p>To which Nate Silver responded via Twitter:</p>
<blockquote>
<blockquote>
<p>If you think it’s a toss-up, let’s bet. If Obama wins, you donate $1,000 to the American Red Cross. If Romney wins, I do. Deal?</p>
</blockquote>
</blockquote>
<p>In 2016, Silver was not as certain and gave Hillary Clinton only a 71% of winning. In contrast, most other forecasters were almost certain she would win. She lost. But 71% is still more than 50%, so was Mr. Silver wrong? And what does probability mean in this context anyway? Are dice being tossed somewhere?</p>
<p>In this section we will demonstrate how <em>poll aggregators</em>, such as FiveThirtyEight, collected and combined data reported by different experts to produce improved predictions. We will introduce ideas behind the <em>statistical models</em>, also known as <em>probability models</em>, that were used by poll aggregators to improve election forecasts beyond the power of individual polls. In this section, we motivate the models, building on the statistical inference concepts we learned in the textbook <a href="https://rafalab.github.io/dsbook/inference.html#inference">(Section - 15 Statistical inference)</a>. We start with relatively simple models, realizing that the actual data science exercise of forecasting elections involves rather complex ones, which we introduce towards the end of the section in the textbook <a href="https://rafalab.github.io/dsbook/models.html#election-forecasting">(Section - 16.8 Case study: election forecasting)</a>.</p>
<p>As we described earlier, a few weeks before the 2012 election Nate Silver was giving Obama a 90% chance of winning. How was Mr. Silver so confident? We will use a Monte Carlo simulation to illustrate the insight Mr. Silver had and others missed. To do this, we generate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls and construct and report 95% confidence intervals for each of the 12 polls. We save the results from this simulation in a data frame and add a poll ID column.</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)
d &lt;- 0.039
Ns &lt;- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p &lt;- (d + 1) / 2

polls &lt;- map_df(Ns, function(N) {
  x &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
  x_hat &lt;- mean(x)
  se_hat &lt;- sqrt(x_hat * (1 - x_hat) / N)
  list(estimate = 2 * x_hat - 1, 
    low = 2*(x_hat - 1.96*se_hat) - 1, 
    high = 2*(x_hat + 1.96*se_hat) - 1,
    sample_size = N)
}) %&gt;% mutate(poll = seq_along(Ns))</code></pre>
<p>Here is a visualization showing the intervals the pollsters would have reported for the difference between Obama and Romney:</p>
<p><img src="datsci_04_files/figure-html/simulated-polls-1.png" width="624" /></p>
<p>Not surprisingly, all 12 polls report confidence intervals that include the election night result (dashed line). However, all 12 polls also include 0 (solid black line) as well. Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up. Below we describe a key insight they are missing.</p>
<p>Poll aggregators, such as Nate Silver, realized that by combining the results of different polls you could greatly improve precision. By doing this, we are effectively conducting a poll with a huge sample size. We can therefore report a smaller 95% confidence interval and a more precise prediction.</p>
<p>Although as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what we would have obtained had we made one large poll with:</p>
<pre class="r"><code>sum(polls$sample_size)</code></pre>
<pre><code>## [1] 11269</code></pre>
<p>participants. Basically, we construct an estimate of the spread, let’s call it <span class="math inline">\(d\)</span>, with a weighted average in the following way:</p>
<pre class="r"><code>d_hat &lt;- polls %&gt;% 
  summarize(avg = sum(estimate*sample_size) / sum(sample_size)) %&gt;% 
  pull(avg)</code></pre>
<p>Once we have an estimate of <span class="math inline">\(d\)</span>, we can construct an estimate for the proportion voting for Obama, which we can then use to estimate the standard error. Once we do this, we see that our margin of error is 0.0184545.</p>
<p>Thus, we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result we eventually observed on election night, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote.</p>
<p><img src="datsci_04_files/figure-html/confidence-coverage-2008-election-1.png" width="624" /></p>
<p>Of course, this was just a simulation to illustrate the idea. The actual data science exercise of forecasting elections is much more complicated and it involves modeling. Below we explain how pollsters fit multilevel models to the data and use this to forecast election results. In the 2008 and 2012 US presidential elections, Nate Silver used this approach to make an almost perfect prediction and silence the pundits.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Poll aggregators combine the results of many polls to simulate polls with a large sample size and therefore generate more precise estimates than individual polls.</p></li>
<li><p>Polls can be simulated with a Monte Carlo simulation and used to construct an estimate of the spread and confidence intervals.</p></li>
<li><p>The actual data science exercise of forecasting elections involves more complex statistical modeling, but these underlying ideas still apply.</p></li>
</ul>
</div>
</div>
<div id="section-pollsters-and-multilevel-models" class="section level3">
<h3>Pollsters and Multilevel Models</h3>
<p>Since the 2008 elections, other organizations have started their own election forecasting group that, like Nate Silver’s, aggregates polling data and uses statistical models to make predictions. In 2016, forecasters underestimated Trump’s chances of winning greatly. The day before the election the <a href="https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html"><em>New York Times</em> reported</a> the following probabilities for Hillary Clinton winning the presidency:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
NYT
</th>
<th style="text-align:left;">
538
</th>
<th style="text-align:left;">
HuffPost
</th>
<th style="text-align:left;">
PW
</th>
<th style="text-align:left;">
PEC
</th>
<th style="text-align:left;">
DK
</th>
<th style="text-align:left;">
Cook
</th>
<th style="text-align:left;">
Roth
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Win Prob
</td>
<td style="text-align:left;">
85%
</td>
<td style="text-align:left;">
71%
</td>
<td style="text-align:left;">
98%
</td>
<td style="text-align:left;">
89%
</td>
<td style="text-align:left;">
&gt;99%
</td>
<td style="text-align:left;">
92%
</td>
<td style="text-align:left;">
Lean Dem
</td>
<td style="text-align:left;">
Lean Dem
</td>
</tr>
</tbody>
</table>
<!--(Source: [New York Times](https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html))-->
<p>For example, the Princeton Election Consortium (PEC) gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, higher than tossing two coins and getting two heads. In fact, four days before the election FiveThirtyEight published an article titled <a href="https://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/"><em>Trump Is Just A Normal Polling Error Behind Clinton</em></a>. By understanding statistical models and how these forecasters use them, we will start to understand how this happened.</p>
<p>Although not nearly as interesting as predicting the electoral college, for illustrative purposes we will start by looking at predictions for the popular vote. <a href="https://projects.fivethirtyeight.com/2016-election-forecast/">FiveThirtyEight predicted a 3.6% advantage for Clinton</a>, included the actual result of 2.1% (48.2% to 46.1%) in their interval, and was much more confident about Clinton winning the election, giving her an 81.4% chance. Their prediction was summarized with a chart like this:</p>
<p><img src="datsci_04_files/figure-html/fivethirtyeight-densities-1.png" width="80%" /></p>
<p>The colored areas represent values with an 80% chance of including the actual result, according to the FiveThirtyEight model. <!--(Source: [FiveThirtyEight](https://projects.fivethirtyeight.com/2016-election-forecast/))--></p>
<p>We introduce actual data from the 2016 US presidential election to show how models are motivated and built to produce these predictions. To understand the “81.4% chance” statement we need to describe Bayesian statistics, which we do in the textbook <a href="https://rafalab.github.io/dsbook/models.html#bayesian-statistics">(Section - 16.4 Bayesian statistics)</a> and the Book <a href="https://rafalab.github.io/dsbook/models.html#bayes-theorem-simulation">(Section - 16.5 Bayes theorem simulation)</a>.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Different poll aggregators generate different models of election results from the same poll data. This is because they use different statistical models.</p></li>
<li><p>We will use actual polling data about the popular vote from the 2016 US presidential election to learn the principles of statistical modeling.</p></li>
</ul>
</div>
</div>
<div id="section-poll-data-and-pollster-bias" class="section level3">
<h3>Poll Data and Pollster Bias</h3>
<p>We use public polling data organized by FiveThirtyEight for the 2016 presidential election. The data is included as part of the <strong>dslabs</strong> package:</p>
<pre class="r"><code>data(polls_us_election_2016)</code></pre>
<p>The table includes results for national polls, as well as state polls, taken during the year prior to the election. For this first example, we will filter the data to include national polls conducted during the week before the election. We also remove polls that FiveThirtyEight has determined not to be reliable and graded with a “B” or less. Some polls have not been graded and we include those:</p>
<pre class="r"><code>polls &lt;- polls_us_election_2016 %&gt;% 
  filter(state == &quot;U.S.&quot; &amp; enddate &gt;= &quot;2016-10-31&quot; &amp;
           (grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade)))</code></pre>
<p>We add a spread estimate:</p>
<pre class="r"><code>polls &lt;- polls %&gt;% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)</code></pre>
<p>For this example, we will assume that there are only two parties and call <span class="math inline">\(p\)</span> the proportion voting for Clinton and <span class="math inline">\(1-p\)</span> the proportion voting for Trump. We are interested in the spread <span class="math inline">\(2p-1\)</span>. Let’s call the spread <span class="math inline">\(d\)</span> (for difference).</p>
<p>We have 49 estimates of the spread. The theory we learned tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread <span class="math inline">\(d\)</span> and the standard error is <span class="math inline">\(2\sqrt{p (1 - p) / N}\)</span>. Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is:</p>
<pre class="r"><code>d_hat &lt;- polls %&gt;% 
  summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %&gt;% 
  pull(d_hat)</code></pre>
<p>and the standard error is:</p>
<pre class="r"><code>p_hat &lt;- (d_hat+1)/2 
moe &lt;- 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(polls$samplesize))
moe</code></pre>
<pre><code>## [1] 0.006623178</code></pre>
<p>So we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?</p>
<p>A histogram of the reported spreads shows a problem:</p>
<pre class="r"><code>polls %&gt;%
  ggplot(aes(spread)) +
  geom_histogram(color=&quot;black&quot;, binwidth = .01)</code></pre>
<p><img src="datsci_04_files/figure-html/polls-2016-spread-histogram-1.png" width="624" /></p>
<p>The data does not appear to be normally distributed and the standard error appears to be larger than 0.0066232. The theory is not quite working here.</p>
<div id="section-pollster-bias" class="section level4">
<h4><strong>Pollster bias</strong></h4>
<p>Notice that various pollsters are involved and some are taking several polls a week:</p>
<pre class="r"><code>polls %&gt;% group_by(pollster) %&gt;% summarize(n())</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["pollster"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["n()"],"name":[2],"type":["int"],"align":["right"]}],"data":[{"1":"ABC News/Washington Post","2":"7"},{"1":"Angus Reid Global","2":"1"},{"1":"CBS News/New York Times","2":"2"},{"1":"Fox News/Anderson Robbins Research/Shaw & Company Research","2":"2"},{"1":"IBD/TIPP","2":"8"},{"1":"Insights West","2":"1"},{"1":"Ipsos","2":"6"},{"1":"Marist College","2":"1"},{"1":"Monmouth University","2":"1"},{"1":"Morning Consult","2":"1"},{"1":"NBC News/Wall Street Journal","2":"1"},{"1":"RKM Research and Communications, Inc.","2":"1"},{"1":"Selzer & Company","2":"1"},{"1":"The Times-Picayune/Lucid","2":"8"},{"1":"USC Dornsife/LA Times","2":"8"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Let’s visualize the data for the pollsters that are regularly polling:</p>
<p><img src="datsci_04_files/figure-html/pollster-bias-1.png" width="624" /></p>
<p>This plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll:</p>
<pre class="r"><code>polls %&gt;% group_by(pollster) %&gt;% 
  filter(n() &gt;= 6) %&gt;%
  summarize(se = 2 * sqrt(p_hat * (1-p_hat) / median(samplesize)))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["pollster"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["se"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"ABC News/Washington Post","2":"0.02654389"},{"1":"IBD/TIPP","2":"0.03332069"},{"1":"Ipsos","2":"0.02251943"},{"1":"The Times-Picayune/Lucid","2":"0.01963986"},{"1":"USC Dornsife/LA Times","2":"0.01831826"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>is between 0.018 and 0.033, which agrees with the within poll variation we see. However, there appears to be differences <em>across the polls</em>. Note, for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump, while Ipsos is predicting a win larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values. All the polls should have the same expected value. FiveThirtyEight refers to these differences as “house effects”. We also call them <em>pollster bias</em>.</p>
<p>In the following section, rather than use the urn model theory, we are instead going to develop a data-driven model.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>We analyze real 2016 US polling data organized by FiveThirtyEight. We start by using reliable national polls taken within the week before the election to generate an urn model.</p></li>
<li><p>Consider 𝑝 the proportion voting for Clinton and 1 −𝑝 the proportion voting for Trump. We are interested in the spread 𝑑 = 2𝑝 − 1.</p></li>
<li><p>Poll results are a random normal variable with expected value of the spread 𝑑 and standard error <span class="math inline">\(2\sqrt{p (1 - p) / N}\)</span></p></li>
<li><p>Our initial estimate of the spread did not include the actual spread. Part of the reason is that different pollsters have different numbers of polls in our dataset, and each pollster has a bias.</p></li>
<li><p>Pollster bias reflects the fact that repeated polls by a given pollster have an expected value different from the actual spread and different from other pollsters. Each pollster has a different bias.</p></li>
<li><p>The urn model does not account for pollster bias. We will develop a more flexible data-driven model that can account for effects like bias.</p></li>
</ul>
</div>
</div>
</div>
<div id="section-data-driven-models" class="section level3">
<h3>Data-Driven Models</h3>
<p>For each pollster, let’s collect their last reported result before the election:</p>
<pre class="r"><code>one_poll_per_pollster &lt;- polls %&gt;% group_by(pollster) %&gt;% 
  filter(enddate == max(enddate)) %&gt;%
  ungroup()</code></pre>
<p>Here is a histogram of the data for these 15 pollsters:</p>
<pre class="r"><code>qplot(spread, data = one_poll_per_pollster, binwidth = 0.01)</code></pre>
<p><img src="datsci_04_files/figure-html/pollster-bias-histogram-1.png" width="624" /></p>
<p>In the previous section, we saw that using the urn model theory to combine these results might not be appropriate due to the pollster effect. Instead, we will model this spread data directly.</p>
<p>The new model can also be thought of as an urn model, although the connection is not as direct. Rather than 0s (Republicans) and 1s (Democrats), our urn now contains poll results from all possible pollsters. We <em>assume</em> that the expected value of our urn is the actual spread <span class="math inline">\(d=2p-1\)</span>.</p>
<p>Because instead of 0s and 1s, our urn contains continuous numbers between -1 and 1, the standard deviation of the urn is no longer <span class="math inline">\(\sqrt{p(1-p)}\)</span>. Rather than voter sampling variability, the standard error now includes the pollster-to-pollster variability. Our new urn also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter. In statistics textbooks, the Greek symbol <span class="math inline">\(\sigma\)</span> is used to represent this parameter.</p>
<p>In summary, we have two unknown parameters: the expected value <span class="math inline">\(d\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>Our task is to estimate <span class="math inline">\(d\)</span>. Because we model the observed values <span class="math inline">\(X_1,\dots X_N\)</span> as a random sample from the urn, the CLT might still work in this situation because it is an average of independent random variables. For a large enough sample size <span class="math inline">\(N\)</span>, the probability distribution of the sample average <span class="math inline">\(\bar{X}\)</span> is approximately normal with expected value <span class="math inline">\(\mu\)</span> and standard error <span class="math inline">\(\sigma/\sqrt{N}\)</span>. If we are willing to consider <span class="math inline">\(N=15\)</span> large enough, we can use this to construct confidence intervals.</p>
<p>A problem is that we don’t know <span class="math inline">\(\sigma\)</span>. But theory tells us that we can estimate the urn model <span class="math inline">\(\sigma\)</span> with the <em>sample standard deviation</em> defined as <span class="math inline">\(s = \sqrt{ \sum_{i=1}^N (X_i - \bar{X})^2 / (N-1)}\)</span>.</p>
<p>Unlike for the population standard deviation definition, we now divide by <span class="math inline">\(N-1\)</span>. This makes <span class="math inline">\(s\)</span> a better estimate of <span class="math inline">\(\sigma\)</span>. There is a mathematical explanation for this, which is explained in most statistics textbooks, but we don’t cover it here.</p>
<p>The <code>sd</code> function in R computes the sample standard deviation:</p>
<pre class="r"><code>sd(one_poll_per_pollster$spread)</code></pre>
<pre><code>## [1] 0.02419369</code></pre>
<p>We are now ready to form a new confidence interval based on our new data-driven model:</p>
<pre class="r"><code>results &lt;- one_poll_per_pollster %&gt;% 
  summarize(avg = mean(spread), 
            se = sd(spread) / sqrt(length(spread))) %&gt;% 
  mutate(start = avg - 1.96 * se, 
         end = avg + 1.96 * se) 
round(results * 100, 1)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["avg"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["se"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["start"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["end"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"2.9","2":"0.6","3":"1.7","4":"4.1"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Our confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.</p>
<p>Are we now ready to declare a probability of Clinton winning the popular vote? Not yet. In our model <span class="math inline">\(d\)</span> is a fixed parameter so we can’t talk about probabilities. To provide probabilities, we will need to learn about Bayesian statistics.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Instead of using an urn model where each poll is a random draw from the same distribution of voters, we instead define a model using an urn that contains poll results from all possible pollsters.</p></li>
<li><p>We assume the expected value of this model is the actual spread <span class="math inline">\(d = 2p - 1\)</span>.</p></li>
<li><p>Our new standard error 𝜎 now factors in pollster-to-pollster variability. It can no longer be calculated from 𝑝 or 𝑑 and is an unknown parameter.</p></li>
<li><p>The central limit theorem still works to estimate the sample average of many polls <span class="math inline">\(X_1,\dots X_N\)</span> because the average of the sum of many random variables is a normally distributed random variable with expected value 𝑑 and standard error <span class="math inline">\(\sigma/\sqrt{N}\)</span>.</p></li>
<li><p>We can estimate the unobserved 𝜎 as the sample standard deviation, which is calculated with the sd function.</p></li>
</ul>
</div>
</div>
<div id="section-assessment-statistical-models" class="section level3">
<h3>4.1 Assessment: Statistical Models</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-5-bayesian-statistics" class="section level2">
<h2>Section 5: Bayesian Statistics</h2>
<p>In Section 5, you will learn about Bayesian statistics through looking at examples from rare disease diagnosis and baseball.</p>
<p>After completing Section 5, you will be able to:</p>
<ul>
<li><p>Apply Bayes’ theorem to calculate the probability of A given B.</p></li>
<li><p>Understand how to use hierarchical models to make better predictions by considering multiple levels of variability.</p></li>
<li><p>Compute a posterior probability using an empirical Bayesian approach.</p></li>
<li><p>Calculate a 95% credible interval from a posterior probability.</p></li>
</ul>
<p>There are 2 assignments for you to practice your coding skills, and one for you to apply Bayesian statistics in a real-world context.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your learning.</p>
<div id="section-bayesian-statistics" class="section level3">
<h3>Bayesian Statistics</h3>
<p>What does it mean when an election forecaster tells us that a given candidate has a 90% chance of winning? In the context of the urn model, this would be equivalent to stating that the probability <span class="math inline">\(p&gt;0.5\)</span> is 90%. However, as we discussed earlier, in the urn model <span class="math inline">\(p\)</span> is a fixed parameter and it does not make sense to talk about probability. With Bayesian statistics, we model <span class="math inline">\(p\)</span> as random variable and thus a statement such as “90% chance of winning” is consistent with the approach.</p>
<p>Forecasters also use models to describe variability at different levels. For example, sampling variability, pollster to pollster variability, day to day variability, and election to election variability. One of the most successful approaches used for this are hierarchical models, which can be explained in the context of Bayesian statistics.</p>
<p>In this section we briefly describe Bayesian statistics. For an in-depth treatment of this topic we recommend one of the following textbooks:</p>
<ul>
<li><p>Berger JO (1985). Statistical Decision Theory and Bayesian Analysis, 2nd edition. Springer-Verlag.</p></li>
<li><p>Lee PM (1989). Bayesian Statistics: An Introduction. Oxford.</p></li>
</ul>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>In the urn model, it does not make sense to talk about the probability of 𝑝 being greater than a certain value because 𝑝 is a fixed value.</p></li>
<li><p>With Bayesian statistics, we assume that 𝑝 is in fact random, which allows us to calculate probabilities related to 𝑝.</p></li>
<li><p><em>Hierarchical models</em> describe variability at different levels and incorporate all these levels into a model for estimating 𝑝.</p></li>
</ul>
</div>
</div>
<div id="section-bayes-theorem" class="section level3">
<h3>Bayes’ Theorem</h3>
<p>We start by describing Bayes theorem. We do this using a hypothetical cystic fibrosis test as an example. Suppose a test for cystic fibrosis has an accuracy of 99%. We will use the following notation:</p>
<p><span class="math display">\[
\mbox{Prob}(+ \mid D=1)=0.99, \mbox{Prob}(- \mid D=0)=0.99 
\]</span></p>
<p>with <span class="math inline">\(+\)</span> meaning a positive test and <span class="math inline">\(D\)</span> representing if you actually have the disease (1) or not (0).</p>
<p>Suppose we select a random person and they test positive. What is the probability that they have the disease? We write this as <span class="math inline">\(\mbox{Prob}(D=1 \mid +)?\)</span> The cystic fibrosis rate is 1 in 3,900 which implies that <span class="math inline">\(\mbox{Prob}(D=1)=0.00025\)</span>. To answer this question, we will use Bayes theorem, which in general tells us that:</p>
<p><span class="math display">\[
\mbox{Pr}(A \mid B)  =  \frac{\mbox{Pr}(B \mid A)\mbox{Pr}(A)}{\mbox{Pr}(B)} 
\]</span></p>
<p>This equation applied to our problem becomes:</p>
<p><span class="math display">\[
\begin{aligned}
\mbox{Pr}(D=1 \mid +) &amp; =  \frac{ P(+ \mid D=1) \cdot P(D=1)} {\mbox{Pr}(+)} \\
&amp; =  \frac{\mbox{Pr}(+ \mid D=1)\cdot P(D=1)} {\mbox{Pr}(+ \mid D=1) \cdot P(D=1) + \mbox{Pr}(+ \mid D=0) \mbox{Pr}( D=0)} 
\end{aligned}
\]</span></p>
<p>Plugging in the numbers we get:</p>
<p><span class="math display">\[
\frac{0.99 \cdot 0.00025}{0.99 \cdot 0.00025 + 0.01 \cdot (.99975)}  =  0.02 
\]</span></p>
<p>This says that despite the test having 0.99 accuracy, the probability of having the disease given a positive test is only 0.02. This may appear counter-intuitive to some, but the reason this is the case is because we have to factor in the very rare probability that a person, chosen at random, has the disease. To illustrate this, we run a Monte Carlo simulation.</p>
<div id="section-bayes-theorem-simulation" class="section level4">
<h4><strong>Bayes theorem simulation</strong></h4>
<p>The following simulation is meant to help you visualize Bayes theorem. We start by randomly selecting 100,000 people from a population in which the disease in question has a 1 in 4,000 prevalence.</p>
<pre class="r"><code>prev &lt;- 0.00025
N &lt;- 100000
outcome &lt;- sample(c(&quot;Disease&quot;,&quot;Healthy&quot;), N, replace = TRUE, 
                  prob = c(prev, 1 - prev))</code></pre>
<p>Note that there are very few people with the disease:</p>
<pre class="r"><code>N_D &lt;- sum(outcome == &quot;Disease&quot;)
N_D</code></pre>
<pre><code>## [1] 23</code></pre>
<pre class="r"><code>N_H &lt;- sum(outcome == &quot;Healthy&quot;)
N_H</code></pre>
<pre><code>## [1] 99977</code></pre>
<p>Also, there are many without the disease, which makes it more probable that we will see some false positives given that the test is not perfect. Now each person gets the test, which is correct 99% of the time:</p>
<pre class="r"><code>accuracy &lt;- 0.99
test &lt;- vector(&quot;character&quot;, N)
test[outcome == &quot;Disease&quot;]  &lt;- sample(c(&quot;+&quot;, &quot;-&quot;), N_D, replace = TRUE, 
                                    prob = c(accuracy, 1 - accuracy))
test[outcome == &quot;Healthy&quot;]  &lt;- sample(c(&quot;-&quot;, &quot;+&quot;), N_H, replace = TRUE, 
                                    prob = c(accuracy, 1 - accuracy))</code></pre>
<p>Because there are so many more controls than cases, even with a low false positive rate we get more controls than cases in the group that tested positive:</p>
<pre class="r"><code>table(outcome, test)</code></pre>
<pre><code>##          test
## outcome       -     +
##   Disease     0    23
##   Healthy 99012   965</code></pre>
<p>From this table, we see that the proportion of positive tests that have the disease is 23 out of 988. We can run this over and over again to see that, in fact, the probability converges to about 0.022.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li>Bayes’ Theorem states that the probability of event A happening given event B is equal to the probability of both A and B divided by the probability of event B:</li>
</ul>
<p><span class="math display">\[
\mbox{Pr}(A \mid B)  =  \frac{\mbox{Pr}(B \mid A)\mbox{Pr}(A)}{\mbox{Pr}(B)} 
\]</span></p>
<ul>
<li>Bayes’ Theorem shows that a test for a very rare disease will have a high percentage of false positives even if the accuracy of the test is high.</li>
</ul>
</div>
</div>
</div>
<div id="section-bayes-in-practice" class="section level3">
<h3>Bayes in Practice</h3>
<p>José Iglesias is a professional baseball player. In April 2013, when he was starting his career, he was performing rather well:</p>
<table>
<thead>
<tr class="header">
<th>Month</th>
<th>At Bat</th>
<th>Hits</th>
<th>AVG</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>April</td>
<td>20</td>
<td>9</td>
<td>.450</td>
</tr>
</tbody>
</table>
<p>The batting average (<code>AVG</code>) statistic is one way of measuring success. Roughly speaking, it tells us the success rate when batting. An <code>AVG</code> of .450 means José has been successful 45% of the times he has batted (<code>At Bats</code>) which is rather high, historically speaking. Keep in mind that no one has finished a season with an <code>AVG</code> of .400 or more since Ted Williams did it in 1941! To illustrate the way hierarchical models are powerful, we will try to predict José’s batting average at the end of the season. Note that in a typical season, players have about 500 at bats.</p>
<p>With the techniques we have learned up to now, referred to as <em>frequentist techniques</em>, the best we can do is provide a confidence interval. We can think of outcomes from hitting as a binomial with a success rate of <span class="math inline">\(p\)</span>. So if the success rate is indeed .450, the standard error of just 20 at bats is:</p>
<p><span class="math display">\[
\sqrt{\frac{.450 (1-.450)}{20}}=.111
\]</span></p>
<p>This means that our confidence interval is <span class="math inline">\(.450 - .222\)</span> to <span class="math inline">\(.450 + .222\)</span> or <span class="math inline">\(.228\)</span> to <span class="math inline">\(.672\)</span>.</p>
<p>This prediction has two problems. First, it is very large, so not very useful. Second, it is centered at .450, which implies that our best guess is that this new player will break Ted Williams’ record.</p>
<p>If you follow baseball, this last statement will seem wrong and this is because you are implicitly using a hierarchical model that factors in information from years of following baseball. Here we show how we can quantify this intuition.</p>
<p>First, let’s explore the distribution of batting averages for all players with more than 500 at bats during the previous three seasons:</p>
<p><img src="datsci_04_files/figure-html/batting-averages-histogram-1.png" width="100%" /></p>
<p>The average player had an <code>AVG</code> of .275 and the standard deviation of the population of players was 0.027. So we can see already that .450 would be quite an anomaly since it is over six standard deviations away from the mean.</p>
<p>So is José lucky or is he the best batter seen in the last 50 years? Perhaps it’s a combination of both luck and talent. But how much of each? If we become convinced that he is lucky, we should trade him to a team that trusts the .450 observation and is maybe overestimating his potential.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>The techniques we have used up until now are referred to as <em>frequentist statistics</em> as they consider only the frequency of outcomes in a dataset and do not include any outside information. Frequentist statistics allow us to compute confidence intervals and p-values.</p></li>
<li><p>Frequentist statistics can have problems when sample sizes are small and when the data are extreme compared to historical results.</p></li>
<li><p><em>Bayesian statistics</em> allows prior knowledge to modify observed results, which alters our conclusions about event probabilities.</p></li>
</ul>
</div>
</div>
<div id="section-the-hierarchical-model" class="section level3">
<h3>The Hierarchical Model</h3>
<p>The hierarchical model provides a mathematical description of how we came to see the observation of .450. First, we pick a player at random with an intrinsic ability summarized by, for example, <span class="math inline">\(p\)</span>. Then we see 20 random outcomes with success probability <span class="math inline">\(p\)</span>.</p>
<p>We use a model to represent two levels of variability in our data. First, each player is assigned a natural ability to hit. We will use the symbol <span class="math inline">\(p\)</span> to represent this ability. You can think of <span class="math inline">\(p\)</span> as the batting average you would converge to if this particular player batted over and over again.</p>
<p>Based on the plots we showed earlier, we assume that <span class="math inline">\(p\)</span> has a normal distribution. With expected value .270 and standard error 0.027.</p>
<p>Now the second level of variability has to do with luck when batting. Regardless of how good the player is, sometimes you have bad luck and sometimes you have good luck. At each at bat, this player has a probability of success <span class="math inline">\(p\)</span>. If we add up these successes and failures, then the CLT tells us that the observed average, call it <span class="math inline">\(Y\)</span>, has a normal distribution with expected value <span class="math inline">\(p\)</span> and standard error <span class="math inline">\(\sqrt{p(1-p)/N}\)</span> with <span class="math inline">\(N\)</span> the number of at bats.</p>
<p>Statistical textbooks will write the model like this: <span class="math display">\[
\begin{aligned}
p &amp;\sim N(\mu, \tau^2) \\
Y \mid p &amp;\sim N(p, \sigma^2) 
\end{aligned}
\]</span> Here the <span class="math inline">\(\sim\)</span> symbol tells us the random variable on the left of the symbol follows the distribution on the right and <span class="math inline">\(N(a,b^2)\)</span> represents the normal distribution with mean <span class="math inline">\(a\)</span> and standard deviation <span class="math inline">\(b\)</span>. The <span class="math inline">\(\mid\)</span> is read as <em>conditioned on</em>, and it means that we are treating the random variable to the right of the symbol as known. We refer to the model as hierarchical because we need to know <span class="math inline">\(p\)</span>, the first level, in order to model <span class="math inline">\(Y\)</span>, the second level. In our example the first level describes randomness in assigning talent to a player and the second describes randomness in this particular player’s performance once we have fixed the talent parameter. In a Bayesian framework, the first level is called a <em>prior distribution</em> and the second the <em>sampling distribution</em>. The data analysis we have conducted here suggests that we set <span class="math inline">\(\mu = .270\)</span>, <span class="math inline">\(\tau = 0.027\)</span>, and <span class="math inline">\(\sigma^2 = p(1-p)/N\)</span>.</p>
<p>Now, let’s use this model for José’s data. Suppose we want to predict his innate ability in the form of his <em>true</em> batting average <span class="math inline">\(p\)</span>. This would be the hierarchical model for our data:</p>
<p><span class="math display">\[
\begin{aligned}
p &amp;\sim N(.275, .027^2) \\
Y \mid p &amp;\sim N(p, .111^2) 
\end{aligned}
\]</span></p>
<p>We now are ready to compute a posterior distribution to summarize our prediction of <span class="math inline">\(p\)</span>. The continuous version of Bayes’ rule can be used here to derive the <em>posterior probability function</em>, which is the distribution of <span class="math inline">\(p\)</span> assuming we observe <span class="math inline">\(Y=y\)</span>. In our case, we can show that when we fix <span class="math inline">\(Y=y\)</span>, <span class="math inline">\(p\)</span> follows a normal distribution with expected value:</p>
<p><span class="math display">\[
\begin{aligned}
\mbox{E}(p \mid Y=y) &amp;= B \mu + (1-B) y\\
&amp;= \mu + (1-B)(y-\mu)\\
\mbox{with } B &amp;= \frac{\sigma^2}{\sigma^2+\tau^2}
\end{aligned}
\]</span></p>
<p>This is a weighted average of the population average <span class="math inline">\(\mu\)</span> and the observed data <span class="math inline">\(y\)</span>. The weight depends on the SD of the population <span class="math inline">\(\tau\)</span> and the SD of our observed data <span class="math inline">\(\sigma\)</span>. This weighted average is sometimes referred to as <em>shrinking</em> because it <em>shrinks</em> estimates towards a prior mean. In the case of José Iglesias, we have:</p>
<p><span class="math display">\[
\begin{aligned}
\mbox{E}(p \mid Y=.450) &amp;= B \times .275 + (1 - B) \times .450 \\
&amp;= .275 + (1 - B)(.450 - .275) \\
B &amp;=\frac{.111^2}{.111^2 + .027^2} = 0.944\\
\mbox{E}(p \mid Y=450) &amp;\approx .285
\end{aligned}
\]</span></p>
<p>We do not show the derivation here, but the standard error can be shown to be:</p>
<p><span class="math display">\[
\mbox{SE}(p\mid y)^2 = \frac{1}{1/\sigma^2+1/\tau^2}
= \frac{1}{1/.111^2 + 1/.027^2} = 0.00069
\]</span> and the standard deviation is therefore <span class="math inline">\(0.026\)</span>. So we started with a frequentist 95% confidence interval that ignored data from other players and summarized just José’s data: .450 <span class="math inline">\(\pm\)</span> 0.220. Then we used a Bayesian approach that incorporated data from other players and other years to obtain a posterior probability. This is actually referred to as an empirical Bayes approach because we used data to construct the prior. From the posterior, we can report what is called a 95% <em>credible interval</em> by reporting a region, centered at the mean, with a 95% chance of occurring. In our case, this turns out to be: .285 <span class="math inline">\(\pm\)</span> 0.052.</p>
<p>The Bayesian credible interval suggests that if another team is impressed by the .450 observation, we should consider trading José as we are predicting he will be just slightly above average. Interestingly, the Red Sox traded José to the Detroit Tigers in July. Here are the José Iglesias batting averages for the next five months:</p>
<table>
<thead>
<tr class="header">
<th>Month</th>
<th>At Bat</th>
<th>Hits</th>
<th>AVG</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>April</td>
<td>20</td>
<td>9</td>
<td>.450</td>
</tr>
<tr class="even">
<td>May</td>
<td>26</td>
<td>11</td>
<td>.423</td>
</tr>
<tr class="odd">
<td>June</td>
<td>86</td>
<td>34</td>
<td>.395</td>
</tr>
<tr class="even">
<td>July</td>
<td>83</td>
<td>17</td>
<td>.205</td>
</tr>
<tr class="odd">
<td>August</td>
<td>85</td>
<td>25</td>
<td>.294</td>
</tr>
<tr class="even">
<td>September</td>
<td>50</td>
<td>10</td>
<td>.200</td>
</tr>
<tr class="odd">
<td>Total w/o April</td>
<td>330</td>
<td>97</td>
<td>.293</td>
</tr>
</tbody>
</table>
<p>Although both intervals included the final batting average, the Bayesian credible interval provided a much more precise prediction. In particular, it predicted that he would not be as good during the remainder of the season.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>Hierarchical models use multiple levels of variability to model results. They are hierarchical because values in the lower levels of the model are computed using values from higher levels of the model.</p></li>
<li><p>We model baseball player batting average using a hierarchical model with two levels of variability:</p>
<ul>
<li><p><span class="math inline">\(p \sim N(\mu, \tau)\)</span> describes player-to-player variability in natural ability to hit, which has a mean 𝜇 and standard deviation 𝜏.</p></li>
<li><p><span class="math inline">\(Y \mid p \sim N(p, \sigma)\)</span> describes a player’s observed batting average given their ability 𝑝, which has a mean 𝑝 and standard deviation <span class="math inline">\(𝜎=\sqrt{p(1-p)/N}\)</span>. This represents variability due to luck.</p></li>
<li><p>In Bayesian hierarchical models, the first level is called the prior distribution and the second level is called the sampling distribution.</p></li>
</ul></li>
<li><p>The posterior distribution allows us to compute the probability distribution of 𝑝 given that we have observed data 𝑌.</p></li>
<li><p>By the continuous version of Bayes’ rule, the expected value of the posterior distribution 𝑝 given 𝑌=𝑦 is a weighted average between the prior mean 𝜇 and the observed data 𝑌:</p></li>
</ul>
<p><span class="math display">\[
\mbox{E}(p \mid Y) = B \mu + (1-B)Y\]</span> where <span class="math display">\[B=\frac{\sigma^2}{\sigma^2+\tau^2}
\]</span></p>
<ul>
<li><p>The standard error of the posterior distribution <span class="math inline">\(\mbox{SE}(p \mid y^2)\)</span> is <span class="math inline">\(\frac{1}{1/\sigma^2+1/\tau^2}\)</span>. Note that you will need to take the square root of both sides to solve for the standard error.</p></li>
<li><p>This Bayesian approach is also known as shrinking. When 𝜎 is large, 𝐵 is close to 1 and our prediction of 𝑝 shrinks towards the mean (<span class="math inline">\(\mu\)</span>). When 𝜎 is small, 𝐵 is close to 0 and our prediction of 𝑝 is more weighted towards the observed data 𝑌.</p></li>
</ul>
</div>
</div>
<div id="section-assessment-bayesian-statistics" class="section level3">
<h3>5.1 Assessment: Bayesian Statistics</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-6-election-forecasting" class="section level2">
<h2>Section 6: Election Forecasting</h2>
<p>In Section 6, you will learn about election forecasting, building on what you’ve learned in the previous sections about statistical modeling and Bayesian statistics.</p>
<p>After completing Section 6, you will be able to:</p>
<ul>
<li><p>Understand how pollsters use hierarchical models to forecast the results of elections.</p></li>
<li><p>Incorporate multiple sources of variability into a mathematical model to make predictions.</p></li>
<li><p>Construct confidence intervals that better model deviations such as those seen in election data using the t-distribution.</p></li>
</ul>
<p>There are 2 assignments for you to practice your coding skills.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your learning.</p>
<div id="section-election-forecasting" class="section level3">
<h3>Election Forecasting</h3>
<p>In a previous section, we generated these data tables:</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)
polls &lt;- polls_us_election_2016 %&gt;% 
  filter(state == &quot;U.S.&quot; &amp; enddate &gt;= &quot;2016-10-31&quot; &amp;
           (grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade))) %&gt;% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

one_poll_per_pollster &lt;- polls %&gt;% group_by(pollster) %&gt;% 
  filter(enddate == max(enddate)) %&gt;%
  ungroup()

results &lt;- one_poll_per_pollster %&gt;% 
  summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %&gt;% 
  mutate(start = avg - 1.96*se, end = avg + 1.96*se) </code></pre>
<p>Below, we will use these for our forecasting.</p>
</div>
<div id="section-bayesian-approach" class="section level3">
<h3>Bayesian approach</h3>
<p>Pollsters tend to make probabilistic statements about the results of the election. For example, “The chance that Obama wins the electoral college is 91%” is a probabilistic statement about a parameter which in previous sections we have denoted with <span class="math inline">\(d\)</span>. We showed that for the 2016 election, FiveThirtyEight gave Clinton an 81.4% chance of winning the popular vote. To do this, they used the Bayesian approach we described.</p>
<p>We assume a hierarchical model similar to what we did to predict the performance of a baseball player. Statistical textbooks will write the model like this:</p>
<p><span class="math display">\[
\begin{aligned}
d &amp;\sim N(\mu, \tau^2) \mbox{ describes our best guess had we not seen any polling data}\\
\bar{X} \mid d &amp;\sim N(d, \sigma^2) \mbox{ describes randomness due to sampling and the  pollster effect}
\end{aligned}
\]</span></p>
<p>For our best guess, we note that before any poll data is available, we can use data sources other than polling data. A popular approach is to use what pollsters call <em>fundamentals</em>, which are based on properties about the current economy that historically appear to have an effect in favor or against the incumbent party. We won’t use these here. Instead, we will use <span class="math inline">\(\mu = 0\)</span>, which is interpreted as a model that simply does not provide any information on who will win. For the standard deviation, we will use recent historical data that shows the winner of the popular vote has an average spread of about 3.5%. Therefore, we set <span class="math inline">\(\tau = 0.035\)</span>.</p>
<p>Now we can use the formulas for the posterior distribution for the parameter <span class="math inline">\(d\)</span>: the probability of <span class="math inline">\(d&gt;0\)</span> given the observed poll data:</p>
<pre class="r"><code>mu &lt;- 0
tau &lt;- 0.035
sigma &lt;- results$se
Y &lt;- results$avg
B &lt;- sigma^2 / (sigma^2 + tau^2)

posterior_mean &lt;- B*mu + (1-B)*Y
posterior_se &lt;- sqrt( 1/ (1/sigma^2 + 1/tau^2))

posterior_mean</code></pre>
<pre><code>## [1] 0.02808534</code></pre>
<pre class="r"><code>posterior_se</code></pre>
<pre><code>## [1] 0.006149604</code></pre>
<p>To make a probability statement, we use the fact that the posterior distribution is also normal. And we have a credible interval of:</p>
<pre class="r"><code>posterior_mean + c(-1.96, 1.96)*posterior_se</code></pre>
<pre><code>## [1] 0.01603212 0.04013857</code></pre>
<p>The posterior probability <span class="math inline">\(\mbox{Pr}(d&gt;0 \mid \bar{X})\)</span> can be computed like this:</p>
<pre class="r"><code>1 - pnorm(0, posterior_mean, posterior_se)</code></pre>
<pre><code>## [1] 0.9999975</code></pre>
<p>This says we are 100% sure Clinton will win the popular vote, which seems too overconfident. Also, it is not in agreement with FiveThirtyEight’s 81.4%. What explains this difference?</p>
<div id="section-the-general-bias" class="section level4">
<h4><strong>The general bias</strong></h4>
<p>After elections are over, one can look at the difference between pollster predictions and actual result. An important observation that our model does not take into account is that it is common to see a general bias that affects many pollsters in the same way making the observed data correlated. There is no good explanation for this, but we do observe it in historical data: in one election, the average of polls favors Democrats by 2%, then in the following election they favor Republicans by 1%, then in the next election there is no bias, then in the following one Republicans are favored by 3%, and so on. In 2016, the polls were biased in favor of the Democrats by 1-2%.</p>
<p>Although we know this bias term affects our polls, we have no way of knowing what this bias is until election night. So we can’t correct our polls accordingly. What we can do is include a term in our model that accounts for this variability.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>In our model:</p>
<ul>
<li><p>The spread <span class="math inline">\(d \sim N(\mu, \tau^2)\)</span> describes our best guess in the absence of polling data. We set 𝜇 = 0 and 𝜏 = 0.035 using historical data.</p></li>
<li><p>The average of observed data <span class="math inline">\(\bar{X} \mid d \sim N(d, \sigma^2)\)</span> describes randomness due to sampling and the pollster effect.</p></li>
</ul></li>
<li><p>Because the posterior distribution is normal, we can report a 95% credible interval that has a 95% chance of overlapping the parameter using <span class="math inline">\(E(p∣Y)\)</span> and <span class="math inline">\(SE(p∣Y)\)</span>.</p></li>
<li><p>Given an estimate of <span class="math inline">\(E(p∣Y)\)</span> and <span class="math inline">\(SE(p∣Y)\)</span>, we can use pnorm to compute the probability that 𝑑 &gt; 0.</p></li>
<li><p>It is common to see a general bias that affects all pollsters in the same way. This bias cannot be predicted or measured before the election. We will include a term in later models to account for this variability.</p></li>
</ul>
</div>
</div>
</div>
<div id="section-mathematical-representations-of-models" class="section level3">
<h3>Mathematical Representations of Models</h3>
<p>Suppose we are collecting data from one pollster and we assume there is no general bias. The pollster collects several polls with a sample size of <span class="math inline">\(N\)</span>, so we observe several measurements of the spread <span class="math inline">\(X_1, \dots, X_J\)</span>. The theory tells us that these random variables have expected value <span class="math inline">\(d\)</span> and standard error <span class="math inline">\(2 \sqrt{p(1-p)/N}\)</span>. Let’s start by using the following model to describe the observed variability:</p>
<p><span class="math display">\[
X_j = d + \varepsilon_j.
\]</span> We use the index <span class="math inline">\(j\)</span> to represent the different polls and we define <span class="math inline">\(\varepsilon_j\)</span> to be a random variable that explains the poll-to-poll variability introduced by sampling error. To do this, we assume its average is 0 and standard error is <span class="math inline">\(2 \sqrt{p(1-p)/N}\)</span>. If <span class="math inline">\(d\)</span> is 2.1 and the sample size for these polls is 2,000, we can simulate <span class="math inline">\(J=6\)</span> data points from this model like this:</p>
<pre class="r"><code>set.seed(3)
J &lt;- 6
N &lt;- 2000
d &lt;- .021
p &lt;- (d + 1)/2
X &lt;- d + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))</code></pre>
<p>Now suppose we have <span class="math inline">\(J=6\)</span> data points from <span class="math inline">\(I=5\)</span> different pollsters. To represent this we now need two indexes, one for pollster and one for the polls each pollster takes. We use <span class="math inline">\(X_{ij}\)</span> with <span class="math inline">\(i\)</span> representing the pollster and <span class="math inline">\(j\)</span> representing the <span class="math inline">\(j\)</span>-th poll from that pollster. If we apply the same model, we write:</p>
<p><span class="math display">\[
X_{i,j} = d + \varepsilon_{i,j}
\]</span></p>
<p>To simulate data, we now have to loop through the pollsters:</p>
<pre class="r"><code>I &lt;- 5
J &lt;- 6
N &lt;- 2000
X &lt;- sapply(1:I, function(i){
  d + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))
})</code></pre>
<p>The simulated data does not really seem to capture the features of the actual data:</p>
<center>
<figure>
<img src="images/pic-xyz.png" alt="R Studio" style="width:800px">
</figure>
</center>
<p>The model above does not account for pollster-to-pollster variability. To fix this, we add a new term for the pollster effect. We will use <span class="math inline">\(h_i\)</span> to represent the house effect of the <span class="math inline">\(i\)</span>-th pollster. The model is now augmented to:</p>
<p><span class="math display">\[
X_{i,j} = d + h_i + \varepsilon_{i,j}
\]</span></p>
<p>To simulate data from a specific pollster, we now need to draw an <span class="math inline">\(h_i\)</span> and then add the <span class="math inline">\(\varepsilon\)</span>s. Here is how we would do it for one specific pollster. We assume <span class="math inline">\(\sigma_h\)</span> is 0.025:</p>
<pre class="r"><code>I &lt;- 5
J &lt;- 6
N &lt;- 2000
d &lt;- .021
p &lt;- (d + 1) / 2
h &lt;- rnorm(I, 0, 0.025)
X &lt;- sapply(1:I, function(i){
  d + h[i] + rnorm(J, 0, 2 * sqrt(p * (1 - p) / N))
})</code></pre>
<p>The simulated data now looks more like the actual data:</p>
<p><img src="datsci_04_files/figure-html/simulated-pollster-data-1.png" width="35%" /></p>
<p>Note that <span class="math inline">\(h_i\)</span> is common to all the observed spreads from a specific pollster. Different pollsters have a different <span class="math inline">\(h_i\)</span>, which explains why we can see the groups of points shift up and down from pollster to pollster.</p>
<p>Now, in the model above, we assume the average house effect is 0. We think that for every pollster biased in favor of our party, there is another one in favor of the other and assume the standard deviation is <span class="math inline">\(\sigma_h\)</span>. But historically we see that every election has a general bias affecting all polls. We can observe this with the 2016 data, but if we collect historical data, we see that the average of polls misses by more than models like the one above predict. To see this, we would take the average of polls for each election year and compare it to the actual value. If we did this, we would see a difference with a standard deviation of between 2-3%. To incorporate this into the model, we can add another term to account for this variability: <span class="math display">\[
X_{i,j} = d + b + h_i + \varepsilon_{i,j}.
\]</span></p>
<p>Here <span class="math inline">\(b\)</span> is a random variable that accounts for the election-to-election variability. This random variable changes from election to election, but for any given election, it is the same for all pollsters and polls within on election. This is why it does not have indexes. This implies that all the random variables <span class="math inline">\(X_{i,j}\)</span> for an election year are correlated since they all have <span class="math inline">\(b\)</span> in common.</p>
<p>One way to interpret <span class="math inline">\(b\)</span> is as the difference between the average of all polls from all pollsters and the actual result of the election. Because we don’t know the actual result until after the election, we can’t estimate <span class="math inline">\(b\)</span> until after the election. However, we can estimate <span class="math inline">\(b\)</span> from previous elections and study the distribution of these values. Based on this approach we assume that, across election years, <span class="math inline">\(b\)</span> has expected value 0 and the standard error is about <span class="math inline">\(\sigma_b = 0.025\)</span>.</p>
<p>An implication of adding this term to the model is that the standard deviation for <span class="math inline">\(X_{i,j}\)</span> is actually higher than what we earlier called <span class="math inline">\(\sigma\)</span>, which combines the pollster variability and the sample in variability, and was estimated with:</p>
<pre class="r"><code>sd(one_poll_per_pollster$spread)</code></pre>
<pre><code>## [1] 0.02419369</code></pre>
<p>This estimate does not include the variability introduced by <span class="math inline">\(b\)</span>. Note that because</p>
<p><span class="math display">\[
\bar{X} = d + b + \frac{1}{N}\sum_{i=1}^N X_i,
\]</span></p>
<p>the standard deviation of <span class="math inline">\(\bar{X}\)</span> is:</p>
<p><span class="math display">\[
\sqrt{\sigma^2/N + \sigma_b^2}.
\]</span> Since the same <span class="math inline">\(b\)</span> is in every measurement, the average does not reduce the variability introduced by the <span class="math inline">\(b\)</span> term. This is an important point: it does not matter how many polls you take, this bias does not get reduced.</p>
<p>If we redo the Bayesian calculation taking this variability into account, we get a result much closer to FiveThirtyEight’s:</p>
<pre class="r"><code>mu &lt;- 0
tau &lt;- 0.035
sigma &lt;- sqrt(results$se^2 + .025^2)
Y &lt;- results$avg
B &lt;- sigma^2 / (sigma^2 + tau^2)

posterior_mean &lt;- B*mu + (1-B)*Y
posterior_se &lt;- sqrt( 1/ (1/sigma^2 + 1/tau^2))

1 - pnorm(0, posterior_mean, posterior_se)</code></pre>
<pre><code>## [1] 0.8174373</code></pre>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>If we collect several polls with measured spreads <span class="math inline">\(X_1, \dots, X_J\)</span> with a sample size of 𝑁, these random variables have expected value 𝑑 and standard error <span class="math inline">\(2 \sqrt{p(1-p)/N}\)</span>.</p></li>
<li><p>We represent each measurement as <span class="math display">\[X_{i,j} = d + b + h_i + \varepsilon_{i,j}\]</span> where:</p>
<ul>
<li>The index 𝑖 represents the different pollsters</li>
<li>The index 𝑗 represents the different polls</li>
<li><span class="math inline">\(X_{i,j}\)</span> is the 𝑗th poll by the 𝑖th pollster</li>
<li>𝑑 is the actual spread of the election</li>
<li>𝑏 is the general bias affecting all pollsters</li>
<li>ℎ𝑖 represents the house effect for the 𝑖th pollster</li>
<li><span class="math inline">\(\varepsilon_{i,j}\)</span> represents the random error associated with the 𝑖,𝑗th poll.</li>
</ul></li>
<li><p>The sample average is now <span class="math inline">\(\bar{X} = d + b + \frac{1}{N}\sum_{i=1}^N X_i\)</span> with standard deviation <span class="math inline">\(\sqrt{\sigma^2/N + \sigma_b^2}\)</span>.</p></li>
<li><p>The standard error of the general bias 𝜎𝑏 does not get reduced by averaging multiple polls, which increases the variability of our final estimate.</p></li>
</ul>
</div>
</div>
<div id="section-predicting-the-electoral-college" class="section level3">
<h3>Predicting the Electoral College</h3>
<p>Up to now we have focused on the popular vote. But in the United States, elections are not decided by the popular vote but rather by what is known as the electoral college. Each state gets a number of electoral votes that depends, in a somewhat complex way, on the population size of the state. Here are the top 5 states ranked by electoral votes in 2016.</p>
<pre class="r"><code>results_us_election_2016 %&gt;% top_n(5, electoral_votes)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["state"],"name":[1],"type":["chr"],"align":["left"]},{"label":["electoral_votes"],"name":[2],"type":["int"],"align":["right"]},{"label":["clinton"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["trump"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["others"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"California","2":"55","3":"61.7","4":"31.6","5":"6.7"},{"1":"Texas","2":"38","3":"43.2","4":"52.2","5":"4.5"},{"1":"Florida","2":"29","3":"47.8","4":"49.0","5":"3.2"},{"1":"New York","2":"29","3":"59.0","4":"36.5","5":"4.5"},{"1":"Illinois","2":"20","3":"55.8","4":"38.8","5":"5.4"},{"1":"Pennsylvania","2":"20","3":"47.9","4":"48.6","5":"3.6"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>With some minor exceptions we don’t discuss, the electoral votes are won all or nothing. For example, if you win California by just 1 vote, you still get all 55 of its electoral votes. This means that by winning a few big states by a large margin, but losing many small states by small margins, you can win the popular vote and yet lose the electoral college. This happened in 1876, 1888, 2000, and 2016. The idea behind this is to avoid a few large states having the power to dominate the presidential election. Nonetheless, many people in the US consider the electoral college unfair and would like to see it abolished.</p>
<p>We are now ready to predict the electoral college result for 2016. We start by aggregating results from a poll taken during the last week before the election. We use the <code>str_detect</code>, a function we introduce later in the textbook <a href="https://rafalab.github.io/dsbook/string-processing.html">(Section - 24 String processing)</a>, to remove polls that are not for entire states.</p>
<pre class="r"><code>results &lt;- polls_us_election_2016 %&gt;%
  filter(state!=&quot;U.S.&quot; &amp; 
           !str_detect(state, &quot;CD&quot;) &amp; 
           enddate &gt;=&quot;2016-10-31&quot; &amp; 
           (grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade))) %&gt;%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %&gt;%
  group_by(state) %&gt;%
  summarize(avg = mean(spread), sd = sd(spread), n = n()) %&gt;%
  mutate(state = as.character(state))</code></pre>
<p>Here are the five closest races according to the polls:</p>
<pre class="r"><code>results %&gt;% arrange(abs(avg))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["state"],"name":[1],"type":["chr"],"align":["left"]},{"label":["avg"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["sd"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["n"],"name":[4],"type":["int"],"align":["right"]}],"data":[{"1":"Florida","2":"0.003557143","3":"0.0163496032","4":"7"},{"1":"North Carolina","2":"-0.007300000","3":"0.0306208262","4":"9"},{"1":"Ohio","2":"-0.010416667","3":"0.0252247828","4":"6"},{"1":"Nevada","2":"0.016857143","3":"0.0440680858","4":"7"},{"1":"Iowa","2":"-0.019733333","3":"0.0436647837","4":"3"},{"1":"Michigan","2":"0.020950000","3":"0.0202606762","4":"6"},{"1":"Arizona","2":"-0.032644444","3":"0.0269547357","4":"9"},{"1":"Pennsylvania","2":"0.035333333","3":"0.0116052790","4":"9"},{"1":"New Mexico","2":"0.038933333","3":"0.0225518661","4":"6"},{"1":"Georgia","2":"-0.044850000","3":"0.0238115518","4":"4"},{"1":"Colorado","2":"0.045200000","3":"0.0294593505","4":"7"},{"1":"Virginia","2":"0.049240000","3":"0.0183777855","4":"5"},{"1":"New Hampshire","2":"0.056416667","3":"0.0359483679","4":"12"},{"1":"Minnesota","2":"0.060500000","3":"0.0163477827","4":"3"},{"1":"Utah","2":"-0.063800000","3":"0.0053740115","4":"2"},{"1":"South Carolina","2":"-0.065100000","3":"0.0305768976","4":"4"},{"1":"Wisconsin","2":"0.071066667","3":"0.0104145411","4":"6"},{"1":"Maine","2":"0.077450000","3":"0.0070003571","4":"2"},{"1":"Connecticut","2":"0.078033333","3":"0.0210984202","4":"3"},{"1":"Oregon","2":"0.091200000","3":"0.0052848841","4":"3"},{"1":"Texas","2":"-0.095200000","3":"0.0038807216","4":"4"},{"1":"Washington","2":"0.101260000","3":"0.0330086049","4":"5"},{"1":"Missouri","2":"-0.104983333","3":"0.0324377815","4":"6"},{"1":"New Jersey","2":"0.109166667","3":"0.0234000712","4":"3"},{"1":"Illinois","2":"0.119400000","3":"0.0098045908","4":"3"},{"1":"Delaware","2":"0.132400000","3":"0.0335168614","4":"2"},{"1":"Tennessee","2":"-0.145666667","3":"0.0110183181","4":"3"},{"1":"Alabama","2":"-0.149433333","3":"0.0253279161","4":"3"},{"1":"Arkansas","2":"-0.151400000","3":"0.0009899495","4":"2"},{"1":"Kansas","2":"-0.156850000","3":"0.0077074639","4":"2"},{"1":"Indiana","2":"-0.159366667","3":"0.0111073549","4":"3"},{"1":"Louisiana","2":"-0.164050000","3":"0.0136471609","4":"2"},{"1":"New York","2":"0.184200000","3":"0.0272118847","4":"4"},{"1":"Montana","2":"-0.184800000","3":"0.0060811183","4":"2"},{"1":"Hawaii","2":"0.185600000","3":"NA","4":"1"},{"1":"Maryland","2":"0.197300000","3":"0.0208453832","4":"3"},{"1":"Massachusetts","2":"0.201575000","3":"0.0660505551","4":"4"},{"1":"Mississippi","2":"-0.204066667","3":"0.0391203698","4":"3"},{"1":"South Dakota","2":"-0.207300000","3":"0.0280014285","4":"2"},{"1":"Nebraska","2":"-0.214800000","3":"0.0038431758","4":"3"},{"1":"Idaho","2":"-0.224550000","3":"0.0105358910","4":"2"},{"1":"North Dakota","2":"-0.235600000","3":"NA","4":"1"},{"1":"Kentucky","2":"-0.236500000","3":"0.0311583055","4":"3"},{"1":"West Virginia","2":"-0.237833333","3":"0.0004725816","4":"3"},{"1":"Vermont","2":"0.248700000","3":"NA","4":"1"},{"1":"California","2":"0.260100000","3":"0.0387329317","4":"5"},{"1":"Oklahoma","2":"-0.307650000","3":"0.0023334524","4":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>We now introduce the command <code>left_join</code> that will let us easily add the number of electoral votes for each state from the dataset <code>us_electoral_votes_2016</code>. We will describe this function in detail in the Wrangling section. Here, we simply say that the function combines the two datasets so that the information from the second argument is added to the information in the first:</p>
<pre class="r"><code>results &lt;- left_join(results, results_us_election_2016, by = &quot;state&quot;)</code></pre>
<p>Notice that some states have no polls because the winner is pretty much known:</p>
<pre class="r"><code>results_us_election_2016 %&gt;% filter(!state %in% results$state) %&gt;% 
  pull(state)</code></pre>
<pre><code>## [1] &quot;Rhode Island&quot;         &quot;Alaska&quot;               &quot;Wyoming&quot;             
## [4] &quot;District of Columbia&quot;</code></pre>
<p>No polls were conducted in DC, Rhode Island, Alaska, and Wyoming because Democrats are sure to win in the first two and Republicans in the last two.</p>
<p>Because we can’t estimate the standard deviation for states with just one poll, we will estimate it as the median of the standard deviations estimated for states with more than one poll:</p>
<pre class="r"><code>results &lt;- results %&gt;%
  mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))</code></pre>
<p>To make probabilistic arguments, we will use a Monte Carlo simulation. For each state, we apply the Bayesian approach to generate an election day <span class="math inline">\(d\)</span>. We could construct the priors for each state based on recent history. However, to keep it simple, we assign a prior to each state that assumes we know nothing about what will happen. Since from election year to election year the results from a specific state don’t change that much, we will assign a standard deviation of 2% or <span class="math inline">\(\tau=0.02\)</span>. For now, we will assume, incorrectly, that the poll results from each state are independent. The code for the Bayesian calculation under these assumptions looks like this:</p>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["state"],"name":[1],"type":["chr"],"align":["left"]},{"label":["avg"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["sd"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["n"],"name":[4],"type":["int"],"align":["right"]},{"label":["electoral_votes"],"name":[5],"type":["int"],"align":["right"]},{"label":["clinton"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["trump"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["others"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["sigma"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["B"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["posterior_mean"],"name":[11],"type":["dbl"],"align":["right"]},{"label":["posterior_se"],"name":[12],"type":["dbl"],"align":["right"]}],"data":[{"1":"Alabama","2":"-0.149433333","3":"0.0253279161","4":"3","5":"9","6":"34.4","7":"62.1","8":"3.6","9":"0.0146230792","10":"0.3483584970","11":"-0.097376962","12":"0.0118043805"},{"1":"Arizona","2":"-0.032644444","3":"0.0269547357","4":"9","5":"11","6":"45.1","7":"48.7","8":"6.2","9":"0.0089849119","10":"0.1679297527","11":"-0.027162471","12":"0.0081958466"},{"1":"Arkansas","2":"-0.151400000","3":"0.0009899495","4":"2","5":"6","6":"33.7","7":"60.6","8":"5.8","9":"0.0007000000","10":"0.0012235012","11":"-0.151214762","12":"0.0006995716"},{"1":"California","2":"0.260100000","3":"0.0387329317","4":"5","5":"55","6":"61.7","7":"31.6","8":"6.7","9":"0.0173218937","10":"0.4286106096","11":"0.148618380","12":"0.0130936719"},{"1":"Colorado","2":"0.045200000","3":"0.0294593505","4":"7","5":"9","6":"48.2","7":"43.3","8":"8.6","9":"0.0111345879","10":"0.2366106969","11":"0.034505197","12":"0.0097285291"},{"1":"Connecticut","2":"0.078033333","3":"0.0210984202","4":"3","5":"7","6":"54.6","7":"40.9","8":"4.5","9":"0.0121811786","10":"0.2705802737","11":"0.056919053","12":"0.0104034662"},{"1":"Delaware","2":"0.132400000","3":"0.0335168614","4":"2","5":"3","6":"53.4","7":"41.9","8":"4.7","9":"0.0237000000","10":"0.5840655513","11":"0.055069721","12":"0.0152848363"},{"1":"Florida","2":"0.003557143","3":"0.0163496032","4":"7","5":"29","6":"47.8","7":"49.0","8":"3.2","9":"0.0061795691","10":"0.0871478805","11":"0.003247145","12":"0.0059041640"},{"1":"Georgia","2":"-0.044850000","3":"0.0238115518","4":"4","5":"16","6":"45.9","7":"51.0","8":"3.1","9":"0.0119057759","10":"0.2616486463","11":"-0.033115058","12":"0.0102303205"},{"1":"Hawaii","2":"0.185600000","3":"0.0209719017","4":"1","5":"4","6":"62.2","7":"30.0","8":"7.7","9":"0.0209719017","10":"0.5237078353","11":"0.088399826","12":"0.0144735322"},{"1":"Idaho","2":"-0.224550000","3":"0.0105358910","4":"2","5":"4","6":"27.5","7":"59.3","8":"13.2","9":"0.0074500000","10":"0.1218489470","11":"-0.197188819","12":"0.0069813737"},{"1":"Illinois","2":"0.119400000","3":"0.0098045908","4":"3","5":"20","6":"55.8","7":"38.8","8":"5.4","9":"0.0056606831","10":"0.0741669431","11":"0.110544467","12":"0.0054467217"},{"1":"Indiana","2":"-0.159366667","3":"0.0111073549","4":"3","5":"11","6":"37.8","7":"56.9","8":"5.3","9":"0.0064128344","10":"0.0932264012","11":"-0.144509486","12":"0.0061065997"},{"1":"Iowa","2":"-0.019733333","3":"0.0436647837","4":"3","5":"6","6":"41.7","7":"51.1","8":"7.1","9":"0.0252098746","10":"0.6137272743","11":"-0.007622448","12":"0.0156681495"},{"1":"Kansas","2":"-0.156850000","3":"0.0077074639","4":"2","5":"6","6":"36.1","7":"56.7","8":"7.3","9":"0.0054500000","10":"0.0691234051","11":"-0.146007994","12":"0.0052582661"},{"1":"Kentucky","2":"-0.236500000","3":"0.0311583055","4":"3","5":"8","6":"32.7","7":"62.5","8":"4.8","9":"0.0179892561","10":"0.4472185882","11":"-0.130732804","12":"0.0133748808"},{"1":"Louisiana","2":"-0.164050000","3":"0.0136471609","4":"2","5":"8","6":"38.4","7":"58.1","8":"3.5","9":"0.0096500000","10":"0.1888425290","11":"-0.133070383","12":"0.0086912031"},{"1":"Maine","2":"0.077450000","3":"0.0070003571","4":"2","5":"4","6":"48.0","7":"45.0","8":"7.0","9":"0.0049500000","10":"0.0577205081","11":"0.072979547","12":"0.0048050185"},{"1":"Maryland","2":"0.197300000","3":"0.0208453832","4":"3","5":"10","6":"60.3","7":"33.9","8":"5.8","9":"0.0120350876","10":"0.2658440041","11":"0.144848978","12":"0.0103120125"},{"1":"Massachusetts","2":"0.201575000","3":"0.0660505551","4":"4","5":"11","6":"60.0","7":"32.8","8":"7.2","9":"0.0330252776","10":"0.7316640977","11":"0.054089810","12":"0.0171074732"},{"1":"Michigan","2":"0.020950000","3":"0.0202606762","4":"6","5":"16","6":"47.3","7":"47.5","8":"5.2","9":"0.0082713864","10":"0.1460579008","11":"0.017890087","12":"0.0076435045"},{"1":"Minnesota","2":"0.060500000","3":"0.0163477827","4":"3","5":"10","6":"46.4","7":"44.9","8":"8.6","9":"0.0094383968","10":"0.1821434657","11":"0.049480320","12":"0.0085356538"},{"1":"Mississippi","2":"-0.204066667","3":"0.0391203698","4":"3","5":"6","6":"40.1","7":"57.9","8":"1.9","9":"0.0225861560","10":"0.5605044920","11":"-0.089686383","12":"0.0149733696"},{"1":"Missouri","2":"-0.104983333","3":"0.0324377815","4":"6","5":"10","6":"38.1","7":"56.8","8":"5.1","9":"0.0132426688","10":"0.3047930944","11":"-0.072985138","12":"0.0110416139"},{"1":"Montana","2":"-0.184800000","3":"0.0060811183","4":"2","5":"3","6":"35.9","7":"56.5","8":"7.6","9":"0.0043000000","10":"0.0441826567","11":"-0.176635045","12":"0.0042039342"},{"1":"Nebraska","2":"-0.214800000","3":"0.0038431758","4":"3","5":"5","6":"34.3","7":"59.9","8":"5.8","9":"0.0022188586","10":"0.0121586802","11":"-0.212188315","12":"0.0022053281"},{"1":"Nevada","2":"0.016857143","3":"0.0440680858","4":"7","5":"6","6":"47.9","7":"45.5","8":"6.6","9":"0.0166561708","10":"0.4095313687","11":"0.009953614","12":"0.0127989276"},{"1":"New Hampshire","2":"0.056416667","3":"0.0359483679","4":"12","5":"4","6":"46.8","7":"46.5","8":"6.7","9":"0.0103773999","10":"0.2121182971","11":"0.044449659","12":"0.0092112604"},{"1":"New Jersey","2":"0.109166667","3":"0.0234000712","4":"3","5":"14","6":"55.5","7":"41.4","8":"3.2","9":"0.0135100374","10":"0.3133296075","11":"0.074961518","12":"0.0111951705"},{"1":"New Mexico","2":"0.038933333","3":"0.0225518661","4":"6","5":"5","6":"48.3","7":"40.0","8":"11.7","9":"0.0092067608","10":"0.1748569752","11":"0.032125568","12":"0.0083631806"},{"1":"New York","2":"0.184200000","3":"0.0272118847","4":"4","5":"29","6":"59.0","7":"36.5","8":"4.5","9":"0.0136059423","10":"0.3163814933","11":"0.125922529","12":"0.0112495599"},{"1":"North Carolina","2":"-0.007300000","3":"0.0306208262","4":"9","5":"15","6":"46.2","7":"49.8","8":"4.0","9":"0.0102069421","10":"0.2066351745","11":"-0.005791563","12":"0.0090914284"},{"1":"North Dakota","2":"-0.235600000","3":"0.0209719017","4":"1","5":"3","6":"27.2","7":"63.0","8":"9.8","9":"0.0209719017","10":"0.5237078353","11":"-0.112214434","12":"0.0144735322"},{"1":"Ohio","2":"-0.010416667","3":"0.0252247828","4":"6","5":"18","6":"43.5","7":"51.7","8":"4.8","9":"0.0102979745","10":"0.2095615822","11":"-0.008233734","12":"0.0091555793"},{"1":"Oklahoma","2":"-0.307650000","3":"0.0023334524","4":"2","5":"7","6":"28.9","7":"65.3","8":"5.7","9":"0.0016500000","10":"0.0067602381","11":"-0.305570213","12":"0.0016444133"},{"1":"Oregon","2":"0.091200000","3":"0.0052848841","4":"3","5":"7","6":"50.1","7":"39.1","8":"10.8","9":"0.0030512293","10":"0.0227455962","11":"0.089125602","12":"0.0030163286"},{"1":"Pennsylvania","2":"0.035333333","3":"0.0116052790","4":"9","5":"20","6":"47.9","7":"48.6","8":"3.6","9":"0.0038684263","10":"0.0360626372","11":"0.034059120","12":"0.0037980330"},{"1":"South Carolina","2":"-0.065100000","3":"0.0305768976","4":"4","5":"9","6":"40.7","7":"54.9","8":"4.4","9":"0.0152884488","10":"0.3688230127","11":"-0.041089622","12":"0.0121461601"},{"1":"South Dakota","2":"-0.207300000","3":"0.0280014285","4":"2","5":"3","6":"31.7","7":"61.5","8":"6.7","9":"0.0198000000","10":"0.4949750013","11":"-0.104691682","12":"0.0140708920"},{"1":"Tennessee","2":"-0.145666667","3":"0.0110183181","4":"3","5":"11","6":"34.7","7":"60.7","8":"4.6","9":"0.0063614289","10":"0.0918745475","11":"-0.132283608","12":"0.0060621629"},{"1":"Texas","2":"-0.095200000","3":"0.0038807216","4":"4","5":"38","6":"43.2","7":"52.2","8":"4.5","9":"0.0019403608","10":"0.0093247310","11":"-0.094312286","12":"0.0019312929"},{"1":"Utah","2":"-0.063800000","3":"0.0053740115","4":"2","5":"6","6":"27.5","7":"45.5","8":"27.0","9":"0.0038000000","10":"0.0348421967","11":"-0.061577068","12":"0.0037332129"},{"1":"Vermont","2":"0.248700000","3":"0.0209719017","4":"1","5":"3","6":"56.7","7":"30.3","8":"13.1","9":"0.0209719017","10":"0.5237078353","11":"0.118453861","12":"0.0144735322"},{"1":"Virginia","2":"0.049240000","3":"0.0183777855","4":"5","5":"13","6":"49.8","7":"44.4","8":"5.8","9":"0.0082187955","10":"0.1444739648","11":"0.042126102","12":"0.0076019462"},{"1":"Washington","2":"0.101260000","3":"0.0330086049","4":"5","5":"12","6":"54.3","7":"38.1","8":"7.6","9":"0.0147618969","10":"0.3526603072","11":"0.065549617","12":"0.0118770418"},{"1":"West Virginia","2":"-0.237833333","3":"0.0004725816","4":"3","5":"5","6":"26.5","7":"68.6","8":"4.9","9":"0.0002728451","10":"0.0001860765","11":"-0.237789078","12":"0.0002728197"},{"1":"Wisconsin","2":"0.071066667","3":"0.0104145411","4":"6","5":"10","6":"46.5","7":"47.2","8":"6.3","9":"0.0042517186","10":"0.0432387008","11":"0.067993836","12":"0.0041587835"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The estimates based on posterior do move the estimates towards 0, although the states with many polls are influenced less. This is expected as the more poll data we collect, the more we trust those results:</p>
<p><img src="datsci_04_files/figure-html/posterior-versus-original-estimates-1.png" width="624" /></p>
<p>Now we repeat this 10,000 times and generate an outcome from the posterior. In each iteration, we keep track of the total number of electoral votes for Clinton. Remember that Trump gets 270 minus the votes for Clinton. Also note that the reason we add 7 in the code is to account for Rhode Island and D.C.:</p>
<pre class="r"><code>B &lt;- 10000
mu &lt;- 0
tau &lt;- 0.02
clinton_EV &lt;- replicate(B, {
  results %&gt;% mutate(sigma = sd/sqrt(n), 
                   B = sigma^2 / (sigma^2 + tau^2),
                   posterior_mean = B * mu + (1 - B) * avg,
                   posterior_se = sqrt(1 / (1/sigma^2 + 1/tau^2)),
                   result = rnorm(length(posterior_mean), 
                                  posterior_mean, posterior_se),
                   clinton = ifelse(result &gt; 0, electoral_votes, 0)) %&gt;% 
    summarize(clinton = sum(clinton)) %&gt;% 
    pull(clinton) + 7
})

mean(clinton_EV &gt; 269)</code></pre>
<pre><code>## [1] 0.998</code></pre>
<p>This model gives Clinton over 99% chance of winning. <!--Here is a histogram of the Monte Carlo outcomes:

<img src="datsci_04_files/figure-html/election-forecast-posterior-no-bias-1.png" width="624" />
--> A similar prediction was made by the Princeton Election Consortium. We now know it was quite off. What happened?</p>
<p>The model above ignores the general bias and assumes the results from different states are independent. After the election, we realized that the general bias in 2016 was not that big: it was between 1 and 2%. But because the election was close in several big states and these states had a large number of polls, pollsters that ignored the general bias greatly underestimated the standard error. Using the notation we introduce, they assumed the standard error was <span class="math inline">\(\sqrt{\sigma^2/N}\)</span> which with large N is quite smaller than the more accurate estimate <span class="math inline">\(\sqrt{\sigma^2/N + \sigma_b^2}\)</span>. FiveThirtyEight, which models the general bias in a rather sophisticated way, reported a closer result. We can simulate the results now with a bias term. For the state level, the general bias can be larger so we set it at <span class="math inline">\(\sigma_b = 0.03\)</span>:</p>
<pre class="r"><code>tau &lt;- 0.02
bias_sd &lt;- 0.03
clinton_EV_2 &lt;- replicate(1000, {
  results %&gt;% mutate(sigma = sqrt(sd^2/n  + bias_sd^2),  
                   B = sigma^2 / (sigma^2 + tau^2),
                   posterior_mean = B*mu + (1-B)*avg,
                   posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2)),
                   result = rnorm(length(posterior_mean), 
                                  posterior_mean, posterior_se),
                   clinton = ifelse(result&gt;0, electoral_votes, 0)) %&gt;% 
    summarize(clinton = sum(clinton) + 7) %&gt;% 
    pull(clinton)
})
mean(clinton_EV_2 &gt; 269)</code></pre>
<pre><code>## [1] 0.848</code></pre>
<p>This gives us a much more sensible estimate. Looking at the outcomes of the simulation, we see how the bias term adds variability to the final results.</p>
<p><img src="datsci_04_files/figure-html/comparison-forecast-with-and-without-bias-1.png" width="624" /></p>
<p>FiveThirtyEight includes many other features we do not include here. One is that they model variability with distributions that have high probabilities for extreme events compared to the normal. One way we could do this is by changing the distribution used in the simulation from a normal distribution to a t-distribution. FiveThirtyEight predicted a probability of 71%.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>In the US election, each state has a certain number of votes that are won all-or-nothing based on the popular vote result in that state (with minor exceptions not discussed here).</p></li>
<li><p>We use the <code>left_join()</code> function to combine the number of electoral votes with our poll results.</p></li>
<li><p>For each state, we apply a Bayesian approach to generate an Election Day 𝑑. We keep our prior simple by assuming an expected value of 0 and a standard deviation based on recent history of 0.02.</p></li>
<li><p>We can run a Monte Carlo simulation that for each iteration simulates poll results in each state using that state’s average and standard deviation, awards electoral votes for each state to Clinton if the spread is greater than 0, then compares the number of electoral votes won to the number of votes required to win the election (over 269).</p></li>
<li><p>If we run a Monte Carlo simulation for the electoral college without accounting for general bias, we overestimate Clinton’s chances of winning at over 99%.</p></li>
<li><p>If we include a general bias term, the estimated probability of Clinton winning decreases significantly.</p></li>
</ul>
</div>
</div>
<div id="section-forecasting" class="section level3">
<h3>Forecasting</h3>
<p>Forecasters like to make predictions well before the election. The predictions are adapted as new polls come out. However, an important question forecasters must ask is: how informative are polls taken several weeks before the election about the actual election? Here we study the variability of poll results across time.</p>
<p>To make sure the variability we observe is not due to pollster effects, let’s study data from one pollster:</p>
<pre class="r"><code>one_pollster &lt;- polls_us_election_2016 %&gt;% 
  filter(pollster == &quot;Ipsos&quot; &amp; state == &quot;U.S.&quot;) %&gt;% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)</code></pre>
<p>Since there is no pollster effect, then perhaps the theoretical standard error matches the data-derived standard deviation. We compute both here:</p>
<pre class="r"><code>se &lt;- one_pollster %&gt;% 
  summarize(empirical = sd(spread), 
            theoretical = 2 * sqrt(mean(spread) * (1 - mean(spread)) /
                                     min(samplesize)))
se</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["empirical"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["theoretical"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.04025194","2":"0.03256719"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>But the empirical standard deviation is higher than the highest possible theoretical estimate. Furthermore, the spread data does not look normal as the theory would predict:</p>
<p><img src="datsci_04_files/figure-html/time-trend-variability-1.png" width="624" /></p>
<p>The models we have described include pollster-to-pollster variability and sampling error. But this plot is for one pollster and the variability we see is certainly not explained by sampling error. Where is the extra variability coming from? The following plots make a strong case that it comes from time fluctuations not accounted for by the theory that assumes <span class="math inline">\(p\)</span> is fixed:</p>
<p><img src="datsci_04_files/figure-html/time-trend-estimate-1.png" width="624" /></p>
<p>Some of the peaks and valleys we see coincide with events such as the party conventions, which tend to give the candidate a boost. We can see the peaks and valleys are consistent across several pollsters:</p>
<p><img src="datsci_04_files/figure-html/time-trend-estimate-several-pollsters-1.png" width="624" /></p>
<p>This implies that, if we are going to forecast, our model must include a term to accounts for the time effect. We need to write a model including a bias term for time:</p>
<p><span class="math display">\[
Y_{i,j,t} = d + b + h_j + b_t + \varepsilon_{i,j,t}
\]</span></p>
<p>The standard deviation of <span class="math inline">\(b_t\)</span> would depend on <span class="math inline">\(t\)</span> since the closer we get to election day, the closer to 0 this bias term should be.</p>
<p>Pollsters also try to estimate trends from these data and incorporate these into their predictions. We can model the time trend with a function <span class="math inline">\(f(t)\)</span> and rewrite the model like this: The blue lines in the plots above:</p>
<p><span class="math display">\[
Y_{i,j,t} = d + b + h_j + b_t + f(t) + \varepsilon_{i,jt,}
\]</span></p>
<p>We usually see the estimated <span class="math inline">\(f(t)\)</span> not for the difference, but for the actual percentages for each candidate like this:</p>
<p><img src="datsci_04_files/figure-html/trend-estimate-for-all-pollsters-1.png" width="624" /></p>
<p>Once a model like the one above is selected, we can use historical and present data to estimate all the necessary parameters to make predictions. There is a variety of methods for estimating trends <span class="math inline">\(f(t)\)</span> which we discuss in the Machine Learning part.</p>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>In poll results, 𝑝 is not fixed over time. Variability within a single pollster comes from time variation.</p></li>
<li><p>In order to forecast, our model must include a bias term 𝑏𝑡 to model the time effect.</p></li>
</ul>
<p>-Pollsters also try to estimate <span class="math inline">\(f(t)\)</span>, the trend of 𝑝 given time 𝑡 using a model like:</p>
<p><span class="math display">\[
Y_{i,j,t} = d + b + h_j + b_t + f(t) + \varepsilon_{i,jt,}
\]</span></p>
<ul>
<li>Once we decide on a model, we can use historical data and current data to estimate the necessary parameters to make predictions.</li>
</ul>
</div>
</div>
<div id="section-assessment-election-forecasting" class="section level3">
<h3>6.1 Assessment: Election Forecasting</h3>
<p>Insert assessment here</p>
</div>
<div id="section-the-t-distribution" class="section level3">
<h3>The t-Distribution</h3>
<p>Above we made use of the CLT with a sample size of 15. Because we are estimating a second parameters <span class="math inline">\(\sigma\)</span>, further variability is introduced into our confidence interval which results in intervals that are too small. For very large sample sizes this extra variability is negligible, but, in general, for values smaller than 30 we need to be cautious about using the CLT.</p>
<p>However, if the data in the urn is known to follow a normal distribution, then we actually have mathematical theory that tells us how much bigger we need to make the intervals to account for the estimation of <span class="math inline">\(\sigma\)</span>. Using this theory, we can construct confidence intervals for any <span class="math inline">\(N\)</span>. But again, this works only if <strong>the data in the urn is known to follow a normal distribution</strong>. So for the 0, 1 data of our previous urn model, this theory definitely does not apply.</p>
<p>The statistic on which confidence intervals for <span class="math inline">\(d\)</span> are based is</p>
<p><span class="math display">\[
Z = \frac{\bar{X} - d}{\sigma/\sqrt{N}}
\]</span></p>
<p>CLT tells us that Z is approximately normally distributed with expected value 0 and standard error 1. But in practice we don’t know <span class="math inline">\(\sigma\)</span> so we use:</p>
<p><span class="math display">\[
Z = \frac{\bar{X} - d}{s/\sqrt{N}}
\]</span></p>
<p>By substituting <span class="math inline">\(\sigma\)</span> with <span class="math inline">\(s\)</span> we introduce some variability. The theory tells us that <span class="math inline">\(Z\)</span> follows a t-distribution with <span class="math inline">\(N-1\)</span> <em>degrees of freedom</em>. The degrees of freedom is a parameter that controls the variability via fatter tails:</p>
<p><img src="datsci_04_files/figure-html/t-distribution-examples-1.png" width="624" /></p>
<p>If we are willing to assume the pollster effect data is normally distributed, based on the sample data <span class="math inline">\(X_1, \dots, X_N\)</span>,</p>
<pre class="r"><code>one_poll_per_pollster %&gt;%
  ggplot(aes(sample=spread)) + stat_qq()</code></pre>
<p><img src="datsci_04_files/figure-html/poll-spread-qq-1.png" width="624" /></p>
<p>then <span class="math inline">\(Z\)</span> follows a t-distribution with <span class="math inline">\(N-1\)</span> degrees of freedom. So perhaps a better confidence interval for <span class="math inline">\(d\)</span> is:</p>
<pre class="r"><code>z &lt;- qt(0.975,  nrow(one_poll_per_pollster)-1)
one_poll_per_pollster %&gt;% 
  summarize(avg = mean(spread), moe = z*sd(spread)/sqrt(length(spread))) %&gt;% 
  mutate(start = avg - moe, end = avg + moe) </code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["avg"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["moe"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["start"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["end"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.02898","2":"0.01339802","3":"0.01558198","4":"0.04237802"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>A bit larger than the one using normal is</p>
<pre class="r"><code>qt(0.975, 14)</code></pre>
<pre><code>## [1] 2.144787</code></pre>
<p>is bigger than</p>
<pre class="r"><code>qnorm(0.975)</code></pre>
<pre><code>## [1] 1.959964</code></pre>
<p>The t-distribution can also be used to model errors in bigger deviations that are more likely than with the normal distribution, as seen in the densities we previously saw. Fivethirtyeight uses the t-distribution to generate errors that better model the deviations we see in election data. For example, in Wisconsin the average of six polls was 7% in favor of Clinton with a standard deviation of 1%, but Trump won by 0.7%. Even after taking into account the overall bias, this 7.7% residual is more in line with t-distributed data than the normal distribution.</p>
<pre class="r"><code>data(&quot;polls_us_election_2016&quot;)
polls_us_election_2016 %&gt;%
  filter(state ==&quot;Wisconsin&quot; &amp;
           enddate &gt;=&quot;2016-10-31&quot; &amp; 
           (grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade))) %&gt;%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %&gt;%
  mutate(state = as.character(state)) %&gt;%
  left_join(results_us_election_2016, by = &quot;state&quot;) %&gt;%
  mutate(actual = clinton/100 - trump/100) %&gt;%
  summarize(actual = first(actual), avg = mean(spread), 
            sd = sd(spread), n = n()) %&gt;%
  select(actual, avg, sd, n)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["actual"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["avg"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["sd"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["n"],"name":[4],"type":["int"],"align":["right"]}],"data":[{"1":"-0.007","2":"0.07106667","3":"0.01041454","4":"6"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>In models where we must estimate two parameters, <span class="math inline">\(p\)</span> and <span class="math inline">\(\sigma\)</span>, the Central Limit Theorem can result in overconfident confidence intervals for sample sizes smaller than approximately 30.</p></li>
<li><p>If the population data are known to follow a normal distribution, theory tells us how much larger to make confidence intervals to account for estimation of <span class="math inline">\(\sigma\)</span>.</p></li>
<li><p>Given <span class="math inline">\(s\)</span> as an estimate of 𝜎, then <span class="math inline">\(Z = \frac{\bar{X} - d}{s/\sqrt{N}}\)</span> follows a t-distribution with <span class="math inline">\(N-1\)</span> degrees of freedom.</p></li>
<li><p>Degrees of freedom determine the weight of the tails of the distribution. Small values of degrees of freedom lead to increased probabilities of extreme values.</p></li>
<li><p>We can determine confidence intervals using the t-distribution instead of the normal distribution by calculating the desired quantile with the function <code>qt()</code>.</p></li>
</ul>
</div>
</div>
<div id="section-assessment-the-t-distribution" class="section level3">
<h3>6.2 Assessment: The t-Distribution</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-section-7-association-tests" class="section level2">
<h2>Section 7: Association Tests</h2>
<p>In Section 7, you will learn how to use association and chi-squared tests to perform inference for binary, categorical, and ordinal data through an example looking at research funding rates.</p>
<p>After completing Section 7, you will be able to:</p>
<ul>
<li><p>Use association and chi-squared tests to perform inference on binary, categorical, and ordinal data.</p></li>
<li><p>Calculate an odds ratio to get an idea of the magnitude of an observed effect.</p></li>
</ul>
<p>There is 1 assignment for you to practice your coding skills.</p>
<p>We encourage you to use <svg style="height: 1em; top:.04em; position: relative; fill: #136CB9;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to interactively test out your answers and further your learning.</p>
<div id="section-association-tests" class="section level3">
<h3>Association Tests</h3>
<p>The statistical tests we have studied up to now leave out a substantial portion of data types. Specifically, we have not discussed inference for binary, categorical, and ordinal data. To give a very specific example, consider the following case study.</p>
<p>A <a href="http://www.pnas.org/content/112/40/12349.abstract">2014 PNAS paper</a> analyzed success rates from funding agencies in the Netherlands and concluded that their:</p>
<blockquote>
<p>results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language use in instructional and evaluation materials.</p>
</blockquote>
<p>The main evidence for this conclusion comes down to a comparison of the percentages. Table S1 in the paper includes the information we need. Here are the three columns showing the overall outcomes:</p>
<pre class="r"><code>library(tidyverse)
library(dslabs)
data(&quot;research_funding_rates&quot;)
research_funding_rates %&gt;% select(discipline, applications_total, 
                                  success_rates_total) %&gt;% head()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["discipline"],"name":[1],"type":["chr"],"align":["left"]},{"label":["applications_total"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["success_rates_total"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"Chemical sciences","2":"122","3":"26.2","_rn_":"1"},{"1":"Physical sciences","2":"174","3":"20.1","_rn_":"2"},{"1":"Physics","2":"76","3":"26.3","_rn_":"3"},{"1":"Humanities","2":"396","3":"16.4","_rn_":"4"},{"1":"Technical sciences","2":"251","3":"17.1","_rn_":"5"},{"1":"Interdisciplinary","2":"183","3":"15.8","_rn_":"6"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>We have these values for each gender:</p>
<pre class="r"><code>names(research_funding_rates)</code></pre>
<pre><code>##  [1] &quot;discipline&quot;          &quot;applications_total&quot;  &quot;applications_men&quot;   
##  [4] &quot;applications_women&quot;  &quot;awards_total&quot;        &quot;awards_men&quot;         
##  [7] &quot;awards_women&quot;        &quot;success_rates_total&quot; &quot;success_rates_men&quot;  
## [10] &quot;success_rates_women&quot;</code></pre>
<p>We can compute the totals that were successful and the totals that were not as follows:</p>
<pre class="r"><code>totals &lt;- research_funding_rates %&gt;% 
  select(-discipline) %&gt;% 
  summarize_all(sum) %&gt;%
  summarize(yes_men = awards_men, 
            no_men = applications_men - awards_men, 
            yes_women = awards_women, 
            no_women = applications_women - awards_women) </code></pre>
<p>So we see that a larger percent of men than women received awards:</p>
<pre class="r"><code>totals %&gt;% summarize(percent_men = yes_men/(yes_men+no_men),
                     percent_women = yes_women/(yes_women+no_women))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["percent_men"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["percent_women"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.17737","2":"0.1489899"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>But could this be due just to random variability? Here we learn how to perform inference for this type of data.</p>
<div id="section-lady-tasting-tea" class="section level4">
<h4><strong>Lady Tasting Tea</strong></h4>
<p><a href="https://en.wikipedia.org/wiki/Ronald_Fisher">R.A. Fisher</a> was one of the first to formalize hypothesis testing. The “Lady Tasting Tea” is one of the most famous examples.</p>
<p>The story is as follows: an acquaintance of Fisher’s claimed that she could tell if milk was added before or after tea was poured. Fisher was skeptical. He designed an experiment to test this claim. He gave her four pairs of cups of tea: one with milk poured first, the other after. The order was randomized. The null hypothesis here is that she is guessing. Fisher derived the distribution for the number of correct picks on the assumption that the choices were random and independent.</p>
<p>As an example, suppose she picked 3 out of 4 correctly. Do we believe she has a special ability? The basic question we ask is: if the tester is actually guessing, what are the chances that she gets 3 or more correct? Just as we have done before, we can compute a probability under the null hypothesis that she is guessing 4 of each. Under this null hypothesis, we can think of this particular example as picking 4 balls out of an urn with 4 blue (correct answer) and 4 red (incorrect answer) balls. Remember, she knows that there are four before tea and four after.</p>
<p>Under the null hypothesis that she is simply guessing, each ball has the same chance of being picked. We can then use combinations to figure out each probability. The probability of picking 3 is <span class="math inline">\({4 \choose 3} {4 \choose 1} / {8 \choose 4} = 16/70\)</span>. The probability of picking all 4 correct is <span class="math inline">\({4 \choose 4} {4 \choose 0}/{8 \choose 4}= 1/70\)</span>. Thus, the chance of observing a 3 or something more extreme, under the null hypothesis, is <span class="math inline">\(\approx 0.24\)</span>. This is the p-value. The procedure that produced this p-value is called <em>Fisher’s exact test</em> and it uses the <em>hypergeometric distribution</em>.</p>
</div>
<div id="section-two-by-two-tables" class="section level4">
<h4><strong>Two-by-two tables</strong></h4>
<p>The data from the experiment is usually summarized by a table like this:</p>
<pre class="r"><code>tab &lt;- matrix(c(3,1,1,3),2,2)
rownames(tab)&lt;-c(&quot;Poured Before&quot;,&quot;Poured After&quot;)
colnames(tab)&lt;-c(&quot;Guessed before&quot;,&quot;Guessed after&quot;)
tab</code></pre>
<pre><code>##               Guessed before Guessed after
## Poured Before              3             1
## Poured After               1             3</code></pre>
<p>These are referred to as a two-by-two table. For each of the four combinations one can get with a pair of binary variables, they show the observed counts for each occurrence.</p>
<p>The function <code>fisher.test</code> performs the inference calculations above:</p>
<pre class="r"><code>fisher.test(tab, alternative=&quot;greater&quot;)$p.value</code></pre>
<pre><code>## [1] 0.2428571</code></pre>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>We learn how to determine the probability that an observation is due to random variability given categorical, binary or ordinal data.</p></li>
<li><p>Fisher’s exact test determines the p-value as the probability of observing an outcome as extreme or more extreme than the observed outcome given the null distribution.</p></li>
<li><p>Data from a binary experiment are often summarized in <em>two-by-two tables</em>.</p></li>
<li><p>The p-value can be calculated from a two-by-two table using Fisher’s exact test with the function <code>fisher.test()</code>.</p></li>
</ul>
</div>
</div>
</div>
<div id="section-chi-squared-tests" class="section level3">
<h3>Chi-Squared Tests</h3>
<p>Notice that, in a way, our funding rates example is similar to the Lady Tasting Tea. However, in the Lady Tasting Tea example, the number of blue and red beads is experimentally fixed and the number of answers given for each category is also fixed. This is because Fisher made sure there were four cups with milk poured before tea and four cups with milk poured after and the lady knew this, so the answers would also have to include four befores and four afters. If this is the case, the sum of the rows and the sum of the columns are fixed. This defines constraints on the possible ways we can fill the two by two table and also permits us to use the hypergeometric distribution. In general, this is not the case. Nonetheless, there is another approach, the Chi-squared test, which is described below.</p>
<p>Imagine we have 290, 1,345, 177, 1,011 applicants, some are men and some are women and some get funded, whereas others don’t. We saw that the success rates for men and woman were:</p>
<pre class="r"><code>totals %&gt;% summarize(percent_men = yes_men/(yes_men+no_men),
                     percent_women = yes_women/(yes_women+no_women))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["percent_men"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["percent_women"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.17737","2":"0.1489899"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>respectively. Would we see this again if we randomly assign funding at the overall rate:</p>
<pre class="r"><code>rate &lt;- totals %&gt;%
  summarize(percent_total = 
              (yes_men + yes_women)/
              (yes_men + no_men +yes_women + no_women)) %&gt;%
  pull(percent_total)
rate</code></pre>
<pre><code>## [1] 0.1654269</code></pre>
<p>The Chi-square test answers this question. The first step is to create the two-by-two data table:</p>
<pre class="r"><code>two_by_two &lt;- data.frame(awarded = c(&quot;no&quot;, &quot;yes&quot;), 
                     men = c(totals$no_men, totals$yes_men),
                     women = c(totals$no_women, totals$yes_women))
two_by_two</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["awarded"],"name":[1],"type":["chr"],"align":["left"]},{"label":["men"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["women"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"no","2":"1345","3":"1011"},{"1":"yes","2":"290","3":"177"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The general idea of the Chi-square test is to compare this two-by-two table to what you expect to see, which would be:</p>
<pre class="r"><code>data.frame(awarded = c(&quot;no&quot;, &quot;yes&quot;), 
       men = (totals$no_men + totals$yes_men) * c(1 - rate, rate),
       women = (totals$no_women + totals$yes_women) * c(1 - rate, rate))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["awarded"],"name":[1],"type":["chr"],"align":["left"]},{"label":["men"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["women"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"no","2":"1364.5271","3":"991.4729"},{"1":"yes","2":"270.4729","3":"196.5271"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>We can see that more men than expected and fewer women than expected received funding. However, under the null hypothesis these observations are random variables. The Chi-square test tells us how likely it is to see a deviation this large or larger. This test uses an asymptotic result, similar to the CLT, related to the sums of independent binary outcomes. The R function <code>chisq.test</code> takes a two-by-two table and returns the results from the test:</p>
<pre class="r"><code>chisq_test &lt;- two_by_two %&gt;% select(-awarded) %&gt;% chisq.test()</code></pre>
<p>We see that the p-value is 0.0509:</p>
<pre class="r"><code>chisq_test$p.value</code></pre>
<pre><code>## [1] 0.05091372</code></pre>
<hr />
</div>
<div id="section-the-odds-ratio" class="section level3">
<h3>The odds ratio</h3>
<p>An informative summary statistic associated with two-by-two tables is the odds ratio. Define the two variables as <span class="math inline">\(X = 1\)</span> if you are a male and 0 otherwise, and <span class="math inline">\(Y=1\)</span> if you are funded and 0 otherwise. The odds of getting funded if you are a man is defined:</p>
<p><span class="math display">\[\mbox{Pr}(Y=1 \mid X=1) / \mbox{Pr}(Y=0 \mid X=1)\]</span></p>
<p>and can be computed like this:</p>
<pre class="r"><code>odds_men &lt;- with(two_by_two, (men[2]/sum(men)) / (men[1]/sum(men)))
odds_men</code></pre>
<pre><code>## [1] 0.2156134</code></pre>
<p>And the odds of being funded if you are a woman is:</p>
<p><span class="math display">\[\mbox{Pr}(Y=1 \mid X=0) / \mbox{Pr}(Y=0 \mid X=0)\]</span></p>
<p>and can be computed like this:</p>
<pre class="r"><code>odds_women &lt;- with(two_by_two, (women[2]/sum(women)) / (women[1]/sum(women)))
odds_women</code></pre>
<pre><code>## [1] 0.1750742</code></pre>
<p>The odds ratio is the ratio for these two odds: how many times larger are the odds for men than for women?</p>
<pre class="r"><code>odds_men / odds_women</code></pre>
<pre><code>## [1] 1.231554</code></pre>
<p>We often see two-by-two tables written out as</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
Men
</th>
<th style="text-align:center;">
Women
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Awarded
</td>
<td style="text-align:center;">
a
</td>
<td style="text-align:center;">
b
</td>
</tr>
<tr>
<td style="text-align:left;">
Not Awarded
</td>
<td style="text-align:center;">
c
</td>
<td style="text-align:center;">
d
</td>
</tr>
</tbody>
</table>
<p>In this case, the odds ratio is <span class="math inline">\(\frac{a/c}{b/d}\)</span> which is equivalent to <span class="math inline">\((ad) / (bc)\)</span></p>
<hr />
</div>
<div id="section-confidence-intervals-for-the-odds-ratio" class="section level3">
<h3>Confidence intervals for the odds ratio</h3>
<p>Computing confidence intervals for the odds ratio is not mathematically straightforward. Unlike other statistics, for which we can derive useful approximations of their distributions, the odds ratio is not only a ratio, but a ratio of ratios. Therefore, there is no simple way of using, for example, the CLT.</p>
<p>However, statistical theory tells us that when all four entries of the two-by-two table are large enough, then the log of the odds ratio is approximately normal with standard error</p>
<p><span class="math display">\[
\sqrt{1/a + 1/b + 1/c + 1/d} 
\]</span></p>
<p>This implies that a 95% confidence interval for the log odds ratio can be formed by:</p>
<p><span class="math display">\[
\log\left(\frac{ad}{bc}\right) \pm 1.96 \sqrt{1/a + 1/b + 1/c + 1/d} 
\]</span></p>
<p>By exponentiating these two numbers we can construct a confidence interval of the odds ratio.</p>
<p>Using R we can compute this confidence interval as follows:</p>
<pre class="r"><code>log_or &lt;- log(odds_men / odds_women)
se &lt;- two_by_two %&gt;% select(-awarded) %&gt;%
  summarize(se = sqrt(sum(1/men) + sum(1/women))) %&gt;%
  pull(se)
ci &lt;- log_or + c(-1,1) * qnorm(0.975) * se</code></pre>
<p>If we want to convert it back to the odds ratio scale, we can exponentiate:</p>
<pre class="r"><code>exp(ci)</code></pre>
<pre><code>## [1] 1.004313 1.510213</code></pre>
<p>Note that 1 is not included in the confidence interval which must mean that the p-value is smaller than 0.05. We can confirm this using:</p>
<pre class="r"><code>2*(1 - pnorm(log_or, 0, se))</code></pre>
<pre><code>## [1] 0.0453586</code></pre>
<p>This is a slightly different p-value than that with the Chi-square test. This is because we are using a different asymptotic approximation to the null distribution. To learn more about inference and asymptotic theory for odds ratio, consult the <em>Generalized Linear Models</em> book by McCullagh and Nelder.</p>
<hr />
</div>
<div id="section-small-count-correction" class="section level3">
<h3>Small count correction</h3>
<p>Note that the log odds ratio is not defined if any of the cells of the two-by-two table is 0. This is because if <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, <span class="math inline">\(c\)</span>, or <span class="math inline">\(d\)</span> is 0, the <span class="math inline">\(\log(\frac{ad}{bc})\)</span> is either the log of 0 or has a 0 in the denominator. For this situation, it is common practice to avoid 0s by adding 0.5 to each cell. This is referred to as the <em>Haldane–Anscombe correction</em> and has been shown, both in practice and theory, to work well.</p>
<hr />
</div>
<div id="section-large-samples-small-p-values" class="section level3">
<h3>Large samples, small p-values</h3>
<p>As mentioned earlier, reporting only p-values is not an appropriate way to report the results of data analysis. In scientific journals, for example, some studies seem to overemphasize p-values. Some of these studies have large sample sizes and report impressively small p-values. Yet when one looks closely at the results, we realize odds ratios are quite modest: barely bigger than 1. In this case the difference may not be <em>practically significant</em> or <em>scientifically significant</em>.</p>
<p>Note that the relationship between odds ratio and p-value is not one-to-one. It depends on the sample size. So a very small p-value does not necessarily mean a very large odds ratio. Notice what happens to the p-value if we multiply our two-by-two table by 10, which does not change the odds ratio:</p>
<pre class="r"><code>two_by_two %&gt;% select(-awarded) %&gt;%
  mutate(men = men*10, women = women*10) %&gt;%
  chisq.test() %&gt;% .$p.value</code></pre>
<pre><code>## [1] 2.625423e-10</code></pre>
<hr />
<div class="infobox">
<p><img src="images/The%20Brain.png" alt="The Brain Logo." style="width:50px;height:50px;" class = "img_left"></p>
<p><strong>Key points:</strong></p>
<hr />
<ul>
<li><p>If the sums of the rows and the sums of the columns in the two-by-two table are fixed, then the hypergeometric distribution and Fisher’s exact test can be used. Otherwise, we must use the chi-squared test.</p></li>
<li><p>The <em>chi-squared test</em> compares the observed two-by-two table to the two-by-two table expected by the null hypothesis and asks how likely it is that we see a deviation as large as observed or larger by chance.</p></li>
<li><p>The function <code>chisq.test()</code> takes a two-by-two table and returns the p-value from the chi-squared test.</p></li>
<li><p>The <em>odds ratio</em> states how many times larger the odds of an outcome are for one group relative to another group.</p></li>
<li><p>A small p-value does not imply a large odds ratio. If a finding has a small p-value but also a small odds ratio, it may not be a practically significant or scientifically significant finding.</p></li>
<li><p>Because the odds ratio is a ratio of ratios, there is no simple way to use the Central Limit Theorem to compute confidence intervals. There are advanced methods for computing confidence intervals for odds ratios that we do not discuss here.</p></li>
</ul>
</div>
</div>
<div id="section-assessment-association-and-chi-squared-tests" class="section level3">
<h3>7.1 Assessment: Association and Chi-Squared Tests</h3>
<p>Insert assessment here</p>
</div>
<div id="section-brexit-poll-analysis---part-1" class="section level3">
<h3>7.2 Brexit poll analysis - Part 1</h3>
<div id="section-directions" class="section level4">
<h4><strong>Directions</strong></h4>
<p>There are 12 multi-part problems in this comprehensive assessment that review concepts from the entire course. The problems are split over 3 pages. Make sure you read the instructions carefully and run all pre-exercise code.</p>
<p>For numeric entry problems, you have 10 attempts to input the correct answer. For true/false problems, you have 2 attempts.</p>
<p>If you have questions, visit the “Brexit poll analysis” discussion forum that follows the assessment.</p>
</div>
</div>
<div id="section-brexit-poll-analysis---part-2" class="section level3">
<h3>7.2 Brexit poll analysis - Part 2</h3>
<p>Insert assessment here</p>
</div>
<div id="section-brexit-poll-analysis---part-3" class="section level3">
<h3>7.2 Brexit poll analysis - Part 3</h3>
<p>Insert assessment here</p>
</div>
</div>
<div id="section-acknowledgement" class="section level2">
<h2>Acknowledgement</h2>
<p>I am extremely grateful to <a href="http://rafalab.github.io/pages/about.html">Prof Rafael Irizarry</a> for his support and encouragement to create this interactive tutorial which is based on his freely available textbook <a href="https://rafalab.github.io/dsbook/">Introduction to Data Science</a>. The textbook has been developed as the basis for the associated edX Course Series <em>HarvardX Professional Certificate in Data Science</em> and this tutorial follows the structure of this online course. I’m further very grateful to <a href="https://profiles.sussex.ac.uk/p9846-andy-field">Andy Field</a> for his generous permission to use his <code>discovr</code> package as a basis for the development of this tutorial. Thanks to his amazing <code>discovr</code> package I also indirectly benefited from the work of <a href="https://www.allisonhorst.com/">Allison Horst</a> and her very informative blog post on <a href="https://education.rstudio.com/blog/2020/05/learnr-for-remote/">styling learnr tutorials with CSS</a> as well as her CSS template file which I adapted here.</p>

<script type="application/shiny-prerendered" data-context="server-start">
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

library(learnr) #necessary to render tutorial correctly

library(forcats)
library(ggplot2)
library(htmltools)
library(kableExtra)
library(lubridate)
library(magrittr)
library(tibble)


source("./www/datsci_helpers.R")
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::register_http_handlers(session, metadata = NULL)
</script>
 
<script type="application/shiny-prerendered" data-context="server">
session$onSessionEnded(function() {
        learnr:::session_stop_event(session)
      })
</script>
 <!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/lumen.min.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["navigation"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/navigation-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tabsets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["default.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["font-awesome"]},{"type":"character","attributes":{},"value":["5.1.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/fontawesome"]}]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["css/all.css","css/v4-shims.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootbox"]},{"type":"character","attributes":{},"value":["4.4.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/bootbox"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["bootbox.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["idb-keyvalue"]},{"type":"character","attributes":{},"value":["3.2.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/idb-keyval"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["idb-keyval-iife-compat.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95]}},"value":[{"type":"character","attributes":{},"value":["assertthat","backports","base","blob","broom","cellranger","cli","colorspace","compiler","crayon","curl","datasets","DBI","dbplyr","digest","dplyr","dslabs","ellipsis","evaluate","fansi","farver","fastmap","forcats","fs","generics","ggplot2","glue","graphics","grDevices","grid","gridExtra","gtable","haven","highr","hms","htmltools","htmlwidgets","httpuv","httr","jsonlite","kableExtra","knitr","labeling","Lahman","later","lattice","learnr","lifecycle","lubridate","magrittr","markdown","Matrix","methods","mgcv","mime","modelr","munsell","nlme","pillar","pkgconfig","promises","purrr","R6","rafalib","RColorBrewer","Rcpp","readr","readxl","reprex","rlang","rmarkdown","rprojroot","rstudioapi","rvest","scales","selectr","shiny","splines","stats","stringi","stringr","tibble","tidyr","tidyselect","tidyverse","tools","utils","vctrs","viridisLite","webshot","withr","xfun","xml2","xtable","yaml"]},{"type":"character","attributes":{},"value":["0.2.1","1.1.8","4.0.2","1.2.1","0.7.0","1.1.0","2.0.2","1.4-1","4.0.2","1.3.4","4.3","4.0.2","1.1.0","1.4.4","0.6.25","1.0.0","0.7.3","0.3.1","0.14","0.4.1","2.0.3","1.0.1","0.5.0","1.4.2","0.0.2","3.3.2","1.4.1","4.0.2","4.0.2","4.0.2","2.3","0.3.0","2.3.1","0.8","0.5.3","0.5.0","1.5.1","1.5.4","1.4.2","1.7.0","1.1.0","1.29","0.3","8.0-0","1.1.0.1","0.20-41","0.10.1","0.2.0","1.7.9","1.5","1.1","1.2-18","4.0.2","1.8-31","0.9","0.1.8","0.5.0","3.1-148","1.4.6","2.0.3","1.1.1","0.3.4","2.4.1","1.0.0","1.1-2","1.0.5","1.3.1","1.3.1","0.3.0","0.4.7","2.3","1.3-2","0.11","0.3.6","1.1.1","0.4-2","1.5.0","4.0.2","4.0.2","1.4.6","1.4.0","3.0.3","1.1.0","1.1.0","1.3.0","4.0.2","4.0.2","0.3.2","0.3.0","0.5.2","2.2.0","0.16","1.3.2","1.8-4","2.2.1"]}]}]}
</script>
<!--/html_preserve-->
</div>
</div>

</div> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h2 class="title toc-ignore" style="display:none;">datsci_04: Inference and Modeling</h2>
<h4 class="author"><em>Rafael Irizarry &amp; Fatih Uenal</em></h4>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</div> <!-- bandContent page -->
</div> <!-- pageContent band -->




<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>

</html>
